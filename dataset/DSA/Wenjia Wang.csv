Year,Sources,Name,Authors,First Author,Chinese/English,Abstract,Venues,doi,Citation,Id,Keywords
2021,the 24th International Conference on Artificial Intelligence and Statistics,Regularization Matters: A Nonparametric Perspective on Overparametrized Neural Network,"Tianyang Hu*, Wenjia Wang*, Cong Lin, Guang Cheng, (*equal contributions)",Tianyang Hu*,English,"Overparametrized neural networks trained by gradient descent (GD) can provably overfit any training data. However, the generalization guarantee may not hold for noisy data. From a nonparametric perspective, this paper studies how well overparametrized neural networks can recover the true target function in the presence of random noises. We establish a lower bound on the L2 estimation error with respect to the GD iteration, which is away from zero without a delicate choice of early stopping. In turn, through a comprehensive analysis of L2-regularized GD trajectories, we prove that for overparametrized one-hidden-layer ReLU neural network with the L2 regularization:(1) the output is close to that of the kernel ridge regression with the corresponding neural tangent kernel;(2) minimax optimal rate of the L2 estimation error is achieved. Numerical experiments confirm our theory and further demonstrate that the L2 regularization approach improves the training robustness and works for a wider range of neural networks.",Article,https://proceedings.mlr.press/v130/hu21a.html,40,hu2021regularization,None
2022,Journal of Machine Learning Research,"Gaussian process regression: Optimality, robustness, and relationship with kernel ridge regression","Wenjia Wang, Bing-Yi Jing",Wenjia Wang,English,"Gaussian process regression is widely used in many fields, for example, machine learning, reinforcement learning and uncertainty quantification. One key component of Gaussian process regression is the unknown correlation function, which needs to be specified. In this paper, we investigate what would happen if the correlation function is misspecified. We derive upper and lower error bounds for Gaussian process regression with possibly misspecified correlation functions. We find that when the sampling scheme is quasi-uniform, the optimal convergence rate can be attained even if the smoothness of the imposed correlation function exceeds that of the true correlation function. We also obtain convergence rates of kernel ridge regression with misspecified kernel function, where the underlying truth is a deterministic function. Our study reveals a close connection between the convergence rates of Gaussian process regression and kernel ridge regression, which is aligned with the relationship between sample paths of Gaussian process and the corresponding reproducing kernel Hilbert space. This work establishes a bridge between Bayesian learning based on Gaussian process and frequentist kernel methods with reproducing kernel Hilbert space.",Article,https://www.jmlr.org/papers/v23/21-0570.html,13,wang2022gaussian,"Gaussian process regression, Bayesian machine learning, Kernel ridge regression, Reproducing kernel Hilbert space, Space-filling designs"
2021,Electronic Journal of Statistics,On the inference of applying Gaussian process modeling to a deterministic function,Wenjia Wang,Wenjia Wang,English," Gaussian process modeling is a standard tool for building emulators for computer experiments, which are usually used to study deterministic functions, for example, a solution to a given system of partial differential equations. This work investigates applying Gaussian process modeling to a deterministic function from prediction and uncertainty quantification perspectives, where the Gaussian process model is misspecified. Specifically, we consider the case where the underlying function is fixed and from a reproducing kernel Hilbert space generated by some kernel function, and the same kernel function is used in the Gaussian process modeling as the correlation function for prediction and uncertainty quantification. While upper bounds and the optimal convergence rate of prediction in the Gaussian process modeling have been extensively studied in the literature, a comprehensive exploration of convergence rates?…",Article,https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-15/issue-2/On-the-inference-of-applying-Gaussian-process-modeling-to-a/10.1214/21-EJS1912.short,19,wang2021inference,"Gaussian process modeling, computer experiments, uncertainty quantification, Matern correlation functions"
2022,arXiv preprint arXiv:2205.14814,Your contrastive learning is secretly doing stochastic neighbor embedding,"Tianyang Hu, Zhili Liu, Fengwei Zhou, Wenjia Wang, Weiran Huang",Tianyang Hu,English,"Contrastive learning, especially self-supervised contrastive learning (SSCL), has achieved great success in extracting powerful features from unlabeled data. In this work, we contribute to the theoretical understanding of SSCL and uncover its connection to the classic data visualization method, stochastic neighbor embedding (SNE), whose goal is to preserve pairwise distances. From the perspective of preserving neighboring information, SSCL can be viewed as a special case of SNE with the input space pairwise similarities specified by data augmentation. The established correspondence facilitates deeper theoretical understanding of learned features of SSCL, as well as methodological guidelines for practical improvement. Specifically, through the lens of SNE, we provide novel analysis on domain-agnostic augmentations, implicit bias and robustness of learned features. To illustrate the practical advantage, we demonstrate that the modifications from SNE to -SNE can also be adopted in the SSCL setting, achieving significant improvement in both in-distribution and out-of-distribution generalization.",Article,https://arxiv.org/abs/2205.14814,17,hu2022your,None
2022,Neural Information Processing Systems 2022,Understanding Square Loss in Training Overparametrized Neural Network Classifiers,"Tianyang Hu*, Jun Wang*, Wenjia Wang*, Zhenguo Li, (*equal contributions)",Tianyang Hu*,English,"Deep learning has achieved many breakthroughs in modern classification tasks. Numerous architectures have been proposed for different data structures but when it comes to the loss function, the cross-entropy loss is the predominant choice. Recently, several alternative losses have seen revived interests for deep classifiers. In particular, empirical evidence seems to promote square loss but a theoretical justification is still lacking. In this work, we contribute to the theoretical understanding of square loss in classification by systematically investigating how it performs for overparametrized neural networks in the neural tangent kernel (NTK) regime. Interesting properties regarding the generalization error, robustness, and calibration error are revealed. We consider two cases, according to whether classes are separable or not. In the general non-separable case, fast convergence rate is established for both misclassification rate and calibration error. When classes are separable, the misclassification rate improves to be exponentially fast. Further, the resulting margin is proven to be lower bounded away from zero, providing theoretical guarantees for robustness. We expect our findings to hold beyond the NTK regime and translate to practical settings. To this end, we conduct extensive empirical studies on practical neural networks, demonstrating the effectiveness of square loss in both synthetic low-dimensional data and real image data. Comparing to cross-entropy, square loss has comparable generalization error but noticeable advantages in robustness and model calibration.",Conference paper,https://arxiv.org/abs/2112.03657,10,hu2021understanding,None
2020,IEEE/ASME Transactions on Mechatronics,Neural network Gaussian process considering input uncertainty for composite structure assembly,"Cheolhei Lee, Jianguo Wu, Wenjia Wang, Xiaowei Yue",Cheolhei Lee,English,"Developing machine-learning-enabled smart manufacturing is promising for a composite structure assembly process. To improve production quality and efficiency of the assembly process, accurate predictive analysis on dimensional deviations and residual stress of the composite structures is required. The novel composite structure assembly involves two challenges: 1) the highly nonlinear and anisotropic properties of composite materials; and 2) inevitable uncertainty in the assembly process. To overcome those problems, in this article, we propose a neural network Gaussian process model considering input uncertainty for composite structure assembly. Deep architecture of our model allows us to approximate a complex process better, and consideration of input uncertainty enables robust modeling with complete incorporation of the process uncertainty. Based on simulation and case study, the neural network?…",Article,https://ieeexplore.ieee.org/abstract/document/9272688/,17,lee2020neural,"Composite structure assembly, Gaussian process, input uncertainty, neural network"
2022,SIAM/ASA Journal on Uncertainty Quantification,Gaussian processes with input location error and applications to the composite parts assembly process,"Wenjia Wang, Xiaowei Yue, Benjamin Haaland, CF Jeff Wu",Wenjia Wang,English," This paper investigates Gaussian process modeling with input location error, where the inputs are corrupted by noise. Here, the best linear unbiased predictor for two cases is considered, according to whether there is noise at the target location or not. We show that the mean squared prediction error converges to a nonzero constant if there is noise at the target location, and we provide an upper bound of the mean squared prediction error if there is no noise at the target location. We investigate the use of stochastic Kriging in the prediction of Gaussian processes with input location error and show that stochastic Kriging is a good approximation when the sample size is large. Several numerical examples are given to illustrate the results, and a case study on the assembly of composite parts is presented. Technical proofs are provided in the appendices.",Article,https://epubs.siam.org/doi/abs/10.1137/20M1312447,11,wang2022gaussian," Gaussian process, input location error, stochastic Kriging, composite parts assembly"
2023,IEEE Transactions on Knowledge and Data Engineering,Differentiable and scalable generative adversarial models for data imputation,"Yangyang Wu, Jun Wang, Xiaoye Miao, Wenjia Wang, Jianwei Yin",Yangyang Wu,English,"Data imputation has been extensively explored to solve the missing data problem. The dramatically increasing volume of incomplete data makes the imputation models computationally infeasible in many real-life applications. In this paper, we propose an effective scalable imputation system named    to significantly speed up the training of the differentiable generative adversarial imputation models under accuracy-guarantees for large-scale incomplete data.    consists of two modules,  differentiable imputation modeling  (DIM) and  sample size estimation  (SSE). DIM leverages a new  masking Sinkhorn  divergence function to make an arbitrary generative adversarial imputation model differentiable, while for such a differentiable imputation model, SSE can estimate an appropriate sample size to ensure the user-specified imputation accuracy of the final model. Moreover,    can also accelerate the autoencoder?…",Article,https://ieeexplore.ieee.org/abstract/document/10175548/,8,wu2023differentiable,"Data imputation, generative adversarial network,large-scale incomplete data"
2023,arXiv preprint arXiv:2305.03531,Random smoothing regularization in kernel gradient descent learning,"Liang Ding, Tianyang Hu, Jiahang Jiang, Donghao Li, Wenjia Wang, Yuan Yao",Liang Ding,English,"Random smoothing data augmentation is a unique form of regularization that can prevent overfitting by introducing noise to the input data, encouraging the model to learn more generalized features. Despite its success in various applications, there has been a lack of systematic study on the regularization ability of random smoothing. In this paper, we aim to bridge this gap by presenting a framework for random smoothing regularization that can adaptively and effectively learn a wide range of ground truth functions belonging to the classical Sobolev spaces. Specifically, we investigate two underlying function spaces: the Sobolev space of low intrinsic dimension, which includes the Sobolev space in -dimensional Euclidean space or low-dimensional sub-manifolds as special cases, and the mixed smooth Sobolev space with a tensor structure. By using random smoothing regularization as novel convolution-based smoothing kernels, we can attain optimal convergence rates in these cases using a kernel gradient descent algorithm, either with early stopping or weight decay. It is noteworthy that our estimator can adapt to the structural assumptions of the underlying data and avoid the curse of dimensionality. This is achieved through various choices of injected noise distributions such as Gaussian, Laplace, or general polynomial noises, allowing for broad adaptation to the aforementioned structural assumptions of the underlying data. The convergence rate depends only on the effective dimension, which may be significantly smaller than the actual data dimension. We conduct numerical experiments on simulated data to validate our theoretical results.",Article,https://arxiv.org/abs/2305.03531,7,ding2023random,None
2023,arXiv preprint arXiv:2301.12189,Deciphering the projection head: Representation evaluation self-supervised learning,"Jiajun Ma, Tianyang Hu, Wenjia Wang",Jiajun Ma,English,"Self-supervised learning (SSL) aims to learn intrinsic features without labels. Despite the diverse architectures of SSL methods, the projection head always plays an important role in improving the performance of the downstream task. In this work, we systematically investigate the role of the projection head in SSL. Specifically, the projection head targets the uniformity part of SSL, which pushes the dissimilar samples away from each other, thus enabling the encoder to focus on extracting semantic features. Based on this understanding, we propose a Representation Evaluation Design (RED) in SSL models in which a shortcut connection between the representation and the projection vectors is built. Extensive experiments with different architectures, including SimCLR, MoCo-V2, and SimSiam, on various datasets, demonstrate that the representation evaluation design can consistently improve the baseline models in the downstream tasks. The learned representation from the RED-SSL models shows superior robustness to unseen augmentations and out-of-distribution data.",Article,https://arxiv.org/abs/2301.12189,6,ma2023deciphering,"self-supervised learning,projection head,contrastive learning"
2021,Journal of Multivariate Analysis 185,Eigenvector-based sparse canonical correlation analysis: Fast computation for estimation of multiple canonical vectors,"Wenjia Wang, Yi-Hui Zhou",Wenjia Wang,English,"Classical canonical correlation analysis (CCA) requires matrices to be low dimensional, i.e. the number of features cannot exceed the sample size. Recent developments in CCA have mainly focused on the high-dimensional setting, where the number of features in both matrices under analysis greatly exceeds the sample size. These approaches impose penalties in the optimization problems that are needed to be solve iteratively, and estimate multiple canonical vectors sequentially. In this work, we provide an explicit link between sparse multiple regression with sparse canonical correlation analysis, and an efficient algorithm that can estimate multiple canonical pairs simultaneously rather than sequentially. Furthermore, the algorithm naturally allows parallel computing. These properties make the algorithm much efficient. We provide theoretical results on the consistency of canonical pairs. The algorithm and?…",Article,https://www.sciencedirect.com/science/article/pii/S0047259X21000592,6,wang2021eigenvector,None
2024,Advances in Neural Information Processing Systems 36,Complexity matters: Rethinking the latent space for generative modeling,"Tianyang Hu, Fei Chen, Haonan Wang, Jiawei Li, Wenjia Wang, Jiacheng Sun, Zhenguo Li",Tianyang Hu,English,"In generative modeling, numerous successful approaches leverage a low-dimensional latent space, eg, Stable Diffusion models the latent space induced by an encoder and generates images through a paired decoder. Although the selection of the latent space is empirically pivotal, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel"" distance"" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propose a two-stage training strategy called Decoupled Autoencoder (DAE), where the encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second stage while the actual decoder is being trained. DAE can improve the latent distribution and as a result, improve the generative performance. Our theoretical analyses are corroborated by comprehensive experiments on various models such as VQGAN and Diffusion Transformer, where our modifications yield significant improvements in sample quality with decreased model complexity.",Article,https://proceedings.neurips.cc/paper_files/paper/2023/hash/5e8023f07625374c6fdf3aa08bb38e0e-Abstract-Conference.html,5,hu2024complexity,None
2024,arXiv preprint arXiv:2402.04059,Deep Learning for Multivariate Time Series Imputation: A Survey,"Jun Wang, Wenjie Du, Wei Cao, Keli Zhang, Wenjia Wang, Yuxuan Liang, Qingsong Wen",Jun Wang,English,"The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis. Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks. In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods. First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations. We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks. Finally, the open issues for future research on multivariate time series imputation are pointed out. All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be found in the GitHub repository~\url{https://github.com/WenjieDu/Awesome\_Imputation}.",Article,https://arxiv.org/abs/2402.04059,4,wang2024deep,None
2023,arXiv preprint arXiv:2310.11311,Elucidating the design space of classifier-guided diffusion generation,"Jiajun Ma, Tianyang Hu, Wenjia Wang, Jiacheng Sun",Jiajun Ma,English,"Guidance in conditional diffusion generation is of great importance for sample quality and controllability. However, existing guidance schemes are to be desired. On one hand, mainstream methods such as classifier guidance and classifier-free guidance both require extra training with labeled data, which is time-consuming and unable to adapt to new conditions. On the other hand, training-free methods such as universal guidance, though more flexible, have yet to demonstrate comparable performance. In this work, through a comprehensive investigation into the design space, we show that it is possible to achieve significant performance improvements over existing guidance schemes by leveraging off-the-shelf classifiers in a training-free fashion, enjoying the best of both worlds. Employing calibration as a general guideline, we propose several pre-conditioning techniques to better exploit pretrained off-the-shelf classifiers for guiding diffusion generation. Extensive experiments on ImageNet validate our proposed method, showing that state-of-the-art diffusion models (DDPM, EDM, DiT) can be further improved (up to 20%) using off-the-shelf classifiers with barely any extra computational cost. With the proliferation of publicly available pretrained classifiers, our proposed approach has great potential and can be readily scaled up to text-to-image generation tasks. The code is available at https://github.com/AlexMaOLS/EluCD/tree/main.",Article,https://arxiv.org/abs/2310.11311,4,ma2023elucidating,None
2022,arXiv preprint arXiv:2201.02958,Smooth Nested Simulation: Bridging Cubic and Square Root Convergence Rates in High Dimensions,"Wenjia Wang*, Yanyuan Wang*, Xiaowei Zhang*, (*equal contributions)",Wenjia Wang*,English,"Nested simulation concerns estimating functionals of a conditional expectation via simulation. In this paper, we propose a new method based on kernel ridge regression to exploit the smoothness of the conditional expectation as a function of the multidimensional conditioning variable. Asymptotic analysis shows that the proposed method can effectively alleviate the curse of dimensionality on the convergence rate as the simulation budget increases, provided that the conditional expectation is sufficiently smooth. The smoothness bridges the gap between the cubic root convergence rate (that is, the optimal rate for the standard nested simulation) and the square root convergence rate (that is, the canonical rate for the standard Monte Carlo simulation). We demonstrate the performance of the proposed method via numerical examples from portfolio risk management and input uncertainty quantification.This paper was?…",Article,https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2022.00204,4,wang2024smooth,"nested simulation, smoothness, kernel ridge regression, convergence rate"
2022,arXiv preprint arXiv:2201.01682,Functional-Input Gaussian Processes with Applications to Inverse Scattering Problems,"Chih-Li Sung*, Wenjia Wang*, Fioralba Cakoni, Isaac Harris, Ying Hung, (*equal contributions)",Chih-Li Sung*,English,"Surrogate modeling based on Gaussian processes (GPs) has received increasing attention in the analysis of complex problems in science and engineering. Despite extensive studies on GP modeling, the developments for functional inputs are scarce. Motivated by an inverse scattering problem in which functional inputs representing the support and material properties of the scatterer are involved in the partial differential equations, a new class of kernel functions for functional inputs is introduced for GPs. Based on the proposed GP models, the asymptotic convergence properties of the resulting mean squared prediction errors are derived and the finite sample performance is demonstrated by numerical examples. In the application to inverse scattering, a surrogate model is constructed with functional inputs, which is crucial to recover the reflective index of an inhomogeneous isotropic scattering region of interest for a given far-field pattern.",Article,https://arxiv.org/abs/2201.01682,4,sung2022functional,"Computer experiments, surrogate model, uncertainty quantification, scalar-on-function regression, functional data analysis"
2023,2023 IEEE International Conference on Data Mining (ICDM),Counterclr: Counterfactual contrastive learning with non-random missing data in recommendation,"Jun Wang, Haoxuan Li, Chi Zhang, Dongxu Liang, Enyun Yu, Wenwu Ou, Wenjia Wang",Jun Wang,English,"Recommender systems are designed to learn user preferences from observed feedback and comprise many fundamental tasks, such as rating prediction and post-click conversion rate (pCVR) prediction. However, the observed feedback usually suffer from two issues: selection bias and data sparsity, where biased and insufficient feedback seriously degrade the performance of recommender systems in terms of accuracy and ranking. Existing solutions for handling the issues, such as data imputation and inverse propensity score, are highly susceptible to additional trained imputation or propensity models. In this work, we propose a novel counterfactual contrastive learning framework for recommendation, named CounterCLR, to tackle the problem of non-random missing data by exploiting the advances in contrast learning. Specifically, the proposed CounterCLR employs a deep representation network, called?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/10415736/,3,wang2023counterclr,"recommendation system, non-random missing data, causal inference, contrastive learning"
2020,arXiv preprint arXiv:2006.03696,High-dimensional non-parametric density estimation in mixed smooth sobolev spaces,"Liang Ding, Lu Zou, Wenjia Wang, Shahin Shahrampour, Rui Tuo",Liang Ding,English,"Density estimation plays a key role in many tasks in machine learning, statistical inference, and visualization. The main bottleneck in high-dimensional density estimation is the prohibitive computational cost and the slow convergence rate. In this paper, we propose novel estimators for high-dimensional non-parametric density estimation called the adaptive hyperbolic cross density estimators, which enjoys nice convergence properties in the mixed smooth Sobolev spaces. As modifications of the usual Sobolev spaces, the mixed smooth Sobolev spaces are more suitable for describing high-dimensional density functions in some applications. We prove that, unlike other existing approaches, the proposed estimator does not suffer the curse of dimensionality under Integral Probability Metric, including Holder Integral Probability Metric, where Total Variation Metric and Wasserstein Distance are special cases. Applications of the proposed estimators to generative adversarial networks (GANs) and goodness of fit test for high-dimensional data are discussed to illustrate the proposed estimator's good performance in high-dimensional problems. Numerical experiments are conducted and illustrate the efficiency of our proposed method.",Article,https://arxiv.org/abs/2006.03696,3,ding2020high,None
2020,arXiv preprint arXiv:2005.01559,Reduced Rank Multivariate Kernel Ridge Regression,"Wenjia Wang, Yi-Hui Zhou",Wenjia Wang,English,"In the multivariate regression, also referred to as multi-task learning in machine learning, the goal is to recover a vector-valued function based on noisy observations. The vector-valued function is often assumed to be of low rank. Although the multivariate linear regression is extensively studied in the literature, a theoretical study on the multivariate nonlinear regression is lacking. In this paper, we study reduced rank multivariate kernel ridge regression, proposed by \cite{mukherjee2011reduced}. We prove the consistency of the function predictor and provide the convergence rate. An algorithm based on nuclear norm relaxation is proposed. A few numerical examples are presented to show the smaller mean squared prediction error comparing with the elementwise univariate kernel ridge regression.",Article,https://arxiv.org/abs/2005.01559,2,wang2020reduced,None
2020,he 25th International Conference on Artificial Intelligence and Statistics,Uncertainty Quantification for Bayesian Optimization,"Rui Tuo*, Wenjia Wang*, (*equal contributions)",Rui Tuo*,English,"Bayesian optimization is a class of global optimization techniques. In Bayesian optimization, the underlying objective function is modeled as a realization of a Gaussian process. Although the Gaussian process assumption implies a random distribution of the Bayesian optimization outputs, quantification of this uncertainty is rarely studied in the literature. In this work, we propose a novel approach to assess the output uncertainty of Bayesian optimization algorithms, which proceeds by constructing confidence regions of the maximum point (or value) of the objective function. These regions can be computed efficiently, and their confidence levels are guaranteed by the uniform error bounds for sequential Gaussian process regression newly developed in the present work. Our theory provides a unified uncertainty quantification framework for all existing sequential sampling policies and stopping criteria.",Conference paper,https://proceedings.mlr.press/v151/tuo22a.html,2,tuo2022uncertainty,None
2023,arXiv preprint arXiv:2310.11756,Improved Convergence Rate of Nested Simulation with LSE on Sieve,"Ruoxue Liu, Liang Ding, Wenjia Wang, Lu Zou",Ruoxue Liu,English,"Nested simulation encompasses the estimation of functionals linked to conditional expectations through simulation techniques. In this paper, we treat conditional expectation as a function of the multidimensional conditioning variable and provide asymptotic analyses of general Least Squared Estimators on sieve, without imposing specific assumptions on the function's form. Our study explores scenarios in which the convergence rate surpasses that of the standard Monte Carlo method and the one recently proposed based on kernel ridge regression. We also delve into the conditions that allow for achieving the best possible square root convergence rate among all methods. Numerical experiments are conducted to support our statements.",Article,https://arxiv.org/abs/2310.11756,0,liu2023improved,"nested simulation, least squared estimation, sieve, convergence rate"
2022,Mathematics,A Double Penalty Model for Ensemble Learning,"Wenjia Wang, Yi-Hui Zhou",Wenjia Wang,English,"Modern statistical learning techniques often include learning ensembles, for which the combination of multiple separate prediction procedures (ensemble components) can improve prediction accuracy. Although ensemble approaches are widely used, work remains to improve our understanding of the theoretical underpinnings of aspects such as identifiability and relative convergence rates of the ensemble components. By considering ensemble learning for two learning ensemble components as a double penalty model, we provide a framework to better understand the relative convergence and identifiability of the two components. In addition, with appropriate conditions the framework provides convergence guarantees for a form of residual stacking when iterating between the two components as a cyclic coordinate ascent procedure. We conduct numerical experiments on three synthetic simulations and two real world datasets to illustrate the performance of our approach, and justify our theory.",Article,https://www.mdpi.com/2227-7390/10/23/4532,0,wang2022double,"double penalty model, interpretability, partially linear model, separability"
