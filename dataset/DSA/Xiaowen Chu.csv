Year,Sources,Name,Authors,First Author,Chinese/English,Abstract,Venues,doi,Citation,Id,Keywords
2021,Knowledge-Based Systems 212,AutoML: A Survey of the State-of-the-Art,"Xin He, Kaiyong Zhao, Xiaowen Chu",Xin He,English,"Deep learning (DL) techniques have obtained remarkable achievements on various tasks, such as image recognition, object detection, and language modeling. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering its wide application. Meanwhile, automated machine learning (AutoML) is a promising solution for building a DL system without human assistance and is being extensively studied. This paper presents a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. According to the DL pipeline, we introduce AutoML methods – covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS) – with a particular focus on NAS, as it is currently a hot sub-topic of AutoML. We summarize the representative NAS algorithms’ performance on the CIFAR-10 and ImageNet datasets and further?…",Article,https://www.sciencedirect.com/science/article/pii/S0950705120307516,1566,he2021automl,"deep learning, automated machine learning (AutoML), neural architecture search (NAS), hyperparameter optimization (HPO)"
2020,IEEE ICDCS 2020,FMore: An Incentive Scheme of Multi-dimensional Auction for Federated Learning in MEC,"Rongfei Zeng, Shixun Zhang, Jiaqi Wang, Xiaowen Chu",Rongfei Zeng,English,"Promising federated learning coupled with Mobile Edge Computing (MEC) is considered as one of the most promising solutions to the AI-driven service provision. Plenty of studies focus on federated learning from the performance and security aspects, but they neglect the incentive mechanism. In MEC, edge nodes would not like to voluntarily participate in learning, and they differ in the provision of multi-dimensional resources, both of which might deteriorate the performance of federated learning. Also, lightweight schemes appeal to edge nodes in MEC. These features require the incentive mechanism to be well designed for MEC. In this paper, we present an incentive mechanism FMore with multi-dimensional procurement auction of K winners. Our proposal FMore not only is lightweight and incentive compatible, but also encourages more high-quality edge nodes with low cost to participate in learning and?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9355731/,185,zeng2020fmore,"Mobile edge computing, multi-dimensional auction, federated learning, incentive mechanism"
2020,arXiv preprint arXiv:2002.07526,A survey of deep learning techniques for neural machine translation,"Shuoheng Yang, Yuxin Wang, Xiaowen Chu",Shuoheng Yang,English,"In recent years, natural language processing (NLP) has got great development with deep learning techniques. In the sub-field of machine translation, a new approach named Neural Machine Translation (NMT) has emerged and got massive attention from both academia and industry. However, with a significant number of researches proposed in the past several years, there is little work in investigating the development process of this new technology trend. This literature survey traces back the origin and principal development timeline of NMT, investigates the important branches, categorizes different research orientations, and discusses some future research trends in this field.",Conference paper,https://arxiv.org/abs/2002.07526,177,yang2020survey,"Neural Machine Translation, Deep Learning,Attention Mechanism"
2021,IEEE Transactions on Network Science and Engineering,Vfchain: Enabling verifiable and auditable federated learning via blockchain systems,"Zhe Peng, Jianliang Xu, Xiaowen Chu, Shang Gao, Yuan Yao, Rong Gu, Yuzhe Tang",Zhe Peng,English,"Advanced artificial intelligence techniques, such as federated learning, has been applied to broad areas, e.g., image classification, speech recognition, smart city, and healthcare. Despite intensive research on federated learning, existing schemes are vulnerable to attacks and can hardly meet the security requirements for real-world applications. The problem of designing a secure federated learning framework to ensure the correctness of training procedure has not been sufficiently studied and remains open. In this paper, we propose VFChain, a verifiable and auditable federated learning framework based on the blockchain system. First, to provide the verifiability, a committee is selected through the blockchain to collectively aggregate models and record verifiable proofs in the blockchain. Then, to provide the auditability, a novel authenticated data structure is proposed for blockchain to improve the search efficiency?…",Article,https://ieeexplore.ieee.org/abstract/document/9321132/,150,peng2021vfchain,"Federated Learning, model verification, auditable training, blockchain"
2020,arXiv preprint arXiv:2003.06307,Communication-efficient distributed deep learning: A comprehensive survey,"Zhenheng Tang, Shaohuai Shi, Xiaowen Chu, Wei Wang, Bo Li",Zhenheng Tang,English,"Distributed deep learning (DL) has become prevalent in recent years to reduce training time by leveraging multiple computing devices (e.g., GPUs/TPUs) due to larger models and datasets. However, system scalability is limited by communication becoming the performance bottleneck. Addressing this communication issue has become a prominent research topic. In this paper, we provide a comprehensive survey of the communication-efficient distributed training algorithms, focusing on both system-level and algorithmic-level optimizations. We first propose a taxonomy of data-parallel distributed training algorithms that incorporates four primary dimensions: communication synchronization, system architectures, compression techniques, and parallelism of communication and computing tasks. We then investigate state-of-the-art studies that address problems in these four dimensions. We also compare the convergence rates of different algorithms to understand their convergence speed. Additionally, we conduct extensive experiments to empirically compare the convergence performance of various mainstream distributed training algorithms. Based on our system-level communication cost analysis, theoretical and experimental convergence speed comparison, we provide readers with an understanding of which algorithms are more efficient under specific distributed environments. Our research also extrapolates potential directions for further optimizations.",Conference paper,https://arxiv.org/abs/2003.06307,121,tang2020communication,"Distributed Deep Learning, Efficient Communication"
2021,arXiv preprint arXiv:2106.15406,A comprehensive survey of incentive mechanism for federated learning,"Rongfei Zeng, Chao Zeng, Xingwei Wang, Bo Li, Xiaowen Chu",Rongfei Zeng,English,"Federated learning utilizes various resources provided by participants to collaboratively train a global model, which potentially address the data privacy issue of machine learning. In such promising paradigm, the performance will be deteriorated without sufficient training data and other resources in the learning process. Thus, it is quite crucial to inspire more participants to contribute their valuable resources with some payments for federated learning. In this paper, we present a comprehensive survey of incentive schemes for federate learning. Specifically, we identify the incentive problem in federated learning and then provide a taxonomy for various schemes. Subsequently, we summarize the existing incentive mechanisms in terms of the main techniques, such as Stackelberg game, auction, contract theory, Shapley value, reinforcement learning, blockchain. By reviewing and comparing some impressive results, we figure out three directions for the future study.",Conference paper,https://arxiv.org/abs/2106.15406,101,zeng2021comprehensive,"Federated learning, incentive mechanism, performance improvement"
2023,IEEE Transactions on Parallel and Distributed Systems,GossipFL: A Decentralized Federated Learning Framework with Sparsified and Adaptive Communication,"Zhenheng Tang, Shaohuai Shi, Bo Li, Xiaowen Chu",Zhenheng Tang,English,"Recently, federated learning (FL) techniques have enabled multiple users to train machine learning models collaboratively without data sharing. However, existing FL algorithms suffer from the communication bottleneck due to network bandwidth pressure and/or low bandwidth utilization of the participating clients in both centralized and decentralized architectures. To deal with the communication problem while preserving the convergence performance, we introduce a communication-efficient decentralized FL framework GossipFL. In GossipFL, we 1) design a novel sparsification algorithm to enable that each client only needs to communicate with one peer with a highly sparsified model, and 2) propose a new and novel gossip matrix generation algorithm that can better utilize the bandwidth resources while preserving the convergence property. We also theoretically prove that GossipFL has convergence guarantees. We?…",Article,https://ieeexplore.ieee.org/abstract/document/9996127/,49,tang2022gossipfl,"Deep learning, federated learning, communication efficiency"
2021,AAAI 2021,Automated Model Design and Benchmarking of 3D Deep Learning Models for COVID-19 Detection with Chest CT Scans,"Xin He, Shihao Wang, Xiaowen Chu, Shaohuai Shi, Jiangping Tang, Xin Liu, Chenggang Yan, Jiyong Zhang, Guiguang Ding",Xin He,English,"The COVID-19 pandemic has spread globally for several months. Because its transmissibility and high pathogenicity seriously threaten people's lives, it is crucial to accurately and quickly detect COVID-19 infection. Many recent studies have shown that deep learning (DL) based solutions can help detect COVID-19 based on chest CT scans. However, most existing work focuses on 2D datasets, which may result in low quality models as the real CT scans are 3D images. Besides, the reported results span a broad spectrum on different datasets with a relatively unfair comparison. In this paper, we first use three state-of-the-art 3D models (ResNet3D101, DenseNet3D121, and MC3\_18) to establish the baseline performance on three publicly available chest CT scan datasets. Then we propose a differentiable neural architecture search (DNAS) framework to automatically search the 3D DL models for 3D chest CT scans classification and use the Gumbel Softmax technique to improve the search efficiency. We further exploit the Class Activation Mapping (CAM) technique on our models to provide the interpretability of the results. The experimental results show that our searched models (CovidNet3D) outperform the baseline human-designed models on three datasets with tens of times smaller model size and higher accuracy. Furthermore, the results also verify that CAM can be well applied in CovidNet3D for COVID-19 datasets to provide interpretability for medical diagnosis. Code: https://github. com/HKBU-HPML/CovidNet3D.",Article,https://ojs.aaai.org/index.php/AAAI/article/view/16614,42,he2021automated,"deep learning,CT scans,COVID-19"
2020,2020 International Conference on Robotics and Automation (ICRA),FADNet: A Fast and Accurate Network for Disparity Estimation,"Qiang Wang, Shaohuai Shi, Shizhen Zheng, Kaiyong Zhao, Xiaowen Chu",Qiang Wang,English,"Deep neural networks (DNNs) have achieved great success in the area of computer vision. The disparity estimation problem tends to be addressed by DNNs which achieve much better prediction accuracy in stereo matching than traditional hand-crafted feature based methods. On one hand, however, the designed DNNs require significant memory and computation resources to accurately predict the disparity, especially for those 3D convolution based networks, which makes it difficult for deployment in real-time applications. On the other hand, existing computation-efficient networks lack expression capability in large-scale datasets so that they cannot make an accurate prediction in many scenarios. To this end, we propose an efficient and accurate deep network for disparity estimation named FADNet with three main features: 1) It exploits efficient 2D based correlation layers with stacked blocks to preserve fast?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9197031/,79,wang2020fadnet,"deep neural networks,Disparity Estimation"
2022,IEEE Transactions on Network Science and Engineering,"Dissecting Mining Pools of Bitcoin Network: Measurement, Analysis and Modeling","Canhui Wang, Xiaowen Chu, Yang Qin",Canhui Wang,English,"Bitcoin network is one of the most popular blockchain systems. Mining pools are the main components of the Bitcoin network that invest a large amount of computing power to maximize their expected mining payoffs, which guarantees the security of the Bitcoin network. Although many existing works about mining pools are available, the long-term evolution of mining pools, and their effects on both the Bitcoin system and end-users, remain to be investigated. To fill this gap, we trace over 2.54 hundred thousand blocks from Feb 2016 to Nov 2020 and collect over 12 million unconfirmed transactions from Mar 2018 to Nov 2020. We then conduct a broad range of analyses, including the pool evolution, labeled transactions, and labeled blocks. We make the following observations from our measured data: 1) A few mining pools control most of the peer-to-peer network's computing power. 2) The long-term computing power?…",Article,https://ieeexplore.ieee.org/abstract/document/9907879/,4,wang2022dissecting,"Bitcoin network, mining pool, proof-of-work,mining strategy, incentive mechanism"
2020,2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS?…,Demystifying tensor cores to optimize half-precision matrix multiply,"Da Yan, Wei Wang, Xiaowen Chu",Da Yan,English,"Half-precision matrix multiply has played a key role in the training of deep learning models. The newly designed Nvidia Tensor Cores offer the native instructions for half-precision small matrix multiply, based on which Half-precision General Matrix Multiply (HGEMM) routines are developed and can be accessed through high-level APIs. In this paper, we, for the first time, demystify how Tensor Cores on NVIDIA Turing architecture work in great details, including the instructions used, the registers and data layout required, as well as the throughput and latency of Tensor Core operations. We further benchmark the memory system of Turing GPUs and conduct quantitative analysis of the performance. Our analysis shows that the bandwidth of DRAM, L2 cache and shared memory is the new bottleneck for HGEMM, whose performance is previously believed to be bound by computation. Based on our newly discovered?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9139835/,74,yan2020demystifying,"GEMM, GPU, Tensor Core, Half-precision"
2020,2020 20th IEEE/ACM International Symposium on Cluster,Benchmarking the performance and energy efficiency of AI accelerators for AI training,"Yuxin Wang, Qiang Wang, Shaohuai Shi, Xin He, Zhenheng Tang, Kaiyong Zhao, Xiaowen Chu",Yuxin Wang,English,"Deep learning has become widely used in complex AI applications. Yet, training a deep neural network (DNNs) model requires a considerable amount of calculations, long running time, and much energy. Nowadays, many-core AI accelerators (e.g., GPUs and TPUs) are designed to improve the performance of AI training. However, processors from different vendors perform dissimilarly in terms of performance and energy consumption. To investigate the differences among several popular off-the-shelf processors (i.e., Intel CPU, NVIDIA GPU, AMD GPU, and Google TPU) in training DNNs, we carry out a comprehensive empirical study on the performance and energy efficiency of these processors 1  by benchmarking a representative set of deep learning workloads, including computation-intensive operations, classical convolutional neural networks (CNNs), recurrent neural networks (LSTM), Deep Speech 2, and?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9139681/,58,wang2020benchmarking,"AI Accelerator, Deep Learning, CPU, GPU, TPU, Computation-intensive Operations, Convolution Neural Networks, Recurrent Neural Networks, Transformer, Deep Speech 2"
2020,IEEE Transactions on Parallel and Distributed Systems,GPGPU performance estimation with core and memory frequency scaling,"Qiang Wang, Xiaowen Chu",Qiang Wang,English,"Contemporary graphics processing units (GPUs) support dynamic voltage and frequency scaling to balance computational performance and energy consumption. However, accurate and straightforward performance estimation for a given GPU kernel under different frequency settings is still lacking for real hardware, which is essential to determine the best frequency configuration for energy saving. In this article, we reveal a fine-grained analytical model to estimate the execution time of GPU kernels with both core and memory frequency scaling. Compared to the cycle-level simulators, which are too slow to apply on real hardware, our model only needs simple and one-off micro-benchmarks to extract a set of hardware parameters and kernel performance counters without any source code analysis. Our experimental results show that the proposed performance model can capture the kernel performance scaling?…",Article,https://ieeexplore.ieee.org/abstract/document/9124659/,72,wang2020gpgpu,"Graphics Processing Units, Dynamic Voltage and Frequency Scaling, GPU Performance Modeling"
2020,Proceedings of the 25th ACM SIGPLAN symposium on principles and practice of?…,Optimizing batched winograd convolution on GPUs,"Da Yan, Wei Wang, Xiaowen Chu",Da Yan,English,"In this paper, we present an optimized implementation for single-precision Winograd convolution on NVIDIA Volta and Turing GPUs. Compared with the state-of-the-art Winograd convolution in cuDNN 7.6.1, our implementation achieves up to 2.13X speedup on Volta V100 and up to 2.65X speedup on Turing RTX2070. On both Volta and Turing GPUs, our implementation achieves up to 93% of device peak. Apart from analyzing and benchmarking different high-level optimization options, we also build a SASS assembler TuringAs for Volta and Turing that enables tuning the performance at the native assembly level. The new optimization opportunities uncovered by TuringAs not only improve the Winograd convolution but can also benefit CUDA compilers and native assembly programming. We have released TuringAs as an open-source software. To the best of our knowledge, this is the first public-available?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3332466.3374520,64,yan2020optimizing,"Convolution, GPU, Performance"
2020,Proc. of IEEE INFOCOM 2020,Communication-efficient distributed deep learning with merged gradient sparsification on gpus,"Shaohuai Shi, Qiang Wang, Xiaowen Chu, Bo Li, Yang Qin, Ruihao Liu, Xinxiao Zhao",Shaohuai Shi,English,"Distributed synchronous stochastic gradient descent (SGD) algorithms are widely used in large-scale deep learning applications, while it is known that the communication bottleneck limits the scalability of the distributed system. Gradient sparsification is a promising technique to significantly reduce the communication traffic, while pipelining can further overlap the communications with computations. However, gradient sparsification introduces extra computation time, and pipelining requires many layer-wise communications which introduce significant communication startup overheads. Merging gradients from neighbor layers could reduce the startup overheads, but on the other hand it would increase the computation time of sparsification and the waiting time for the gradient computation. In this paper, we formulate the trade-off between communications and computations (including backward computation and?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9155269/,64,shi2020communication,"Distributed Deep Learning, Gradient Communication, Merged Gradient"
2021,arXiv preprint arXiv:2111.11066,Fedcv: a federated learning framework for diverse computer vision tasks,"Chaoyang He, Alay Dilipbhai Shah, Zhenheng Tang, Di Fan1Adarshan Naiynar Sivashunmugam, Keerti Bhogaraju, Mita Shimpi, Li Shen, Xiaowen Chu, Mahdi Soltanolkotabi, Salman Avestimehr",Chaoyang He,English,"Federated Learning (FL) is a distributed learning paradigm that can learn a global or personalized model from decentralized datasets on edge devices. However, in the computer vision domain, model performance in FL is far behind centralized training due to the lack of exploration in diverse tasks with a unified FL framework. FL has rarely been demonstrated effectively in advanced computer vision tasks such as object detection and image segmentation. To bridge the gap and facilitate the development of FL for computer vision tasks, in this work, we propose a federated learning library and benchmarking framework, named FedCV, to evaluate FL on the three most representative computer vision tasks: image classification, image segmentation, and object detection. We provide non-I.I.D. benchmarking datasets, models, and various reference FL algorithms. Our benchmark study suggests that there are multiple challenges that deserve future exploration: centralized training tricks may not be directly applied to FL; the non-I.I.D. dataset actually downgrades the model accuracy to some degree in different tasks; improving the system efficiency of federated training is challenging given the huge number of parameters and the per-client memory cost. We believe that such a library and benchmark, along with comparable evaluation settings, is necessary to make meaningful progress in FL on computer vision tasks. FedCV is publicly available: https://github.com/FedML-AI/FedCV.",Article,https://arxiv.org/abs/2111.11066,64,he2021fedcv,"Federated Learning,benchmark,computer vision"
2020,BlockApp 2020,Performance Characterization and Bottleneck Analysis of Hyperledger Fabric,"Canhui Wang, Xiaowen Chu",Canhui Wang,English,"Hyperledger Fabric is a popular open-source project for deploying permissioned blockchains. Many performance characteristics of the latest Hyperledger Fabric (e.g., performance characteristics of each phase, the impacts of ordering services, bottleneck and scalability) are still not well understood due to the performance complexity of distributed systems. We conducted a thorough performance evaluation on the first long term support release of Hyperledger Fabric. We studied the performance characteristics of each phase, including execute, order, and the validate phase, according to Hyperledger Fabric's new execute-order-validate architecture. We also studied the ordering services, including Solo, Kafka, and Raft. Our experimental results showed some findings as follows. 1) The execution phase exhibited a good scalability under the OR endorsement policy but not with the AND endorsement policy. 2) We were?…",Article,https://ieeexplore.ieee.org/abstract/document/9355625/,61,wang2020performance,"Blockchain, Hyperledger Fabric, Benchmarking,Performance Evaluation"
2022,The 39th International Conference on Machine Learning (ICML 2022),Virtual Homogeneity Learning: Defending against Data Heterogeneity in Federated Learning,"Zhenheng Tang, Yonggang Zhang, Shaohuai Shi, Xin He, Bo Han, Xiaowen Chu",Zhenheng Tang,English,"In federated learning (FL), model performance typically suffers from client drift induced by data heterogeneity, and mainstream works focus on correcting client drift. We propose a different approach named virtual homogeneity learning (VHL) to directly “rectify” the data heterogeneity. In particular, VHL conducts FL with a virtual homogeneous dataset crafted to satisfy two conditions: containing no private information and being separable. The virtual dataset can be generated from pure noise shared across clients, aiming to calibrate the features from the heterogeneous clients. Theoretically, we prove that VHL can achieve provable generalization performance on the natural distribution. Empirically, we demonstrate that VHL endows FL with drastically improved convergence speed and generalization performance. VHL is the first attempt towards using a virtual dataset to address data heterogeneity, offering new and effective means to FL.",Conference paper,https://proceedings.mlr.press/v162/tang22d.html,63,tang2022virtual,"virtual homogeneity learning,Federated Learning"
2021,Proceedings of the 2021 international conference on management of data,P2B-Trace: Privacy-Preserving Blockchain-based Contact Tracing to Combat Pandemics,"Zhe Peng, Cheng Xu, Haixin Wang, Jinbin Huang, Jianliang Xu, Xiaowen Chu",Zhe Peng,English,"The eruption of a pandemic, such as COVID-19, can cause an unprecedented global crisis. Contact tracing, as a pillar of communicable disease control in public health for decades, has shown its effectiveness on pandemic control. Despite intensive research on contact tracing, existing schemes are vulnerable to attacks and can hardly simultaneously meet the requirements of data integrity and user privacy. The design of a privacy-preserving contact tracing framework to ensure the integrity of the tracing procedure has not been sufficiently studied and remains a challenge. In this paper, we propose P2B-Trace, a privacy-preserving contact tracing initiative based on blockchain. First, we design a decentralized architecture with blockchain to record an authenticated data structure of the user's contact records, which prevents the user from intentionally modifying his local records afterward. Second, we develop a zero?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3448016.3459237,60,peng2021p2b,"Contact tracing, Integrity, Privacy-preserving, Blockchain"
2021,MLSys 2021,Towards Scalable Distributed Training of Deep Learning on Public Cloud Clusters,"Shaohuai Shi, Xianhao Zhou, Shutao Song, Xingyao Wang, Zilin Zhu, Xue Huang, Xinan Jiang, Feihu Zhou, Zhenyu Guo, Liqiang Xie, Rui Lan, Xianbin Ouyang, Yan Zhang, Jieqian Wei, Jing Gong, Weiliang Lin, Ping Gao, Peng Meng, Xiaomin Xu, Chenyang Guo, Bo Yang, Zhibo Chen, Yongjian Wu, Xiaowen Chu",Shaohuai Shi,English,"Distributed training techniques have been widely deployed in large-scale deep models training on dense-GPU clusters. However, on public cloud clusters, due to the moderate inter-connection bandwidth between instances, traditional state-of-the-art distributed training systems cannot scale well in training large-scale models. In this paper, we propose a new computing and communication efficient top-k sparsification communication library for distributed training. To further improve the system scalability, we optimize I/O by proposing a simple yet efficient multi-level data caching mechanism and optimize the update operation by introducing a novel parallel tensor operator. Experimental results on a 16-node Tencent Cloud cluster (each node with 8 Nvidia Tesla V100 GPUs) show that our system achieves 25%-40% faster than existing state-of-the-art systems on CNNs and Transformer. We finally break the record on DAWNBench on training ResNet-50 to 93% top-5 accuracy on ImageNet.",Article,https://proceedings.mlsys.org/paper_files/paper/2021/hash/25a3192c804d6b1c7d309c0155d3aa1a-Abstract.html,51,shi2021towards,"stochastic gradient descent,deep learning,cloud cluster"
2021,2021 IEEE International Conference on Multimedia and Expo (ICME),Irs: A large naturalistic indoor robotics stereo dataset to train deep models for disparity and surface normal estimation,"Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao, Xiaowen Chu",Qiang Wang,English,"Indoor robotics applications heavily rely on scene understanding and reconstruction. Compared to monocular vision, stereo vision methods are more promising to produce accurate geometrical information, such as surface normal and depth/disparity. Besides, deep learning models have shown their superior performance in stereo vision tasks. However, existing stereo datasets rarely contain high-quality surface normal and disparity ground truth, hardly satisfying the demand of training a prospective deep model. To this end, we introduce a large-scale indoor robotics stereo (IRS) dataset with over 100K stereo images and high-quality surface normal and disparity maps. Leveraging the advanced techniques of our customized rendering engine, the dataset is considerably close to the real-world scenes. Besides, we present DTN-Net, a two-stage deep model for surface normal estimation. Extensive experiments show?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9428423/,46,wang2021irs,"indoor robotics stereo, deep learning"
2020,IEEE Network,A quantitative survey of communication optimizations in distributed deep learning,"Shaohuai Shi, Zhenheng Tang, Xiaowen Chu, Chengjian Liu, Wei Wang, Bo Li",Shaohuai Shi,English,"Nowadays, large and complex deep learning (DL) models are increasingly trained in a distributed manner across multiple worker machines, in which extensive communications between workers pose serious scaling problems. In this article, we present a quantitative survey of communication optimization techniques for data parallel distributed DL. We first identify the major communication challenges and classify the existing solutions into three levels, namely the learning algorithm, the system architecture, and the network infrastructure. We present the state-of-the-art communication optimization techniques and conduct a comparative study of seven common lossless distributed DL methods on a 32-GPU cluster with 100Gb/s InfiniBand (IB). We show that the DL models with low model intensity (such as BERT and BERT-Large) are difficult to scale out even with the best available lossless algorithm over 100Gb/s IB?…",Article,https://ieeexplore.ieee.org/abstract/document/9275615/,39,shi2020quantitative,"deep learning, data parallel distributed,GPU"
2021,Process Safety and Environmental Protection 155,Deep learning identifies leak in water pipeline system using transient frequency response,"Ziyuan Liao, Hexiang Yan, Zhenheng Tang, Xiaowen Chu, Tao Tao",Ziyuan Liao,English,"Pipeline leak identification method using transient frequency response (TFR) has been researched in the past two decades. To extend this method to a more general water pipeline system with hydraulic uncertainties, this work (1) introduces deep learning (DL) into the TFR-based leak identification framework and (2) develops extended TFR equations in matrix form for DL learning set generation. In this framework, TFR equations are firstly solved in a pre-calibrated hydraulic model of the system to extract frequency response function (FRF) for the training set preparation. Then the simulated FRFs are fed to train fully linear DenseNet (FL-DenseNet) for feature recognition. Finally, the measured FRF of the system is fed to the trained FL-DenseNet to identify a leak to a pipe in the suspected leak area. A study on a hypothetical small system shows that the proposed framework has robustness against uncertainties of?…",Article,https://www.sciencedirect.com/science/article/pii/S0957582021005061,33,liao2021deep,"Leak identification,Deep learning,Frequency response,Water pipelines"
2021,Neurocomputing 462,Leveraging graph neural networks for point-of-interest recommendations,"Jiyong Zhang, Xin Liu, Xiaofei Zhou, Xiaowen Chu",Jiyong Zhang,English,"Point-of-Interest (POI) recommendation, i.e., suggesting POIs that a user is likely to visit, is a key task to improve user experience in location based social networks (LBSNs). Existing models either focus on geographical influence without considering other factors such as social influence and temporal influence or rely on linear methods to combine different modeling factors, lacking a sophisticated and systematical way to learn representations for users and POIs for recommendation. To remedy these issues, in this work we propose GNN-POI, a generic POI recommendation framework that leverages Graph Neural Networks (GNNs), which demonstrate powerful modeling capacity to learn node representations from node information and topological structure to improve POI recommendation. Specifically, we construct a LBSN graph comprising of two types of nodes, i.e., user node and POI node. For a target user, her?…",Article,https://www.sciencedirect.com/science/article/pii/S0925231221011395,28,zhang2021leveraging,"POI recommendation,Graph neural network,Bi-LTSM,Location based social network"
2022,IEEE Network,Incentive mechanisms in federated learning and a game-theoretical approach,"Rongfei Zeng, Chao Zeng, Xingwei Wang, Bo Li, Xiaowen Chu",Rongfei Zeng,English,"Federated learning (FL) represents a new machine learning paradigm, utilizing various resources from participants to collaboratively train a global model without exposing the privacy of training data. The learning performance critically depends on various resources provided by participants and their active participation. Hence, it is essential to enable more participants to actively contribute their valuable resources in FL. In this article, we present a survey of incentive mechanisms for FL. We identify the incentive problem, outline its framework, and categorically discuss the state-of-the-art incentive mechanisms in Shapley value, Stackelberg game, auction, contract, and reinforcement learning. In addition, we propose three multi-dimensional game-theoretical models to study the economical behaviors of participants and demonstrate their applicability in cross-silo FL scenarios.",Article,https://ieeexplore.ieee.org/abstract/document/9843871/,26,zeng2022incentive,Federated Learning
2023,arXiv preprint arXiv:2308.03303,Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning,"Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, Bo Li",Longteng Zhang,English,"The low-rank adaptation (LoRA) method can largely reduce the amount of trainable parameters for fine-tuning large language models (LLMs), however, it still requires expensive activation memory to update low-rank weights. Reducing the number of LoRA layers or using activation recomputation could harm the fine-tuning performance or increase the computational overhead. In this work, we present LoRA-FA, a memory-efficient fine-tuning method that reduces the activation memory without performance degradation and expensive recomputation. LoRA-FA chooses to freeze the projection-down weight of  and update the projection-up weight of  in each LoRA layer. It ensures the change of model weight reside in a low-rank space during LLMs fine-tuning, while eliminating the requirement to store full-rank input activations. We conduct extensive experiments across multiple model types (RoBERTa, T5, LLaMA) and model scales. Our results show that LoRA-FA can always achieve close fine-tuning accuracy across different tasks compared to full parameter fine-tuning and LoRA. Furthermore, LoRA-FA can reduce the overall memory cost by up to 1.4 compared to LoRA.",Article,https://arxiv.org/abs/2308.03303,15,zhang2023lora,"low-rank adaptation,large language models,LLaMA"
2020,The 24th European Conference on Artificial Intelligence,Layer-wise Adaptive Gradient Sparsification for Distributed Deep Learning with Convergence Guarantees,"Shaohuai Shi, Zhenheng Tang, Qiang Wang, Kaiyong Zhao, Xiaowen Chu",Shaohuai Shi,English,"To reduce the long training time of large deep neural network (DNN) models, distributed synchronous stochastic gradient descent (S-SGD) is commonly used on a cluster of workers. However, the speedup brought by multiple workers is limited by the communication overhead. Two approaches, namely pipelining and gradient sparsification, have been separately proposed to alleviate the impact of communication overheads. Yet, the gradient sparsification methods can only initiate the communication after the backpropagation, and hence miss the pipelining opportunity. In this paper, we propose a new distributed optimization method named LAGS-SGD, which combines S-SGD with a novel layer-wise adaptive gradient sparsification (LAGS) scheme. In LAGS-SGD, every worker selects a small set of ""significant"" gradients from each layer independently whose size can be adaptive to the communication-to-computation ratio of that layer. The layer-wise nature of LAGS-SGD opens the opportunity of overlapping communications with computations, while the adaptive nature of LAGS-SGD makes it flexible to control the communication time. We prove that LAGS-SGD has convergence guarantees and it has the same order of convergence rate as vanilla S-SGD under a weak analytical assumption. Extensive experiments are conducted to verify the analytical assumption and the convergence performance of LAGS-SGD. Experimental results on a 16-GPU cluster show that LAGS-SGD outperforms the original S-SGD and existing sparsified S-SGD without losing obvious model accuracy.",Conference paper,https://arxiv.org/abs/1911.08727,25,shi2020layer," deep neural network,stochastic gradient descent, layer-wise adaptive gradient sparsification"
2022,IEEE Transactions on Parallel and Distributed Systems,Energy-aware non-preemptive task scheduling with deadline constraint in dvfs-enabled heterogeneous clusters,"Qiang Wang, Xinxin Mei, Hai Liu, Yiu-Wing Leung, Zongpeng Li, Xiaowen Chu",Qiang Wang,English,"Energy conservation of large data centers for high performance computing workloads, such as deep learning with Big Data, is of critical significance, where cutting down a few percent of electricity translates into million-dollar savings. This work studies energy conservation on emerging CPU-GPU hybrid clusters through dynamic voltage and frequency scaling (DVFS). We aim at minimizing the total energy consumption of processing a batch of offline tasks or a sequence of real-time tasks under deadline constraints. We derive a fast and accurate analytical model to compute the appropriate voltage/frequency setting for each task, and assign multiple tasks to the cluster with heuristic scheduling algorithms. In particular, our model stresses the nonlinear relationship between task execution time and processor speed for GPU-accelerated applications, for more accurately capturing real-world GPU energy consumption. In?…",Article,https://ieeexplore.ieee.org/abstract/document/9790352/,12,wang2022energy,"Graphics processing units, dynamic voltage and frequency scaling, task scheduling"
2021,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?…,Ednet: Efficient disparity estimation with cost volume combination and attention-based spatial residual,"Songyan Zhang, Zhicheng Wang, Qiang Wang, Jinshuo Zhang, Gang Wei, Xiaowen Chu",Songyan Zhang,English,"Existing state-of-the-art disparity estimation works mostly leverage the 4D concatenation volume and construct a very deep 3D convolution neural network (CNN) for disparity regression, which is inefficient due to the high memory consumption and slow inference speed. In this paper, we propose a network named EDNet for efficient disparity estimation. Firstly, we construct a combined volume which incorporates contextual information from the squeezed concatenation volume and feature similarity measurement from the correlation volume. The combined volume can be next aggregated by 2D convolutions which are faster and require less memory than 3D convolutions. Secondly, we propose an attention-based spatial residual module to generate attention-aware residual features. The attention mechanism is applied to provide intuitive spatial evidence about inaccurate regions with the help of error maps at multiple scales and thus improve the residual learning efficiency. Extensive experiments on the Scene Flow and KITTI datasets show that EDNet outperforms the previous 3D CNN based works and achieves state-of-the-art performance with significantly faster speed and less memory consumption.",Conference paper,http://openaccess.thecvf.com/content/CVPR2021/html/Zhang_EDNet_Efficient_Disparity_Estimation_With_Cost_Volume_Combination_and_Attention-Based_CVPR_2021_paper.html,21,zhang2021ednet,"convolution neural network,Stereo Matching"
2023,Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and?…,Adaprop: Learning adaptive propagation for graph neural network based knowledge graph reasoning,"Yongqi Zhang, Zhanke Zhou, Quanming Yao, Xiaowen Chu, Bo Han",Yongqi Zhang,English,"Due to the popularity of Graph Neural Networks (GNNs), various GNN-based methods have been designed to reason on knowledge graphs (KGs). An important design component of GNN-based KG reasoning methods is called the propagation path, which contains a set of involved entities in each propagation step. Existing methods use hand-designed propagation paths, ignoring the correlation between the entities and the query relation. In addition, the number of involved entities will explosively grow at larger propagation steps. In this work, we are motivated to learn an adaptive propagation path in order to filter out irrelevant entities while preserving promising targets. First, we design an incremental sampling mechanism where the nearby targets and layer-wise connections can be preserved with linear complexity. Second, we design a learning-based sampling distribution to identify the semantically related?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3580305.3599404,14,zhang2023adaprop,"Knowledge graph, Graph embedding, Knowledge graph reasoning,Graph sampling, Graph neural network"
2021,IEEE Transactions on Parallel and Distributed Systems,MG-WFBP: Merging Gradients Wisely for Efficient Communication in Distributed Deep Learning,"Shaohuai Shi, Xiaowen Chu, Bo Li",Shaohuai Shi,English,"Distributed synchronous stochastic gradient descent has been widely used to train deep neural networks (DNNs) on computer clusters. With the increase of computational power, network communications generally limit the system scalability. Wait-free backpropagation (WFBP) is a popular solution to overlap communications with computations during the training process. In this article, we observe that many DNNs have a large number of layers with only a small amount of data to be communicated at each layer in distributed training, which could make WFBP inefficient. Based on the fact that merging some short communication tasks into a single one can reduce the overall communication time, we formulate an optimization problem to minimize the training time in pipelining communications and computations. We derive an optimal solution that can be solved efficiently without affecting the training performance. We then?…",Article,https://ieeexplore.ieee.org/abstract/document/9328614/,23,shi2021mg,"Deep Learning, GPU, Distributed Stochastic Gradient Descent, Gradient Communication, Merged-gradient"
2021,IEEE INFOCOM 2021-IEEE Conference on Computer Communications,Exploiting simultaneous communications to accelerate data parallel distributed deep learning,"Shaohuai Shi, Xiaowen Chu, Bo Li",Shaohuai Shi,English,"Synchronous stochastic gradient descent (S-SGD) with data parallelism is widely used for training deep learning (DL) models in distributed systems. A pipelined schedule of the computing and communication tasks of a DL training job is an effective scheme to hide some communication costs. In such pipelined S-SGD, tensor fusion (i.e., merging some consecutive layers' gradients for a single communication) is a key ingredient to improve communication efficiency. However, existing tensor fusion techniques schedule the communication tasks sequentially, which overlooks their independence nature. In this paper, we expand the design space of scheduling by exploiting simultaneous All-Reduce communications. Through theoretical analysis and experiments, we show that simultaneous All-Reduce communications can effectively improve the communication efficiency of small tensors. We formulate an optimization?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9488803/,20,shi2021exploiting,"Distributed Deep Learning, Communication,Efficient, Simultaneous Communications"
2020,IEEE Access 8,An erasure-coded storage system for edge computing,"Lixin Liang, Huan He, Jian Zhao, Chengjian Liu, Qiuming Luo, Xiaowen Chu",Lixin Liang,English,"Emerging computing paradigm edge computing expects to store and process data at the network edge with reduced latency and improved network bandwidth. To the best of our knowledge, key performance issues such as coding performance of erasure-coded storage systems haven't been investigated for edge computing. In this paper, we present an erasure-coded storage system for edge computing. Unlike the data center and cloud storage systems, it employs edge devices to perform encoding and decoding operations, which can be a performance bottleneck of the whole storage system due to limited computing power. Hence, we present a comprehensive study of the performance of erasure coding to see if it can match the network performance of 5G and Wi-Fi 6 at the network edge. We use the popular edge device Jetson Nano and two state-of-the-art coding libraries: Jerasure and G-CRS. Our evaluation?…",Article,https://ieeexplore.ieee.org/abstract/document/9097196/,19,liang2020erasure,"Erasure-coded storage system, edge computing, erasure coding, jetson nano"
2020,IEEE transactions on parallel and distributed systems,Esetstore: An erasure-coded storage system with fast data recovery,"Chengjian Liu, Qiang Wang, Xiaowen Chu, Yiu-Wing Leung, Hai Liu",Chengjian Liu,English,"Erasure codes have been used extensively in large-scale storage systems to reduce the storage overhead of triplication-based storage systems. One key performance issue introduced by erasure codes is the long time needed to recover from a single failure, which occurs constantly in large-scale storage systems. We present ESetStore, a prototype erasure-coded storage system that aims to achieve fast recovery from failures. ESetStore is novel in the following aspects. We proposed a data placement algorithm named ESet for our ESetStore that can aggregate adequate I/O resources from available storage servers to recover from each single failure. We designed and implemented efficient read and write operations on our erasure-coded storage system via effective use of available I/O and computation resources. We evaluated the performance of ESetStore with extensive experiments on a cluster with 50 storage?…",Article,https://ieeexplore.ieee.org/abstract/document/9051846/,16,liu2020esetstore,"ESetStore, ESet, Erasure coded storage systems, Fast data recovery"
2022,IEEE Transactions on Industrial Informatics,A blockchain-enabled framework for enhancing scalability and security in IIoT,"Ruonan Li, Yang Qin, Canhui Wang, Mengya Li, Xiaowen Chu",Ruonan Li,English,"Industrial Internet of Things (IIoT) technology is widely used in modern industrial fields like transportation, but data security remains a major challenge. The blockchain-based access control mechanism can address the data security issue by preventing unauthorized devices from accessing limited IIoT resources. However, most existing blockchain-based access control mechanism for IIoT still has scalability and privacy issues. To deal with the above-mentioned problems, we propose a new scalable and secure strategy for the blockchain-based access control framework for IIoT via sharding, which consists of two components. First, the network sharding scheme based on the access frequency set (N2SAF) is designed to 1) improve the scalability of our proposed strategy by transaction sharding to reduce the storage pressure on nodes, and 2) increase the transaction processing speed on a three-layer architecture?…",Article,https://ieeexplore.ieee.org/abstract/document/9904919/,18,li2022blockchain,"Blockchain, Industrial Internet of Things (IIoT), privacy protection, scalability, sharding"
2020,2020 International Conference on Parallel and Distributed Systems,Efficient Sparse-Dense Matrix-Matrix Multiplication on GPUs Using the Customized Sparse Storage Format,"Shaohuai Shi, Qiang Wang, Xiaowen Chu",Shaohuai Shi,English,"Multiplication of a sparse matrix to a dense matrix (SpDM) is widely used in many areas like scientific computing and machine learning. However, existing work under-looks the performance optimization of SpDM on modern manycore architectures like GPUs. The storage data structures help sparse matrices store in a memory-saving format, but they bring difficulties in optimizing the performance of SpDM on modern GPUs due to irregular data access of the sparse structure, which results in lower resource utilization and poorer performance. In this paper, we refer to the roofline performance model of GPUs to design an efficient SpDM algorithm called GCOOSpDM, in which we exploit coalescent global memory access, fast shared memory reuse, and more operations per byte of global memory traffic. Experiments are evaluated on three Nvidia GPUs (i.e., GTX 980, GTX Titan X Pascal, and Tesla P100) using a large?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9359142/,18,shi2020efficient,"Sparse Matrix Multiplication, COO, GCOO,GPU"
2020,IEEE INFOCOM 2020-IEEE Conference on Computer Communications,Joint access point placement and power-channel-resource-unit assignment for 802.11 ax-based dense WiFi with QoS requirements,"Shuwei Qiu, Xiaowen Chu, Yiu-Wing Leung, Joseph Kee Yin Ng",Shuwei Qiu,English,"IEEE 802.11ax is a promising standard for the next-generation WiFi network, which uses orthogonal frequency division multiple access (OFDMA) to segregate the wireless spectrum into time-frequency resource units (RUs). In this paper, we aim at designing an 802.11ax-based dense WiFi network to provide WiFi services to a large number of users within a given area with the following objectives: (1) to minimize the number of access points (APs); (2) to fulfil the users’ throughput requirement; and (3) to be resistant to AP failures. We formulate the above into a joint AP placement and power-channel-RU assignment optimization problem, which is NP-hard. To tackle this problem, we first derive an analytical model to estimate each user’s throughput under the mechanism of OFDMA and a widely used interference model. We then design a heuristic algorithm to find high-quality solutions with polynomial time complexity?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9155490/,17,qiu2020joint,"IEEE 802.11ax, AP placement, quality of service, fault tolerance, resource assignment, dense WiFi network"
2021,Information Processing & Management,Enhancing the efficiency and scalability of blockchain through probabilistic verification and clustering,"Mengya Li, Yang Qin, Bing Liu, Xiaowen Chu",Mengya Li,English,"Blockchain is a disruptive technique that finds many applications in FinTech, IoT, and token economy. Because of the asynchrony, the competitive mining, and the indeterministic block propagation delay in networks, forks in the blockchain occur frequently, which not only waste a lot of computing resources but also result in potential security issues. This issue will greatly affect the efficiency of blockchain networks. In the meantime, when blockchain networks expand, the storage data for each node will be increasing dramatically. Participates are about to face the problem of storage limitation. Blockchain is hard to scale. This paper introduced PvScheme, a probabilistic verification scheme that could effectively reduce the block propagation delay and reduce the occurrence of blockchain forks. We further enhanced the security of PvScheme to provide reliable block delivery. We also analysed the resistance of PvScheme?…",Article,https://www.sciencedirect.com/science/article/pii/S0306457321001394,16,li2021enhancing,"Blockchain,Fork,Security,Storage limitation,Clustering ,Node collaboration"
2021,International Conference on Database Systems for Advanced Applications,BU-trace: A permissionless mobile system for privacy-preserving intelligent contact tracing,"Zhe Peng, Jinbin Huang, Haixin Wang, Shihao Wang, Xiaowen Chu, Xinzhi Zhang, Li Chen, Xin Huang, Xiaoyi Fu, Yike Guo, Jianliang Xu",Zhe Peng,English," The coronavirus disease 2019 (COVID-19) pandemic has caused an unprecedented health crisis for the global. Digital contact tracing, as a transmission intervention measure, has shown its effectiveness on pandemic control. Despite intensive research on digital contact tracing, existing solutions can hardly meet users’ requirements on privacy and convenience. In this paper, we propose -, a novel permissionless mobile system for privacy-preserving intelligent contact tracing based on QR code and NFC technologies. First, a user study is conducted to investigate and quantify the user acceptance of a mobile contact tracing system. Second, a decentralized system is proposed to enable contact tracing while protecting user privacy. Third, an intelligent behavior detection algorithm is designed to ease the use of our system. We implement - and conduct extensive experiments in several real-world?…",Conference paper,https://link.springer.com/chapter/10.1007/978-3-030-73216-5_26,15,peng2021bu,"Privacy-preserving, Permissionless, Intelligent,Contact tracing."
2023,2023 IEEE 43rd International Conference on Distributed Computing Systems?…,Dear: accelerating distributed deep learning with fine-grained all-reduce pipelining,"Lin Zhang, Shaohuai Shi, Xiaowen Chu, Wei Wang, Bo Li, Chengjian Liu",Lin Zhang,English,"Communication scheduling has been shown to be effective in accelerating distributed training, which enables all-reduce communications to be overlapped with backpropagation computations. This has been commonly adopted in popular distributed deep learning frameworks. However, there exist two fundamental problems: (1) excessive startup latency proportional to the number of workers for each all-reduce operation; (2) it only achieves sub-optimal training performance due to the dependency and synchronization requirement of the feed-forward computation in the next iteration. We propose a novel scheduling algorithm, DeAR, that decouples the all-reduce primitive into two continuous operations, which overlaps with both backpropagation and feed-forward computations without extra communications. We further design a practical tensor fusion algorithm to improve the training performance. Experimental results?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/10272398/,11,zhang2023dear,"GPU,Distributed Deep Learning"
2022,US Patent 11,System for efficient large-scale data distribution in distributed and parallel processing environment,"Xiaowen Chu, SHI Shaohuai, Kaiyong Zhao",Xiaowen Chu,English,"The present invention relates to a system for efficient large-scale data distribution in a distributed and parallel processing environment. In particular, the present invention relates to global Top-k sparsification for low bandwidth networks. The present invention verifies that gTop-k S-SGD has nearly consistent convergence performance with S-SGD and evaluates the training efficiency of gTop-k on a cluster with 32 GPU machines which are inter-connected with 1 Gbps Ethernet. The experimental results show that the present invention achieves up to 2.7-12× higher scaling efficiency than S-SGD with dense gradients, and 1.1-1.7× improvement than the existing Top-k S-SGD.",Conference paper,https://patents.google.com/patent/US11436065B2/en,0,chu2022system,None
2021,None,Efficient multi-objective evolutionary 3D neural architecture search for COVID-19 detection with chest CT scans,"Xin He, Shihao Wang, Guohao Ying, Jiyong Zhang, Xiaowen Chu",Xin He,English,"COVID-19 pandemic has spread globally for months. Due to its long incubation period and high testing cost, there is no clue showing its spread speed is slowing down, and hence a faster testing method is in dire need. This paper proposes an efficient Evolutionary Multi-objective neural ARchitecture Search (EMARS) framework, which can automatically search for 3D neural architectures based on a well-designed search space for COVID-19 chest CT scan classification. Within the framework, we use weight sharing strategy to significantly improve the search efficiency and finish the search process in 8 hours. We also propose a new objective, namely potential, which is of benefit to improve the search process's robustness. With the objectives of accuracy, potential, and model size, we find a lightweight model (3.39 MB), which outperforms three baseline human-designed models, ie, ResNet3D101 (325.21 MB), DenseNet3D121 (43.06 MB), and MC3_18 (43.84 MB). Besides, our well-designed search space enables the class activation mapping algorithm to be easily embedded into all searched models, which can provide the interpretability for medical diagnosis by visualizing the judgment based on the models to locate the lesion areas.",Conference paper,https://europepmc.org/article/ppr/ppr310659,12,he2021efficient,"COVID-19, Evolutionary Algorithm, Neural Architecture Search, Multi-objective optimization"
2021,International Workshop on Federated and Transfer Learning for Data Sparsity?…,Data Resampling for Federated Learning with Non-IID Labels,"Zhenheng Tang, Zhikai Hu, Shaohuai Shi, Yiu-ming Cheung, Yilun Jin, Zhenghang Ren, Xiaowen Chu",Zhenheng Tang,English,"Recently, federated learning has received increasing attention from academe and industry, since it makes training models with decentralized data possible. However, most existing federated learning approaches suffer from Non-Independent and Identically data distribution in clients. Observing that each client has an imbalanced label distribution in many federated learning scenarios, we examine the effects of combining imbalanced learning techniques with federated learning. Through comprehensive experiments, we obtain the following findings:(1) By data resampling, the label sampling probabilities are made more similar across clients, which leads to faster convergence;(2) Imbalanced data resampling results in final accuracy decreasing on local dataset. Based on these two key findings, we propose a simple but effective data resampling strategy named Imbalanced Weight Decay Sampling (IWDS) that dynamically regulates the sampling probability of labels, remarkably accelerating the training process. The effectiveness of IWDS has been verified on several modern federated learning algorithms such as FedAvg, FedProx, and FedNova.",Conference paper,https://federated-learning.org/fl-ijcai-2021/FTL-IJCAI21_paper_3.pdf,12,tang2021data,"Federated Learning,data sampling"
2022,European Conference on Computer Vision 2022,EAGAN: Efficient Two-stage Evolutionary Architecture Search for GANs,"Guohao Ying, Xin He, Bin Gao, Bo Han, Xiaowen Chu",Guohao Ying,English,"Generative adversarial networks (GANs) have proven successful in image generation tasks. However, GAN training is inherently unstable. Although many works try to stabilize it by manually modifying GAN architecture, it requires much expertise. Neural architecture search (NAS) has become an attractive solution to search GANs automatically. The early NAS-GANs search only generators to reduce search complexity but lead to a sub-optimal GAN. Some recent works try to search both generator (G) and discriminator (D), but they suffer from the instability of GAN training. To alleviate the instability, we propose an efficient two-stage evolutionary algorithm-based NAS framework to search GANs, namely EAGAN. We decouple the search of G and D into two stages, where stage-1 searches G with a fixed D and adopts the many-to-one training strategy, and stage-2 searches D with the optimal G found in stage-1 and?…",Conference paper,https://link.springer.com/chapter/10.1007/978-3-031-19787-1_3,11,ying2022eagan,Generative adversarial networks
2020,arXiv preprint arXiv:2002.10105,Communication contention aware scheduling of multiple deep learning training jobs,"Qiang Wang, Shaohuai Shi, Canhui Wang, Xiaowen Chu",Qiang Wang,English,"Distributed Deep Learning (DDL) has rapidly grown its popularity since it helps boost the training performance on high-performance GPU clusters. Efficient job scheduling is indispensable to maximize the overall performance of the cluster when training multiple jobs simultaneously. However, existing schedulers do not consider the communication contention of multiple communication tasks from different distributed training jobs, which could deteriorate the system performance and prolong the job completion time. In this paper, we first establish a new DDL job scheduling framework which organizes DDL jobs as Directed Acyclic Graphs (DAGs) and considers communication contention between nodes. We then propose an efficient algorithm, LWF-, to balance the GPU utilization and consolidate the allocated GPUs for each job. When scheduling those communication tasks, we observe that neither avoiding all the contention nor blindly accepting them is optimal to minimize the job completion time. We thus propose a provable algorithm, AdaDUAL, to efficiently schedule those communication tasks. Based on AdaDUAL, we finally propose Ada-SRSF for the DDL job scheduling problem. Simulations on a 64-GPU cluster connected with 10 Gbps Ethernet show that LWF- achieves up to  improvement over the classical first-fit algorithms. More importantly, Ada-SRSF reduces the average job completion time by  and , as compared to the SRSF(1) scheme (avoiding all the contention) and the SRSF(2) scheme (blindly accepting all of two-way communication contention) respectively.",Article,https://arxiv.org/abs/2002.10105,10,wang2020communication,"Distributed Deep Learning, Job Scheduling,Communication Contention"
2023,arXiv preprint arXiv:2309.01172,FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs,"Zhenheng Tang, Yuxin Wang, Xin He, Longteng Zhang, Xinglin Pan, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Bingsheng He, Xiaowen Chu",Zhenheng Tang,English,"The rapid growth of memory and computation requirements of large language models (LLMs) has outpaced the development of hardware, hindering people who lack large-scale high-end GPUs from training or deploying LLMs. However, consumer-level GPUs, which constitute a larger market share, are typically overlooked in LLM due to their weaker computing performance, smaller storage capacity, and lower communication bandwidth. Additionally, users may have privacy concerns when interacting with remote LLMs. In this paper, we envision a decentralized system unlocking the potential vast untapped consumer-level GPUs in pre-training, inference and fine-tuning of LLMs with privacy protection. However, this system faces critical challenges, including limited CPU and GPU memory, low network bandwidth, the variability of peer and device heterogeneity. To address these challenges, our system design incorporates: 1) a broker with backup pool to implement dynamic join and quit of computing providers; 2) task scheduling with hardware performance to improve system efficiency; 3) abstracting ML procedures into directed acyclic graphs (DAGs) to achieve model and task universality; 4) abstracting intermediate represention and execution planes to ensure compatibility of various devices and deep learning (DL) frameworks. Our performance analysis demonstrates that 50 RTX 3080 GPUs can achieve throughputs comparable to those of 4 H100 GPUs, which are significantly more expensive.",Article,https://arxiv.org/abs/2309.01172,9,tang2023fusionai,"large language models,GPU,deep learning"
2022,Mobile Networks and Applications,"Advances in mobile, edge and cloud computing","Xiaowen Chu, Hongbo Jiang, Bo Li, Dan Wang, Wei Wang",Xiaowen Chu,English,"Editorial: Emerging mobile applications exhibit heterogeneous requirements on the computing power, communication bandwidth, security and privacy. Given the restriction on the computing capability on battery-operated mobile devices, a variety of offloading techniques have been designed to leverage the abundant computing resources available on cloud servers. Mobile Cloud Computing provides enormous computing and storage resources for mobile applications that can tolerate a certain level of network delay, while Mobile Edge Computing offers an intelligent platform to enhance mobile devices’ capabilities and improve the Quality of Service of mobile applications. Both Mobile Cloud Computing and Mobile Edge Computing are key enabling paradigms for emerging mobile applications in Internet of Things (IoT), smart grids, robotics, crowd sensing, etc. New research challenges arise due to the heterogeneity?…",Article,https://link.springer.com/article/10.1007/s11036-020-01654-9,9,chu2022advances,mobile cloud computing
2022,International Conference on Medical Image Computing and Computer-Assisted?…,Evolutionary multi-objective architecture search framework: Application to covid-19 3d ct classification,"Xin He, Guohao Ying, Jiyong Zhang, Xiaowen Chu",Xin He,English,"The COVID-19 pandemic has threatened global health. Many studies have applied deep convolutional neural networks (CNN) to recognize COVID-19 based on chest 3D computed tomography (CT). Recent works show that no model generalizes well across CT datasets from different countries, and manually designing models for specific datasets requires expertise; thus, neural architecture search (NAS) that aims to search models automatically has become an attractive solution. To reduce the search cost on large 3D CT datasets, most NAS-based works use the weight-sharing (WS) strategy to make all models share weights within a supernet; however, WS inevitably incurs search instability, leading to inaccurate model estimation. In this work, we propose an efficient Evolutionary Multi-objective ARchitecture Search (EMARS) framework. We propose a new objective, namely potential, which can help exploit?…",Conference paper,https://link.springer.com/chapter/10.1007/978-3-031-16431-6_53,8,he2022evolutionary,"COVID-19,Neural Architecture Search (NAS), Weight sharing,Evolutionary Algorithm (EA),3D Computed Tomograph (CT)"
2023,Proceedings of the AAAI Conference on Artificial Intelligence,Rethinking Disparity: A Depth Range Free Multi-View Stereo Based on Disparity,"Qingsong Yan, Qiang Wang, Kaiyong Zhao, Bo Li, Xiaowen Chu, Fei Deng",Qingsong Yan,English,"Existing learning-based multi-view stereo (MVS) methods rely on the depth range to build the 3D cost volume and may fail when the range is too large or unreliable. To address this problem, we propose a disparity-based MVS method based on the epipolar disparity flow (E-flow), called DispMVS, which infers the depth information from the pixel movement between two views. The core of DispMVS is to construct a 2D cost volume on the image plane along the epipolar line between each pair (between the reference image and several source images) for pixel matching and fuse uncountable depths triangulated from each pair by multi-view geometry to ensure multi-view consistency. To be robust, DispMVS starts from a randomly initialized depth map and iteratively refines the depth map with the help of the coarse-to-fine strategy. Experiments on DTUMVS and Tanks\&Temple datasets show that DispMVS is not sensitive to the depth range and achieves state-of-the-art results with lower GPU memory.",Article,https://ojs.aaai.org/index.php/AAAI/article/view/25413,7,yan2023rethinking,"multi-view stereo,disparity"
2023,Proceedings of the AAAI Conference on Artificial Intelligence,NAS-LID: Efficient Neural Architecture Search with Local Intrinsic Dimension,"Xin He, Jiangchao Yao, Yuxin Wang, Zhenheng Tang, Ka Chu Cheung, Simon See, Bo Han, Xiaowen Chu",Xin He,English,"One-shot neural architecture search (NAS) substantially improves the search efficiency by training one supernet to estimate the performance of every possible child architecture (ie, subnet). However, the inconsistency of characteristics among subnets incurs serious interference in the optimization, resulting in poor performance ranking correlation of subnets. Subsequent explorations decompose supernet weights via a particular criterion, eg, gradient matching, to reduce the interference; yet they suffer from huge computational cost and low space separability. In this work, we propose a lightweight and effective local intrinsic dimension (LID)-based method NAS-LID. NAS-LID evaluates the geometrical properties of architectures by calculating the low-cost LID features layer-by-layer, and the similarity characterized by LID enjoys better separability compared with gradients, which thus effectively reduces the interference among subnets. Extensive experiments on NASBench-201 indicate that NAS-LID achieves superior performance with better efficiency. Specifically, compared to the gradient-driven method, NAS-LID can save up to 86% of GPU memory overhead when searching on NASBench-201. We also demonstrate the effectiveness of NAS-LID on ProxylessNAS and OFA spaces. Source code: https://github. com/marsggbo/NAS-LID.",Article,https://ojs.aaai.org/index.php/AAAI/article/view/25949,6,he2023lid,"neural architecture search,local intrinsic dimension"
2021,Biosensors and Bioelectronics 192,“Barcode” cell sensor microfluidic system: Rapid and sample-to-answer antimicrobial susceptibility testing applicable in resource-limited conditions,"Chiu-Wing Chan, Han Sun, Yisu Wang, Zhihao Zhao, Ryan O'Neill, Sin-Yung Siu, Xiaowen Chu, Niaz Banaei, Kangning Ren",Chiu-Wing Chan,English,"Many rapid antimicrobial susceptibility testing (AST) methods have been proposed to contain clinical antimicrobial resistance (AMR) and preserve the effectiveness of remaining antimicrobials. However, far fewer methods have been proposed to test AMR in resource-limited conditions, such as for frequent safety screenings of water/food/public facilities, urgent surveys of massive samples during a pandemic, or AMR tests in low-income countries. Rapid AST methods realized thus far have a variety of drawbacks when used for such surveys, e.g., high cost and the requirement of expensive instruments such as microscopy. A more reasonable strategy would be to screen samples via onsite testing first, and then send any sample suspected to contain AMR bacteria for advanced testing. Accordingly, a cost-efficient AST is demanded, which can rapidly process a large number of samples without using expensive?…",Article,https://www.sciencedirect.com/science/article/pii/S0956566321005534,6,chan2021barcode,"Microfluidic,Chip,'Barcode' Cell Sensor,Portable analysis,Cost-efficient,Onsite detection,Cell phone app,Rapid antimicrobial susceptibility testing,Resource-limited condition"
2021,arXiv preprint arXiv:2110.02582,Fadnet++: Real-time and accurate disparity estimation with configurable networks,"Qiang Wang, Shaohuai Shi, Shizhen Zheng, Kaiyong Zhao, Xiaowen Chu",Qiang Wang,English,"Deep neural networks (DNNs) have achieved great success in the area of computer vision. The disparity estimation problem tends to be addressed by DNNs which achieve much better prediction accuracy than traditional hand-crafted feature-based methods. However, the existing DNNs hardly serve both efficient computation and rich expression capability, which makes them difficult for deployment in real-time and high-quality applications, especially on mobile devices. To this end, we propose an efficient, accurate, and configurable deep network for disparity estimation named FADNet++. Leveraging several liberal network design and training techniques, FADNet++ can boost its accuracy with a fast model inference speed for real-time applications. Besides, it enables users to easily configure different sizes of models for balancing accuracy and inference efficiency. We conduct extensive experiments to demonstrate the effectiveness of FADNet++ on both synthetic and realistic datasets among six GPU devices varying from server to mobile platforms. Experimental results show that FADNet++ and its variants achieve state-of-the-art prediction accuracy, and run at a significant order of magnitude faster speed than existing 3D models. With the constraint of running at above 15 frames per second (FPS) on a mobile GPU, FADNet++ achieves a new state-of-the-art result for the SceneFlow dataset.",Article,https://arxiv.org/abs/2110.02582,6,wang2021fadnet++,"3D Vision, Stereo Matching, Disparity Estimation, Deep Learning, Efficient Inference"
2020,2020 IEEE 40th International Conference on Distributed Computing Systems?…,A multi-node collaborative storage strategy via clustering in blockchain network,"Mengya Li, Yang Qin, Bing Liu, Xiaowen Chu",Mengya Li,English,"Blockchain is essentially a distributed ledger shared by all nodes in the system. All nodes in blockchain are equal, and each node holds all transactions and blocks in the network. As the network continues to expand, the data rises linearly. Participates are about to face the problem of storage limitation. Blockchain is hard to scale.This paper introduces ICIStrategy, a multi-node collaborative storage strategy based on intra-cluster integrity. In ICIStrategy, we divide all participates into several clusters. Each cluster requires holding all data of the network, whereas a node within the cluster does not need to maintain data integrity. It aims to solve the storage pressure by reducing the amount data that each participate need to store and reduce communication overhead by collaboratively storing and verifying blocks through in-cluster nodes. Moreover, the ICIStrategy could greatly save the overhead of bootstrapping. We?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9355650/,6,li2020multi,"blockchain, storage limitation, clustering, node collaboration"
2020,Knowledge-Based Systems 192,A probabilistic approach towards an unbiased semi-supervised cluster tree,"Zhaocai Sun, Xiaofeng Zhang, Yunming Ye, Xiaowen Chu, Zhi Liu",Zhaocai Sun,English,"Conventionally, it is a prerequisite to acquire a good number of annotated data to train an accurate classifier. However, the acquisition of such dataset is usually infeasible due to the high annotation cost. Therefore, semi-supervised learning has emerged and attracts increasing research efforts in recent years. Essentially, semi-supervised learning is sensitive to the manner how the unlabeled data is sampled. However, the model performance might be seriously deteriorated if biased unlabeled data is sampled at the early stage. In this paper, an unbiased semi-supervised cluster tree is proposed which is learnt using only very few labeled data. Specifically, a K-means algorithm is adopted to build each level of this hierarchical tree in a decent top-down manner. The number of clusters is determined by the number of classes contained in the labeled data. The confidence error of the cluster tree is theoretically analyzed?…",Article,https://www.sciencedirect.com/science/article/pii/S0950705119305908,6,sun2020probabilistic,"Semi-supervised learning,Cluster tree,Text classification"
2023,arXiv preprint arXiv:2303.01778,FedML Parrot: A scalable federated learning system via heterogeneity-aware scheduling on sequential and hierarchical training,"Zhenheng Tang, Xiaowen Chu, Ryan Yide Ran, Sunwoo Lee, Shaohuai Shi, Yonggang Zhang, Yuxin Wang, Alex Qiaozhong Liang, Salman Avestimehr, Chaoyang He",Zhenheng Tang,English,"Federated Learning (FL) enables collaborations among clients for train machine learning models while protecting their data privacy. Existing FL simulation platforms that are designed from the perspectives of traditional distributed training, suffer from laborious code migration between simulation and production, low efficiency, low GPU utility, low scalability with high hardware requirements and difficulty of simulating stateful clients. In this work, we firstly demystify the challenges and bottlenecks of simulating FL, and design a new FL system named as FedML \texttt{Parrot}. It improves the training efficiency, remarkably relaxes the requirements on the hardware, and supports efficient large-scale FL experiments with stateful clients by: (1) sequential training clients on devices; (2) decomposing original aggregation into local and global aggregation on devices and server respectively; (3) scheduling tasks to mitigate straggler problems and enhance computing utility; (4) distributed client state manager to support various FL algorithms. Besides, built upon our generic APIs and communication interfaces, users can seamlessly transform the simulation into the real-world deployment without modifying codes. We evaluate \texttt{Parrot} through extensive experiments for training diverse models on various FL datasets to demonstrate that \texttt{Parrot} can achieve simulating over 1000 clients (stateful or stateless) with flexible GPU devices setting () and high GPU utility, 1.2  4 times faster than FedScale, and 10  100 times memory saving than FedML. And we verify that \texttt{Parrot} works well with homogeneous and heterogeneous devices in three?…",Article,https://arxiv.org/abs/2303.01778,6,tang2023fedml,"Federated Learning,GPU"
2020,2020 International Conferences on Internet of Things (iThings) and IEEE?…,Energy-efficient inference service of transformer-based deep learning models on GPUS,"Yuxin Wang, Qiang Wang, Xiaowen Chu",Yuxin Wang,English,"Inference-as-a-service (IAAS) has been recently launched by cloud service providers to support on-demand AI applications. Many natural language processing (NLP) services are based on the Transformer Sequence Transduction model. However, the inference process of the Transformer model consumes a significant amount of energy due to the large model size (e.g., billions of parameters) and tremendous computations. How to reduce the energy consumption of IAAS without violating the service-level agreement (SLA) becomes a practical challenge for service providers. In this work, we conduct a comprehensive study on the inference performance and energy efficiency of a Transformer model trained for the language translation service. First, we empirically characterize some essential performance metrics, including latency, throughput, and energy consumption on three different GPUs with diversified workload?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9291633/,5,wang2020energy,"Graphics Processing Units, Transformer Model, Energy Efficiency, Batch Inference, Inference Scheduling, Cloud Service"
2024,arXiv preprint arXiv:2401.17644,Towards Efficient and Reliable LLM Serving: A Real-World Workload Study,"Yuxin Wang, Yuhan Chen, Zeyu Li, Zhenheng Tang, Rui Guo, Xin Wang, Qiang Wang, Amelie Chi Zhou, Xiaowen Chu",Yuxin Wang,English,"Large language models (LLMs), especially Generative Pretrained Transformer (GPT) models, have significantly advanced in the industry in recent years. However, these models' broader development faces considerable challenges due to high operational and deployment costs. This has led to active research in improving the hardware efficiency of LLMs. Yet, the characteristics of real-world LLM workloads are often overlooked in current optimizations of LLM serving systems. In this work, we find that the absence of reliable workload data for evaluating LLM serving systems impacts the quality of service (QoS) and reliability in industrial deployments. This paper introduces the first real-world trace dataset of LLM serving workloads, detailing user, system, and LLM behaviors. We analyze this trace, highlighting burstiness, request and response distributions, and focusing on the reliability of GPT services. Based on this, we have developed a benchmark suite that reflects our dataset's workload patterns, enabling performance evaluation of serving systems. This suite captures the core patterns of workload distributions, allowing for precise scaling of the workload dataset to match system sizes. Our evaluation uncovers a previously unrecognized vulnerability of LLM serving systems to short-term burstiness, particularly in common workload scenarios. We observe that GPU memory limitations, caused by the fluctuating nature of burstiness, lead to significant performance degradation in existing LLM serving systems. Beyond benchmarking, understanding these patterns is valuable for optimizing LLM workload management, enabling elastic hardware?…",Article,https://arxiv.org/abs/2401.17644,4,wang2024towards,"large language models, Generative Pretrained Transformer, Batch Inference, GPU Serving, Bursty Workloads,Benchmarking, Quality of Service"
2024,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?…,Vmrnn: Integrating vision mamba and lstm for efficient and accurate spatiotemporal forecasting,"Yujin Tang, Peijie Dong, Zhenheng Tang, Xiaowen Chu, Junwei Liang",Yujin Tang,English,Combining Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs) with Recurrent Neural Networks (RNNs) for spatiotemporal forecasting has yielded unparalleled results in predicting temporal and spatial dynamics. However modeling extensive global information remains a formidable challenge; CNNs are limited by their narrow receptive fields and ViTs struggle with the intensive computational demands of their attention mechanisms. The emergence of recent Mamba-based architectures has been met with enthusiasm for their exceptional long-sequence modeling capabilities surpassing established vision models in efficiency and accuracy which motivates us to develop an innovative architecture tailored for spatiotemporal forecasting. In this paper we propose the VMRNN cell a new recurrent unit that integrates the strengths of Vision Mamba blocks with LSTM. We construct a network centered on VMRNN cells to tackle spatiotemporal prediction tasks effectively. Our extensive evaluations show that our proposed approach secures competitive results on a variety of tasks while maintaining a smaller model size. Our code is available at https://github. com/yyyujintang/VMRNN-PyTorch.,Conference paper,https://openaccess.thecvf.com/content/CVPR2024W/PRECOGNITION/html/Tang_VMRNN_Integrating_Vision_Mamba_and_LSTM_for_Efficient_and_Accurate_CVPRW_2024_paper.html,4,tang2024vmrnn,"Convolutional Neural Networks,Vision Transformers, Recurrent Neural Network, LSTM"
2022,European Conference on Computer Vision 2022,EASNet: Searching Elastic and Accurate Network Architecture for Stereo Matching,"Qiang Wang, Shaohuai Shi, Kaiyong Zhao, Xiaowen Chu",Qiang Wang,English,"Recent advanced studies have spent considerable human efforts on optimizing network architectures for stereo matching but hardly achieved both high accuracy and fast inference speed. To ease the workload in network design, neural architecture search (NAS) has been applied with great success to various sparse prediction tasks, such as image classification and object detection. However, existing NAS studies on the dense prediction task, especially stereo matching, still cannot be efficiently and effectively deployed on devices of different computing capability. To this end, we propose to train an elastic and accurate network for stereo matching (EASNet) that supports various 3D architectural settings on devices with different compute capability. Given the deployment latency constraint on the target device, we can quickly extract a sub-network from the full EASNet without additional training while the accuracy of the?…",Conference paper,https://link.springer.com/chapter/10.1007/978-3-031-19824-3_26,4,wang2022easnet,"Stereo Matching, Neural Architecture Search"
2020,Proceedings of the 13th Annual Workshop on General Purpose Processing using?…,GPGPU performance estimation for frequency scaling using cross-benchmarking,"Qiang Wang, Chengjian Liu, Xiaowen Chu",Qiang Wang,English,"Dynamic Voltage and Frequency Scaling (D VFS) on General-Purpose Graphics Processing Units (GPGPUs) is now becoming one of the most significant techniques to balance computational performance and energy consumption. However, there are still few fast and accurate models for predicting GPU kernel execution time under different core and memory frequency settings, which is important to determine the best frequency configuration for energy saving. Accordingly, a novel GPGPU performance estimation model with both core and memory frequency scaling is herein proposed. We design a cross-benchmarking suite, which simulates kernels with a wide range of instruction distributions. The synthetic kernels generated by this suite can be used for model pre-training or as supplementary training samples. Then we apply two different machine learning algorithms, Support Vector Regression (SVR) and Gradient?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3366428.3380767,4,wang2020gpgpu,"Graphics Processing Units, Dynamic Voltage and Frequency Scaling,GPU Performance Modeling, Machine Learning"
2024,arXiv preprint arXiv:2402.10631,BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation,"Dayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen Chu, Ningyi Xu",Dayou Du,English,"The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges. Weight quantization has emerged as a widely embraced solution to reduce memory and computational demands. This paper introduces BitDistiller, a framework that synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD) to boost the performance of LLMs at ultra-low precisions (sub-4-bit). Specifically, BitDistiller first incorporates a tailored asymmetric quantization and clipping technique to maximally preserve the fidelity of quantized weights, and then proposes a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which is employed in a self-distillation manner to enable faster convergence and superior model performance. Empirical evaluations demonstrate that BitDistiller significantly surpasses existing methods in both 3-bit and 2-bit configurations on general language understanding and complex reasoning benchmarks. Notably, BitDistiller is shown to be more cost-effective, demanding fewer data and training resources. The code is available at https://github.com/DD-DuDa/BitDistiller.",Article,https://arxiv.org/abs/2402.10631,3,du2024bitdistiller,"large language models, Quantization Aware Training,Knowledge Distillation"
2023,arXiv preprint arXiv:2311.03687,"Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models","Longteng Zhang, Xiang Liu, Zeyu Li, Xinglin Pan, Peijie Dong, Ruibo Fan, Rui Guo, Xin Wang, Qiong Luo, Shaohuai Shi, Xiaowen Chu",Longteng Zhang,English,"Large Language Models (LLMs) have seen great advance in both academia and industry, and their popularity results in numerous open-source frameworks and techniques in accelerating LLM pre-training, fine-tuning, and inference. Training and deploying LLMs are expensive as it requires considerable computing resources and memory, hence many efficient approaches have been developed for improving system pipelines as well as operators. However, the runtime performance can vary significantly across hardware and software stacks, which makes it difficult to choose the best configuration. In this work, we aim to benchmark the performance from both macro and micro perspectives. First, we benchmark the end-to-end performance of pre-training, fine-tuning, and serving LLMs in different sizes , i.e., 7, 13, and 70 billion parameters (7B, 13B, and 70B) on three 8-GPU platforms with and without individual optimization techniques, including ZeRO, quantization, recomputation, FlashAttention. Then, we dive deeper to provide a detailed runtime analysis of the sub-modules, including computing and communication operators in LLMs. For end users, our benchmark and findings help better understand different optimization techniques, training and inference frameworks, together with hardware platforms in choosing configurations for deploying LLMs. For researchers, our in-depth module-wise analyses discover potential opportunities for future work to further optimize the runtime performance of LLMs.",Article,https://arxiv.org/abs/2311.03687,3,zhang2023dissecting,"large language models, Performance Evaluation, Benchmarks"
2023,2023 IEEE 43rd International Conference on Distributed Computing Systems?…,Evaluation and optimization of gradient compression for distributed deep learning,"Lin Zhang, Longteng Zhang, Shaohuai Shi, Xiaowen Chu, Bo Li",Lin Zhang,English,"To accelerate distributed training, many gradient compression methods have been proposed to alleviate the communication bottleneck in synchronous stochastic gradient descent (S-SGD), but their efficacy in real-world applications still remains unclear. In this work, we first evaluate the efficiency of three representative compression methods (quantization with Sign-SGD, sparsification with Top-k SGD, and low-rank with Power-SGD) on a 32-GPU cluster. The results show that they cannot always outperform well-optimized S-SGD or even worse due to their incompatibility with three key system optimization techniques (all-reduce, pipelining, and tensor fusion) in S-SGD. To this end, we propose a novel gradient compression method, called alternate compressed Power-SGD (ACP-SGD), which alternately compresses and communicates low-rank matrices. ACP-SGD not only significantly reduces the communication?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/10272442/,4,zhang2023evaluation,"Distributed Deep Learning, Gradient Compression, Power-SGD, System Optimization"
2023,2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS?…,Fast Sparse GPU Kernels for Accelerated Training of Graph Neural Networks,"Ruibo Fan, Wei Wang, Xiaowen Chu",Ruibo Fan,English,"Graph Neural Networks (GNNs) are gaining huge traction recently as they achieve state-of-the-art performance on various graph-related problems. GNN training typically follows the standard Message Passing Paradigm, in which SpMM and SDDMM are the two essential sparse kernels. However, existing sparse GPU kernels are inefficient and may suffer from load imbalance, dynamics in GNN computing, poor memory efficiency, and tail effect. We propose two new kernels, Hybrid-Parallel SpMM (HP-SpMM) and Hybrid-Parallel SDDMM (HP-SDDMM), that efficiently perform SpMM and SDDMM on GPUs with a unified hybrid parallel strategy of mixing nodes and edges. In view of the emerging graph-sampling training, we design the Dynamic Task Partition (DTP) method to minimize the tail effect by exposing sufficient parallelism. We further devise the Hierarchical Vectorized Memory Access scheme to achieve?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/10177444/,3,fan2023fast,"Graph Neural Networks,GPU"
2024,arXiv preprint arXiv:2402.13499,Benchmarking and Dissecting the Nvidia Hopper GPU Architecture,"Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Qiang Wang, Xiaowen Chu",Weile Luo,English,"Graphics processing units (GPUs) are continually evolving to cater to the computational demands of contemporary general-purpose workloads, particularly those driven by artificial intelligence (AI) utilizing deep learning techniques. A substantial body of studies have been dedicated to dissecting the microarchitectural metrics characterizing diverse GPU generations, which helps researchers understand the hardware details and leverage them to optimize the GPU programs. However, the latest Hopper GPUs present a set of novel attributes, including new tensor cores supporting FP8, DPX, and distributed shared memory. Their details still remain mysterious in terms of performance and operational characteristics. In this research, we propose an extensive benchmarking study focused on the Hopper GPU. The objective is to unveil its microarchitectural intricacies through an examination of the new instruction-set architecture (ISA) of Nvidia GPUs and the utilization of new CUDA APIs. Our approach involves two main aspects. Firstly, we conduct conventional latency and throughput comparison benchmarks across the three most recent GPU architectures, namely Hopper, Ada, and Ampere. Secondly, we delve into a comprehensive discussion and benchmarking of the latest Hopper features, encompassing the Hopper DPX dynamic programming (DP) instruction set, distributed shared memory, and the availability of FP8 tensor cores. The microbenchmarking results we present offer a deeper understanding of the novel GPU AI function units and programming features introduced by the Hopper architecture. This newfound understanding is expected to?…",Article,https://arxiv.org/abs/2402.13499,3,luo2024benchmarking,"Instruction Latency, Tensor Core, PTX, Hopper,DPX, Asynchronous Execution, Distributed Shared Memory"
2024,arXiv preprint arXiv:2402.02105,ParZC: Parametric Zero-Cost Proxies for Efficient NAS,"Peijie Dong, Lujun Li, Xinglin Pan, Zimian Wei, Xiang Liu, Qiang Wang, Xiaowen Chu",Peijie Dong,English,"Recent advancements in Zero-shot Neural Architecture Search (NAS) highlight the efficacy of zero-cost proxies in various NAS benchmarks. Several studies propose the automated design of zero-cost proxies to achieve SOTA performance but require tedious searching progress. Furthermore, we identify a critical issue with current zero-cost proxies: they aggregate node-wise zero-cost statistics without considering the fact that not all nodes in a neural network equally impact performance estimation. Our observations reveal that node-wise zero-cost statistics significantly vary in their contributions to performance, with each node exhibiting a degree of uncertainty. Based on this insight, we introduce a novel method called Parametric Zero-Cost Proxies (ParZC) framework to enhance the adaptability of zero-cost proxies through parameterization. To address the node indiscrimination, we propose a Mixer Architecture with Bayesian Network (MABN) to explore the node-wise zero-cost statistics and estimate node-specific uncertainty. Moreover, we propose DiffKendall as a loss function to directly optimize Kendall's Tau coefficient in a differentiable manner so that our ParZC can better handle the discrepancies in ranking architectures. Comprehensive experiments on NAS-Bench-101, 201, and NDS demonstrate the superiority of our proposed ParZC compared to existing zero-shot NAS methods. Additionally, we demonstrate the versatility and adaptability of ParZC by transferring it to the Vision Transformer search space.",Article,https://arxiv.org/abs/2402.02105,2,dong2024parzc,"neural architecture search,Zero-Cost proxies"
2023,arXiv preprint arXiv:2310.12670,Reliable and Efficient In-Memory Fault Tolerance of Large Language Model Pretraining,"Yuxin Wang, Shaohuai Shi, Xin He, Zhenheng Tang, Xinglin Pan, Yang Zheng, Xiaoyu Wu, Amelie Chi Zhou, Bingsheng He, Xiaowen Chu",Yuxin Wang,English,"Extensive system scales (i.e. thousands of GPU/TPUs) and prolonged training periods (i.e. months of pretraining) significantly escalate the probability of failures when training large language models (LLMs). Thus, efficient and reliable fault-tolerance methods are in urgent need. Checkpointing is the primary fault-tolerance method to periodically save parameter snapshots from GPU memory to disks via CPU memory. In this paper, we identify the frequency of existing checkpoint-based fault-tolerance being significantly limited by the storage I/O overheads, which results in hefty re-training costs on restarting from the nearest checkpoint. In response to this gap, we introduce an in-memory fault-tolerance framework for large-scale LLM pretraining. The framework boosts the efficiency and reliability of fault tolerance from three aspects: (1) Reduced Data Transfer and I/O: By asynchronously caching parameters, i.e., sharded model parameters, optimizer states, and RNG states, to CPU volatile memory, Our framework significantly reduces communication costs and bypasses checkpoint I/O. (2) Enhanced System Reliability: Our framework enhances parameter protection with a two-layer hierarchy: snapshot management processes (SMPs) safeguard against software failures, together with Erasure Coding (EC) protecting against node failures. This double-layered protection greatly improves the survival probability of the parameters compared to existing checkpointing methods. (3) Improved Snapshotting Frequency: Our framework achieves more frequent snapshotting compared with asynchronous checkpointing optimizations under the same saving time?…",Article,https://arxiv.org/abs/2310.12670,3,wang2023reliable,"Fault Tolerance, Checkpoint Optimization, Large Language Model, 3D parallelism"
2022,Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of?…,An LLVM-based open-source compiler for NVIDIA GPUs,"Da Yan, Wei Wang, Xiaowen Chu",Da Yan,English,"We present GASS, an LLVM-based open-source compiler for NVIDIA GPU's SASS machine assembly. GASS is the first open-source compiler targeting SASS, and it provides a unified toolchain for currently fragmented low-level performance research on NVIDIA GPUs. GASS supports all recent architectures, including Volta, Turing, and Ampere. Our evaluation shows that our specialized optimizations deliver significant speedup over LLVM's algorithms.",Conference paper,https://dl.acm.org/doi/abs/10.1145/3503221.3508428,2,yan2022llvm,"GPU, SASS, compiler, LLVM"
2020,GLOBECOM 2020-2020 IEEE Global Communications Conference,Multi-Fingerprint for wireless localization in time-varying indoor environment,"Lu Yu, Yiu-Wing Leung, Xiaowen Chu, Joseph KY Ng",Lu Yu,English,"Fingerprint is one of the representative methods for wireless indoor localization. It uses a fingerprint database (measured in the offline phase) and the current received signal strengths (RSSs) (measured by the user's device in the online phase) to determine the location of this device. However, the RSSs and hence the localization accuracy would be affected by time-varying environmental factors (e.g., number of people in a shopping mall). In this paper, we propose a new method for wireless localization in time-varying indoor environments. In the offline phase, the proposed method measures extra information: it measures E fingerprint databases for E respective environmental conditions, where E is a design parameter (e.g., E=2 for the peak period and the non-peak period in a shopping mall). In the online phase, it leverages the extra information for better localization in time-varying indoor environment, even when?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9348052/,2,yu2020multi,"Wireless indoor localization, time-varying indoor environment, Wi-Fi, fingerprint"
2020,None,Benchmarking Deep Learning Models and Automated Model Design for COVID-19 Detection with Chest CT Scans (preprint),"Xin He, Shihao Wang, Shaohuai Shi, Xiaowen Chu, Jiangping Tang, Xin Liu, Chenggang Yan, Jiyong Zhang, Guiguang Ding",Xin He,English,"COVID-19 pandemic has spread all over the world for months. As its transmissibility and high pathogenicity seriously threaten people's lives, the accurate and fast detection of the COVID-19 infection is crucial. Although many recent studies have shown that deep learning based solutions can help detect COVID-19 based on chest CT scans, there lacks a consistent and systematic comparison and evaluation on these techniques. In this paper, we first build a clean and segmented CT dataset called Clean-CC-CCII by fixing the errors and removing some noises in a large CT scan dataset CC-CCII with three classes novel coronavirus pneumonia (NCP), common pneumonia (CP), and normal controls (Normal). After cleaning, our dataset consists of a total of 340,190 slices of 3,993 scans from 2,698 patients. Then we benchmark and compare the performance of a series of state-of-the-art (SOTA) 3D and 2D convolutional neural networks (CNNs). The results show that 3D CNNs outperform 2D CNNs in general. With extensive effort of hyperparameter tuning, we find that the 3D CNN model DenseNet3D121 achieves the highest accuracy of 88.63%(F1-score is 88.14% and AUC is 0.940), and another 3D CNN model ResNet3D34 achieves the best AUC of 0.959 (accuracy is 87.83% and F1-score is 86.04%). We further demonstrate that the mixup data augmentation technique can largely improve the model performance. At last, we design an automated deep learning methodology to generate a lightweight deep learning model MNas3DNet41 that achieves an accuracy of 87.14%, F1-score of 87.25%, and AUC of 0.957, which are on par with the best?…",Conference paper,https://scholar.google.com/scholar?cluster=15500412731343836107&hl=en&oi=scholarr,56,he2020benchmarking,"COVID-19, Benchmark, Deep learning, AutoML, Chest CT, Data augmentation"
2024,Proceedings of the 30th Annual International Conference on Mobile Computing?…,Asteroid: Resource-Efficient Hybrid Pipeline Parallelism for Collaborative DNN Training on Heterogeneous Edge Devices,"Shengyuan Ye, Liekang Zeng, Xiaowen Chu, Guoliang Xing, Xu Chen",Shengyuan Ye,English,"On-device Deep Neural Network (DNN) training has been recognized as crucial for privacy-preserving machine learning at the edge. However, the intensive training workload and limited onboard computing resources pose significant challenges to the availability and efficiency of model training. While existing works address these challenges through native resource management optimization, we instead leverage our observation that edge environments usually comprise a rich set of accompanying trusted edge devices with idle resources beyond a single terminal. We propose Asteroid, a distributed edge training system that breaks the resource walls across heterogeneous edge devices for efficient model training acceleration. Asteroid adopts a hybrid pipeline parallelism to orchestrate distributed training, along with a judicious parallelism planning for maximizing throughput under certain resource constraints?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3636534.3649363,1,ye2024asteroid,"Edge intelligence, distributed machine learning, data parallelism, pipeline parallelism, hybrid parallelism"
2024,arXiv preprint arXiv:2405.00314,Model quantization and hardware acceleration for vision transformers: A comprehensive survey,"Dayou Du, Gu Gong, Xiaowen Chu",Dayou Du,English,"Vision Transformers (ViTs) have recently garnered considerable attention, emerging as a promising alternative to convolutional neural networks (CNNs) in several vision-related applications. However, their large model sizes and high computational and memory demands hinder deployment, especially on resource-constrained devices. This underscores the necessity of algorithm-hardware co-design specific to ViTs, aiming to optimize their performance by tailoring both the algorithmic structure and the underlying hardware accelerator to each other's strengths. Model quantization, by converting high-precision numbers to lower-precision, reduces the computational demands and memory needs of ViTs, allowing the creation of hardware specifically optimized for these quantized algorithms, boosting efficiency. This article provides a comprehensive survey of ViTs quantization and its hardware acceleration. We first delve into the unique architectural attributes of ViTs and their runtime characteristics. Subsequently, we examine the fundamental principles of model quantization, followed by a comparative analysis of the state-of-the-art quantization techniques for ViTs. Additionally, we explore the hardware acceleration of quantized ViTs, highlighting the importance of hardware-friendly algorithm design. In conclusion, this article will discuss ongoing challenges and future research paths. We consistently maintain the related open-source materials at https://github.com/DD-DuDa/awesome-vit-quantization-acceleration.",Conference paper,https://arxiv.org/abs/2405.00314,1,du2024model,"Vision Transformers, hardware acceleration"
2024,Proceedings of the Nineteenth European Conference on Computer Systems,ScheMoE: An Extensible Mixture-of-Experts Distributed Training System with Tasks Scheduling,"Shaohuai Shi, Xinglin Pan, Qiang Wang, Chengjian Liu, Xiaozhe Ren, Zhongzhe Hu, Yu Yang, Bo Li, Xiaowen Chu",Shaohuai Shi,English,"In recent years, large-scale models can be easily scaled to trillions of parameters with sparsely activated mixture-of-experts (MoE), which significantly improves the model quality while only requiring a sub-linear increase in computational costs. However, MoE layers require the input data to be dynamically routed to a particular GPU for computing during distributed training. The highly dynamic property of data routing and high communication costs in MoE make the training system low scaling efficiency on GPU clusters. In this work, we propose an extensible and efficient MoE training system, ScheMoE, which is equipped with several features. 1) ScheMoE provides a generic scheduling framework that allows the communication and computation tasks in training MoE models to be scheduled in an optimal way. 2) ScheMoE integrates our proposed novel all-to-all collective which better utilizes intra- and inter-connect?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3627703.3650083,1,shi2024schemoe,"Distributed Deep Learning, large language model,Mixture-of-Experts, Scheduling"
2024,Proceedings of the AAAI Conference on Artificial Intelligence,CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning,"Qingsong Yan, Qiang Wang, Kaiyong Zhao, Jie Chen, Bo Li, Xiaowen Chu, Fei Deng",Qingsong Yan,English,"Neural Radiance Fields (NeRF) have demonstrated impressive performance in novel view synthesis. However, NeRF and most of its variants still rely on traditional complex pipelines to provide extrinsic and intrinsic camera parameters, such as COLMAP. Recent works, like NeRFmm, BARF, and L2G-NeRF, directly treat camera parameters as learnable and estimate them through differential volume rendering. However, these methods work for forward-looking scenes with slight motions and fail to tackle the rotation scenario in practice.To overcome this limitation, we propose a novel \underline{c}amera parameter \underline{f}ree neural radiance field (CF-NeRF), which incrementally reconstructs 3D representations and recovers the camera parameters inspired by incremental structure from motion (SfM). Given a sequence of images, CF-NeRF estimates the camera parameters of images one by one and reconstructs the scene through initialization, implicit localization, and implicit optimization. To evaluate our method, we use a challenging real-world dataset NeRFBuster which provides 12 scenes under complex trajectories. Results demonstrate that CF-NeRF is robust to camera rotation and achieves state-of-the-art results without providing prior information and constraints.",Article,https://ojs.aaai.org/index.php/AAAI/article/view/28464,1,yan2024cf,"incremental learning,camera"
2024,arXiv preprint arXiv:2402.07011,FedImpro: Measuring and Improving Client Update in Federated Learning,"Zhenheng Tang, Yonggang Zhang, Shaohuai Shi, Xinmei Tian, Tongliang Liu, Bo Han, Xiaowen Chu",Zhenheng Tang,English,"Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models. First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients. Then, we propose FedImpro, to construct similar conditional distributions for local training. Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions. This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL. Experimental results show that FedImpro can help FL defend against data heterogeneity and enhance the generalization performance of the model.",Article,https://arxiv.org/abs/2402.07011,1,tang2024fedimpro,Federated Learning
2023,2023 IEEE International Conference on Medical Artificial Intelligence (MedAI?…,MedPipe: End-to-End Joint Search of Data Augmentation and Neural Architecture for 3D Medical Image Classification,"Xin He, Xiaowen Chu",Xin He,English,"Data augmentation plays a crucial role in deep learning-based medical imaging analysis, but manually designing tailored data augmentation strategies for each dataset is impractical. Although automatic data augmentation (ADA) techniques have been explored, they often focus solely on data augmentation without considering the importance of neural architecture. Similarly, neural architecture search (NAS) methods mainly concentrate on optimizing the neural architecture while overlooking the impact of data augmentation. However, both data augmentation and neural architecture are interrelated and should be considered together. The joint optimization of data augmentation and neural architecture can lead to improved model performance by harnessing the complementary effects of customized data augmentation strategies and compatible neural architectures. Despite this, the seamless integration of data?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/10403239/,1,he2023medpipe,"Neural Architecture Search (NAS), Automatic Data Augmentation (ADA), Medical Image Classification"
2023,International Conference on Algorithms and Architectures for Parallel?…,SMCoEdge: Simultaneous Multi-server Offloading for Collaborative Mobile Edge Computing,"Changfu Xu, Yupeng Li, Xiaowen Chu, Haodong Zou, Weijia Jia, Tian Wang",Changfu Xu,English,"Collaborative Mobile Edge Computing (MEC) has emerged as a promising solution for low service delay in computation-intensive Internet of Things (IoT) applications. However, current approaches typically perform offline task partitioning and offload each subtask to an Edge Server (ES) for processing. This leads to varying delays in subtask processing across different ESs, resulting in a high make-span of task offloading. To address this issue, we propose a novel approach called SMCoEdge, which utilizes simultaneous multi-ES offloading to minimize the make-span of task offloading for computation-intensive IoT applications. Specifically, we formulate our problem as a mixed integer non-linear programming problem and prove its NP-hardness. We then decompose our problem into two sub-problems of multi-ES selection and task allocation, and propose a Deep Reinforcement Learning-based Simultaneous Multi?…",Conference paper,https://link.springer.com/chapter/10.1007/978-981-97-0808-6_5,1,xu2023smcoedge,"Mobile edge computing,Edge-edge collaboration,Task allocation,Simultaneous multi-server offloading"
2023,2023 IEEE/ACM 31st International Symposium on Quality of Service (IWQoS),Improving Fairness in Coexisting 5G and Wi-Fi Network on Unlicensed Band with URLLC,"Haodong Zou, Yupeng Li, Xiaowen Chu, Changfu Xu, Tian Wang",Haodong Zou,English,"To meet the growing need of mobile traffic with Ultra-Reliable and Low Latency Communication (URLLC) requirement, 5G New Radio (NR) is extending from licensed band to unlicensed band on which Wi-Fi has already been operated, resulting in coexisting NR/Wi-Fi network. Existing works have made great efforts on throughput and latency of coexisting NR/Wi-Fi network. However, excessive NR requests offloaded from licensed band lead to unfair utilization of unlicensed band, which further causes unsatisfaction on URLLC and performance degradation of Wi-Fi. In this paper, we propose a novel Reinforcement Learning based Transmission Revoking Approach (RL-TRA) to address this problem aiming at fairer utilization of unlicensed band restrained by URLLC. Firstly, we formulate the coexistence problem of NR/Wi-Fi as integer non-linear programming and show its NP-hardness. Secondly, we decompose the?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/10188806/,1,zou2023improving,"Federated Learning,gradient"
2023,IEEE INFOCOM 2023-IEEE Conference on Computer Communications,PipeMoE: Accelerating Mixture-of-Experts through Adaptive Pipelining,"Shaohuai Shi, Xinglin Pan, Xiaowen Chu, Bo Li",Shaohuai Shi,English,"Large models have attracted much attention in the AI area. The sparsely activated mixture-of-experts (MoE) technique pushes the model size to a trillion-level with a sub-linear increase of computations as an MoE layer can be equipped with many separate experts, but only one or two experts need to be trained for each input data. However, the feature of dynamically activating experts of MoE introduces extensive communications in distributed training. In this work, we propose PipeMoE to adaptively pipeline the communications and computations in MoE to maximally hide the communication time. Specifically, we first identify the root reason why a higher pipeline degree does not always achieve better performance in training MoE models. Then we formulate an optimization problem that aims to minimize the training iteration time. To solve this problem, we build performance models for computation and communication?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/10228874/,2,shi2023pipemoe,"Distributed Deep Learning, Communication-Efficient Training, Mixture-of-Experts, Pipelining"
2022,2022 International Conference on 3D Vision (3DV),SphereDepth: Panorama Depth Estimation from Spherical Domain,"Qingsong Yan, Qiang Wang, Kaiyong Zhao, Bo Li, Xiaowen Chu, Fei Deng",Qingsong Yan,English,"The panorama image can simultaneously demonstrate complete information of the surrounding environment and has many advantages in virtual tourism, games, robotics, etc. However, the progress of panorama depth estimation cannot completely solve the problems of distortion and discontinuity caused by the commonly used projection methods. This paper proposes SphereDepth, a novel panorama depth estimation method that predicts the depth directly on the spherical mesh without projection preprocessing. The core idea is to establish the relationship between the panorama image and the spherical mesh and then use a deep neural network to extract features on the spherical domain to predict depth. To address the efficiency challenges brought by the high-resolution panorama data, we introduce two hyper-parameters for the proposed spherical mesh processing framework to balance the inference speed?…",Article,https://ieeexplore.ieee.org/abstract/document/10044404/,1,yan2022spheredepth,"depth estimation,spherical demain"
2022,IEEE Transactions on Green Communications and Networking,Energy-Efficient Online Scheduling of Transformer Inference Services on GPU Servers,"Yuxin Wang, Qiang Wang, Xiaowen Chu",Yuxin Wang,English,"Cloud service providers are deploying Transformer-based deep learning models on GPU servers to support many online inference-as-a-service (IAAS) applications, given the predominant performance of Transformers in natural language processing (NLP) tasks. However, Transformers’ inherent high complexity and large model size (e.g., billions to hundreds of billions of parameters) tax the resource-constrained GPU servers. Improving the energy efficiency and payload capability of IAAS without violating the service-level agreement (SLA) becomes a practical challenge for service providers. This work conducts a comprehensive study on the inference performance and energy efficiency of Transformer models. First, we empirically characterize essential performance metrics, including latency, throughput, and energy consumption on NVIDIA GPUs under various workload configurations. Second, we establish a?…",Article,https://ieeexplore.ieee.org/abstract/document/9769760/,1,wang2022energy,"Cloud computing, energy conservation, artificial intelligence, transformer, GPU computing"
2021,ICC 2021-IEEE International Conference on Communications,Traffic Management for Distributed Machine Learning in RDMA-enabled Data Center Networks,"Weihong Yang, Yang Qin, Zukai Jiang, Xiaowen Chu",Weihong Yang,English,"It has become a common practice to train large machine learning (ML) models across a cluster of computing nodes connected by RDMA-enabled networks. However, the communication overhead caused by parameter synchronization deteriorates the performance of such distributed ML (DML), especially in a large-scale setting. This paper tackles this issue by developing a traffic management scheme to support DML traffic, called TMDML (Traffic Management for DML), which needs only a minor modification to the existing RDMA congestion control scheme DCQCN. We assume that there is only one instance of DML workload running in a network. Existing literature has shown that Fat-Tree, a predominant topology in the data center, poorly supports DML compared with BCube. With our proposed TMDML, training DML in Fat-Tree can achieve better performance than that in BCube. We first study the impact of multi?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9500355/,1,yang2021traffic,"distributed machine learning (DML), multi-bottlenecks, RDMA, transport protocol"
2021,Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of?…,Simplifying low-level GPU programming with GAS,"Da Yan, Wei Wang, Xiaowen Chu",Da Yan,English,"Many low-level optimizations for NVIDIA GPU can only be implemented in native hardware assembly (SASS). However, programming in SASS is unproductive and not portable. To simplify low-level GPU programming, we present GAS (Gpu ASsembly), a PTX-like language that provides a stable instruction set across hardware architectures while giving programmers a low-level control of code execution. We demonstrate that GAS can be used with ease for low-level benchmarking and performance tuning in the context of Tensor Core HGEMM.",Conference paper,https://dl.acm.org/doi/abs/10.1145/3437801.3441591,1,yan2021simplifying,"GPU, SASS, compiler"
2024,Forty-first International Conference on Machine Learning,Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models,"Peijie Dong, Lujun Li, Zhenheng Tang, Xiang Liu, Xinglin Pan, Qiang Wang, Xiaowen Chu",Peijie Dong,English,"Despite the remarkable capabilities, Large Language Models (LLMs) face deployment challenges due to their extensive size. Pruning methods drop a subset of weights to accelerate, but many of them require retraining, which is prohibitively expensive and computationally demanding. Recently, post-training pruning approaches introduced novel metrics, enabling the pruning of LLMs without retraining. However, these metrics require the involvement of human experts and tedious trial and error. To efficiently identify superior pruning metrics, we develop an automatic framework for searching symbolic pruning metrics using genetic programming. In particular, we devise an elaborate search space encompassing the existing pruning metrics to discover the potential symbolic pruning metric. We propose an opposing operation simplification strategy to increase the diversity of the population. In this way, Pruner-Zero allows auto-generation of symbolic pruning metrics. Based on the searched results, we explore the correlation between pruning metrics and performance after pruning and summarize some principles. Extensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot tasks demonstrate that our Pruner-Zero obtains superior performance than SOTA post-training pruning methods. Code at: https://github.com/pprp/Pruner-Zero.",Conference paper,https://openreview.net/forum?id=1tRLxQzdep,0,dong2024pruner,"large language models,pruning"
2024,arXiv preprint arXiv:2405.17245,Galaxy: A Resource-Efficient Collaborative Edge AI System for In-situ Transformer Inference,"Shengyuan Ye, Jiangsu Du, Liekang Zeng, Wenzhong Ou, Xiaowen Chu, Yutong Lu, Xu Chen",Shengyuan Ye,English,"Transformer-based models have unlocked a plethora of powerful intelligent applications at the edge, such as voice assistant in smart home. Traditional deployment approaches offload the inference workloads to the remote cloud server, which would induce substantial pressure on the backbone network as well as raise users' privacy concerns. To address that, in-situ inference has been recently recognized for edge intelligence, but it still confronts significant challenges stemming from the conflict between intensive workloads and limited on-device computing resources. In this paper, we leverage our observation that many edge environments usually comprise a rich set of accompanying trusted edge devices with idle resources and propose Galaxy, a collaborative edge AI system that breaks the resource walls across heterogeneous edge devices for efficient Transformer inference acceleration. Galaxy introduces a novel hybrid model parallelism to orchestrate collaborative inference, along with a heterogeneity-aware parallelism planning for fully exploiting the resource potential. Furthermore, Galaxy devises a tile-based fine-grained overlapping of communication and computation to mitigate the impact of tensor synchronizations on inference latency under bandwidth-constrained edge environments. Extensive evaluation based on prototype implementation demonstrates that Galaxy remarkably outperforms state-of-the-art approaches under various edge environment setups, achieving up to 2.5x end-to-end latency reduction.",Article,https://arxiv.org/abs/2405.17245,0,ye2024galaxy,"Transformer,collaborative edge AI system"
2024,Proceedings of the 29th ACM International Conference on Architectural?…,DTC-SpMM: Bridging the Gap in Accelerating General Sparse Matrix Multiplication with Tensor Cores,"Ruibo Fan, Wei Wang, Xiaowen Chu",Ruibo Fan,English,"Sparse Matrix-Matrix Multiplication (SpMM) is a building-block operation in scientific computing and machine learning applications. Recent advancements in hardware, notably Tensor Cores (TCs), have created promising opportunities for accelerating SpMM. However, harnessing these hardware accelerators to speed up general SpMM necessitates considerable effort. In this paper, we undertake a comprehensive analysis of the state-of-the-art techniques for accelerating TC-based SpMM and identify crucial performance gaps. Drawing upon these insights, we propose DTC-SpMM, a novel approach with systematic optimizations tailored for accelerating general SpMM on TCs. DTC-SpMM encapsulates diverse aspects, including efficient compression formats, reordering methods, and runtime pipeline optimizations. Our extensive experiments on modern GPUs with a diverse range of benchmark matrices?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3620666.3651378,0,fan2024dtc,"Sparse Matrix-Matrix Multiplication, SpMM, unstructured sparsity, GPU, Tensor Core"
2024,IEEE Transactions on Industrial Informatics,Multipath Based Congestion Propagation via Information Network Interaction in IIoT,"Ruonan Li, Yang Qin, Jie Liu, Xiaowen Chu, Jinlong Li",Ruonan Li,English,"The Industrial Internet of Things (IIoT) has found extensive applications in intelligent transportation. However, as the number of vehicles increases, the issue of traffic congestion becomes more prominent, emphasizing the need for accurate congestion propagation prediction to enhance traffic conditions. Current methods for predicting congestion propagation lack consideration for the influence of communication networks and do not incorporate the path characteristics of congestion propagation. Therefore, we propose a path-based congestion propagation model, UAU_SIS_Path, employing multigrain abstraction of traffic congestion and information propagation. Specifically, UAU_SIS_Path effectively captures the propagation dynamics of congestion in IIoT by leveraging the interaction of two-layer networks and the path propagation characteristics of traffic congestion. Subsequently, we validate the effectiveness of?…",Article,https://ieeexplore.ieee.org/abstract/document/10470384/,0,li2024multipath,"Congestion propagation, intelligent transportation, interaction of two-layer networks, path propagation."
2023,IEEE Transactions on Intelligent Vehicles,LF-Net: A Learning-based Frenet Planning Approach for Urban Autonomous Driving,"Zihan Yu, Meixin Zhu, Kehua Chen, Xiaowen Chu, Xuesong Wang",Zihan Yu,English,"Learning-based approaches hold great potential for autonomous urban driving motion planning. Compared to traditional rule-based methods, they offer greater flexibility in planning safe and human-like trajectories based on human driver demonstration data and diverse traffic scenarios. Frenet planning is widely applied in autonomous driving motion planning due to itssimplerepresentationofself-drivingvehicleinformation.However,it is challenging to select proper terminal statesand generate human-like trajectories. To address this issue, we propose a learning-based Frenet planning network (LF-Net) that learns a policy to sample and select the most human-like terminal states and then generate safe trajectories. The LF-Net includes 1) a Transformer-based sub-network that encodes environmental and vehicle interaction features, 2) a classification and scoring subnetwork based on cross-attention mechanisms that?…",Article,https://ieeexplore.ieee.org/abstract/document/10319340/,0,yu2023lf,"Autonomous driving, deep learning, frenet planning, motion planning, transformer, urban driving"
2023,IEEE Network,Guest Editorial: Interplay Between Machine Learning and Networking Systems,"Xiaowen Chu, Shadi Ibrahim, Jia Liu, Shiqiang Wang, Chuan Wu, Rongfei Zeng",Xiaowen Chu,English,"IEEE Network? July/August 2023 73 erated Learning) to detect and mitigate the effects of Byzantine agents, while maintaining high accuracy and efficiency. In the article,“FDSFL: Filtering Defense Strategies Toward Targeted Poisoning Attacks in IIoT-based Federated Learning Networking System,” the authors present filtering defense strategies to mitigate the impact of these attacks and ensure the security and privacy of the data being used in the federated learning process. In the article,“BC-MetaCast: A Blockchain-Enhanced Intelligent Computing Framework for Metaverse Livecast,” the authors present a cutting-edge blockchain-enhanced intelligent computing framework for metaverse livecast. The proposed framework offers an efficient, decentralized, and truthful task assignment for live stream processing.In the article,“Blockchain-Enabled Cross-Layer Radio Frequency Fingerprinting Identification with Machine Learning for IIoT,” a blockchain-enabled cross-layer radio frequency fingerprinting (RFF) identification framework is proposed. It enhances security in the industrial Internet of Things (IIoT) by using physical layer authentication, machine learning, and blockchain technology.",Article,https://kevinliu-osu.github.io/publications/Guest_Editorial_Interplay_Between_Machine_Learning_and_Networking_Systems.pdf,0,chu2023guest,None
2023,None,All for One and One for All: A Collaborative FL Framework for Generic Federated Learning with Personalized Plug-ins,"Lei Shen, Zhenheng Tang, Lijun Wu, Yonggang Zhang, Xiaowen Chu, Tao Qin, Bo Han",Lei Shen,English,"Personalized federated learning (PFL) mitigates the notorious data heterogeneity issue in generic federated learning (GFL) by assuming that client models only need to fit on local datasets individually. However, real-world FL clients may meet with test data from other distributions. To endow clients with the ability to handle other datasets, we theoretically formulate a new problem named as Selective FL (SFL), bridging the GFL and PFL together. To practically solve SFL, we design a general effective framework named as Hot-Pluggable Federated Learning (HPFL). In HPFL, clients firstly learn a global shared feature extractor. Next, with the frozen feature extractor, multiple personalized plug-in modules are individually learned based on the local data and saved in a modular store on the server. In inference stage, an accurate selection algorithm allows clients to choose and download suitable plug-in modules from the modular store to achieve the high generalization performance on target data distribution. We conduct comprehensive experiments and ablation studies following common FL settings including four datasets and three neural networks, showing that HPFL significantly outperforms advanced FL algorithms. Additionally, we empirically show the remarkable potential of HPFL to resolve other practical FL problems like continual federated learning and discuss its possible applications in one-shot FL, anarchic FL and an FL plug-in market.",Conference paper,https://openreview.net/forum?id=8rhHI6C8iC,0,shenall,Federated Learning
2023,arXiv preprint arXiv:2309.09547,Stochastic Performance Analysis of Phase Decomposition in Hyperledger Fabric,"Canhui Wang, Xiaowen Chu",Canhui Wang,English,"Hyperledger Fabric is one of the most popular permissioned blockchain platforms. Although many existing works on the overall system performance of Hyperledger Fabric are available, a decomposition of each phase in Hyperledger Fabric remains to be explored. Admittedly, the overall system performance of Hyperledger Fabric might provide an end-user with satisfied performance information when invoking a transaction; however, it is far from informative when deploying a distributed system with specific performance goals, except for understanding each phase in Hyperledger Fabric. In this paper, we develop a measurement framework to characterize each phase's transaction and block data in Hyperledger Fabric based on the Fabric SDK Nodejs, where we thoroughly analyze and open source the implementation details of the measurement framework. We evaluate the performance of Hyperledger Fabric and have some interesting observations; 1. The number of CPU cores has a linear impact on the throughput of an endorsing peer. 2. The Raft-based ordering service shows good scalability with the number of ordering service nodes. 3. The communication latencies between the client and service in Hyperledger Fabric are significant. We then identify each phase's dominant latency in Hyperledger Fabric via primitive operation analysis and propose a stochastic computation model for performance analysis. We also use the alpha-beta communication model to analyze the corresponding communication latency. Finally, we validate the accuracy of the performance model on both local and cloud clusters. The experiment results and the?…",Article,https://arxiv.org/abs/2309.09547,0,wang2023stochastic,"Hyperledger fabric, blockchain, phase decomposition, stochastic computation model, alpha-beta communication mode"
2022,2022 18th International Conference on Mobility,A Quality-Aware Rendezvous Framework for Cognitive Radio Networks,"Hai Liu, Lu Yu, Chung Keung Poon, Zhiyong Lin, Yiu-Wing Leung, Xiaowen Chu",Hai Liu,English,"In cognitive radio networks, rendezvous is a fundamental operation by which cognitive users establish communication links. Most of existing works were devoted to shortening the time-to-rendezvous (TTR) but paid little attention to qualities of the channels on which rendezvous is achieved. In fact, qualities of channels, such as resistance to primary users' activities, have a great effect on the rendezvous operation. If users achieve a rendezvous on a low-quality channel, the communication link is unstable and the communication performance is poor. In this case, re- rendezvous is required which results in considerable communication overhead and a large latency. In this paper, we first show that actual TTRs of existing rendezvous solutions increase by 65.40-104.38% if qualities of channels are not perfect. Then we propose a Quality-Aware Rendezvous Framework (QARF) that can be applied to any existing ren-dezvous algorithms to?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/10076613/,0,liu2022quality,"Cognitive radio networks, Channel hopping, Quality-Aware, Channel-duplicate"
2022,None,Harnessing Client Drift with Decoupled Gradient Dissimilarity,"TANG Zhenheng, Yonggang Zhang, Shaohuai Shi, Xinmei Tian, Tongliang Liu, Bo Han, Xiaowen Chu",TANG Zhenheng,English,"The performance of Federated learning (FL) typically suffers from client drift caused by heterogeneous data, where data distributions vary with clients. Recent studies show that the gradient dissimilarity between clients induced by the data distribution discrepancy causes the client drift. Thus, existing methods mainly focus on correcting the gradients. However, it is challenging to identify which client should (or not) be corrected. This challenge raises a series of questions: will the local training, without gradient correction, contribute to the server model's generalization of other clients' distributions? when the generalization contribution holds? how to address the challenge when it fails? To answer these questions, we analyze the generalization contribution of local training and conclude that the generalization contribution of local training is bounded by the conditional Wasserstein distance between clients' distributions. Thus, the key to promote generalization contribution is to leverage similar conditional distributions for local training. As collecting data distribution can cause privacy leakage, we propose decoupling the deep models, i.e., splitting into high-level models and low-level models, for harnessing client drift. Namely, high-level models are trained on shared feature distributions, causing promoted generalization contribution and alleviated gradient dissimilarity. Experimental results demonstrate that FL with decoupled gradient dissimilarity is robust to data heterogeneity.",Conference paper,https://openreview.net/forum?id=bp6Lr0TmmUS,0,zhenhengharnessing," Graphics processing units, dynamic voltage and frequency scaling, task scheduling"
2022,None,FedPD: Defying data heterogeneity through privacy distillation,"Zhiqin Brian Yang, Yonggang Zhang, Yu Zheng, TANG Zhenheng, Xiaowen Chu, Hao Peng, Bo Han",Zhiqin Brian Yang,English,"Model performance of federated learning (FL) typically suffers from data heterogeneity, i.e., data distribution varies with clients. Advanced works have already shown great potential for sharing client information to mitigate data heterogeneity. Yet, some literature shows a dilemma in preserving strong privacy and promoting model performance simultaneously. Revisiting the purpose of sharing information motivates us to raise the fundamental questions: Which part of the data is more critical for model generalization? Which part of the data is more privacy-sensitive? Can we solve this dilemma by sharing useful (for generalization) features and maintaining more sensitive data locally? Our work sheds light on data-dominated sharing and training, in a way that we decouple original training data into sensitive features and generalizable features. To be specific, we propose a \textbf{Fed}erated \textbf{P}rivacy \textbf{D}istillation framework named FedPD to alleviate the privacy-performance dilemma. Namely, FedPD keeps the distilled sensitive features locally and constructs a global dataset using shared generalizable features in a differentially private manner. Accordingly, clients can perform local training on both the local and securely shared data for acquiring high model performance and avoiding the leakage of not distilled privacy. Theoretically, we demonstrate the superiority of the sharing-only useful feature strategy over sharing raw data. Empirically, we show the efficacy of FedPD in promoting performance with comprehensive experiments.",Conference paper,https://openreview.net/forum?id=IERSU0La-Nt,0,yangfedpd,"Federated Learning,Privacy Distillation"
2022,IEEE Transactions on Network Science and Engineering,Guest Editorial: Introduction to the Special Section on Communication-Efficient Distributed Machine Learning,"Xiaowen Chu, Fausto Giunchiglia, Giovanni Neglia, David Gregg, Jiangchuang Liu",Xiaowen Chu,English,"The papers in this special section focus on communication-efficient distributed machine learning. Machine learning, especially deep learning, has been successfully applied in a wealth of practical AI applications in the field of computer vision, natural language processing, healthcare, finance, robotics, etc. With the increasing size of machine learning models and training data sets, training deep learning models requires significant amount of computations and may take days to months on a single GPU or TPU. Therefore, it becomes a common practice to exploit distributed machine learning to accelerate the training process with multiple processors. Distributed machine learning typically requires the processors to exchange information repeatedly throughout the training process. With the fast-growing computing power of the AI processors, the data communications among processors gradually become the performance?…",Article,https://ieeexplore.ieee.org/abstract/document/9808097/,0,chu2022guest,"machine learning,deep learning"
