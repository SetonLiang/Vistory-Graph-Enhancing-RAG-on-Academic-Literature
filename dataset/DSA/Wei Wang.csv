Year,Sources,Name,Authors,First Author,Chinese/English,Abstract,Venues,doi,Citation,Id,Keywords
2021,Proceedings of the AAAI Conference on Artificial Intelligence,Two-Stream Convolution Augmented Transformer for Human Activity Recognition,"Bing Li, Wei Cui, Wei Wang, Le Zhang, Zhenghua Chen, Min Wu",Bing Li,English,"Recognition of human activities is an important task due to its far-reaching applications such as healthcare system, context-aware applications, and security monitoring. Recently, WiFi based human activity recognition (HAR) is becoming ubiquitous due to its non-invasiveness. Existing WiFi-based HAR methods regard WiFi signals as a temporal sequence of channel state information (CSI), and employ deep sequential models (eg, RNN, LSTM) to automatically capture channel-over-time features. Although being remarkably effective, they suffer from two major drawbacks. Firstly, the granularity of a single temporal point is blindly elementary for representing meaningful CSI patterns. Secondly, the time-over-channel features are also important, and could be a natural data augmentation. To address the drawbacks, we propose a novel Two-stream Convolution Augmented Human Activity Transformer (THAT) model. Our model proposes to utilize a two-stream structure to capture both time-over-channel and channel-over-time features, and use the multi-scale convolution augmented transformer to capture range-based patterns. Extensive experiments on four real experiment datasets demonstrate that our model outperforms state-of-the-art models in terms of both effectiveness and efficiency.",Article,https://ojs.aaai.org/index.php/AAAI/article/view/16103,113,li2021two,"Human Activity Recognition(HAR), Transformer"
2020,IEEE Transactions on Knowledge & Data Engineering,An experimental study of state-of-the-art entity alignment approaches,"Xiang Zhao, Weixin Zeng, Jiuyang Tang, Wei Wang, Fabian Suchanek",Xiang Zhao,English,"Entity alignment (EA) finds equivalent entities that are located in different knowledge graphs (KGs), which is an essential step to enhance the quality of KGs, and hence of significance to downstream applications (e.g., question answering and recommendation). Recent years have witnessed a rapid increase of EA approaches, yet the relative performance of them remains unclear, partly due to the incomplete empirical evaluations, as well as the fact that comparisons were carried out under different settings (i.e., datasets, information used as input, etc.). In this paper, we fill in the gap by conducting a comprehensive evaluation and detailed analysis of state-of-the-art EA approaches. We first propose a general EA framework that encompasses all the current methods, and then group existing methods into three major categories. Next, we judiciously evaluate these solutions on a wide range of use cases, based on their?…",Article,https://ieeexplore.ieee.org/abstract/document/9174835/,104,zhao2020experimental,None
2020,Proceedings of the 43rd International ACM SIGIR Conference on Research and?…,Degree-Aware Alignment for Entities in Tail,"Weixin Zeng, Xiang Zhao, Wei Wang, Jiuyang Tang, Zhen Tan",Weixin Zeng,English,"Entity alignment (EA) is to discover equivalent entities in knowledge graphs (KGs), which bridges heterogeneous sources of information and facilitates the integration of knowledge. Existing EA solutions mainly rely on structural information to align entities, typically through KG embedding. Nonetheless, in real-life KGs, only a few entities are densely connected to others, and the rest majority possess rather sparse neighborhood structure. We refer to the latter as long-tail entities, and observe that such phenomenon arguably limits the use of structural information for EA.  To mitigate the issue, we revisit and investigate into the conventional EA pipeline in pursuit of elegant performance. For pre-alignment, we propose to amplify long-tail entities, which are of relatively weak structural information, with entity name information that is generally available (but overlooked) in the form of concatenated power mean word?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3397271.3401161,62,zeng2020degree,"Entity alignment, Long-tail, Co-attention, Iterative training"
2021,IEEE Transactions on Industrial Informatics,Unsupervised domain adaptation for nonintrusive load monitoring via adversarial and joint adaptation network,"Yinyan Liu, Li Zhong, Jing Qiu, Junda Lu, Wei Wang",Yinyan Liu,English,"Nonintrusive load monitoring (NILM) is a technique to disaggregate an appliance's load consumption from the aggregate load in a house. Monitoring the energy behavior has become increasingly important for home energy management. For many machine learning-based models, model training needs enough, and diverse appliance-level labeled data from different houses, which is very time-consuming, expensive, and unacceptable for users. In this article, we propose an algorithm based on the adversarial network and the joint adaptation network for energy disaggregation to decrease the distribution gaps of both the feature space and the label space between the source and target domains. With only very limited labeled data in the source domain and enough unlabeled data in the target domain, our proposed algorithm can obtain satisfactory accuracy results for NILM. Extensive experiments for intradomain and?…",Article,https://ieeexplore.ieee.org/abstract/document/9380223/,57,liu2021unsupervised,"Adversarial network, joint probability adaptation, nonintrusive load monitoring, unsupervised Mon (UDA)"
2020,Proceedings of the VLDB Endowment,VHP: approximate nearest neighbor search via virtual hypersphere partitioning,"Kejing Lu, Hongya Wang, Wei Wang, Mineichi Kudo",Kejing Lu,English,"Locality sensitive hashing (LSH) is a widely practiced c-approximate nearest neighbor(c-ANN) search algorithm in high dimensional spaces. The state-of-the-art LSH based algorithm searches an unbounded and irregular space to identify candidates, which jeopardizes the efficiency. To address this issue, we introduce the concept of virtual hypersphere partitioning. The core idea is to impose a virtual hypersphere, centered at the query, in the original feature space and only examine points inside the hypersphere. The search space of a hypersphere is isotropic and bounded, and thus more efficient than the existing one. In practice, we use multiple physical hyperspheres with different radii in corresponding projection subspaces to emulate the single virtual hypersphere. We also developed a principled method to compute the hypersphere radii for given success probability. Based on virtual hypersphere partitioning, we propose a novel disk-based indexing and searching scheme VHP to answer c-ANN queries. In the indexing phase, VHP stores LSH projections with independent B+-trees. To process a query, VHP keeps increasing the radii of physical hyperspheres coordinately, which in effect amounts to enlarging the virtual hypersphere, to accommodate more candidates until the success probability is met. Rigorous theoretical analysis shows that the proposed algorithm supports c-ANN search for arbitrarily small c >= 1 with probability guarantee. Extensive experiments on a variety of datasets, including the billion-scale ones, demonstrate that VHP could achieve different tradeoffs between efficiency and accuracy, and achieves up to 2x speedup in?…",Article,https://eprints.lib.hokudai.ac.jp/dspace/handle/2115/79717,46,lu2020vhp,"Approximate Nearest Neighbor Search(ANNS),Virtual Hypersphere Partitioning"
2020,AAAI,GraphER: Token-Centric Entity Resolution with Graph Convolutional Neural Networks,"Bing Li, Wei Wang, Yifang Sun, Linhan Zhang, Muhammad Asif Ali, Yi Wang",Bing Li,English,"Entity resolution (ER) aims to identify entity records that refer to the same real-world entity, which is a critical problem in data cleaning and integration. Most of the existing models are attribute-centric, that is, matching entity pairs by comparing similarities of pre-aligned attributes, which require the schemas of records to be identical and are too coarse-grained to capture subtle key information within a single attribute. In this paper, we propose a novel graph-based ER model GraphER. Our model is token-centric: the final matching results are generated by directly aggregating token-level comparison features, in which both the semantic and structural information has been softly embedded into token embeddings by training an Entity Record Graph Convolutional Network (ER-GCN). To the best of our knowledge, our work is the first effort to do token-centric entity resolution with the help of GCN in entity resolution task. Extensive experiments on two real-world datasets demonstrate that our model stably outperforms state-of-the-art models.",Conference paper,https://aaai.org/ojs/index.php/AAAI/article/view/6330,45,li2020grapher,"Entity Resolution, Graph Neural Network(GNN)"
2023,arXiv preprint arXiv:2305.12870,Lion: Adversarial Distillation of Closed-Source Large Language Model,"Yuxin Jiang, Chunkit Chan, Mingyang Chen, Wei Wang",Yuxin Jiang,English,"The practice of transferring knowledge from a sophisticated, closed-source large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher model to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any reciprocal ""feedback""--identifying challenging instructions where the student model's performance falls short--to boost the student model's proficiency iteratively. To this end, we propose a novel adversarial distillation framework for a more efficient knowledge transfer. Leveraging the versatile role adaptability of LLMs, we prompt the closed-source model to identify ""hard"" instructions and generate new ""hard"" instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. By applying this adversarial framework, we successfully transfer knowledge from ChatGPT to a 7B student model (named Lion), achieving nearly 95% capability approximation using a mere 70k training data. We aspire that this proposed model may serve as the baseline to reflect the performance of ChatGPT, especially the open-source instruction-following language model baseline for our community.",Article,https://arxiv.org/abs/2305.12870,44,jiang2023lion,"Adversarial Distillation,Large Language Model(LLM)"
2021,arXiv preprint arXiv:2110.06651,MDERank: A masked document embedding rank approach for unsupervised keyphrase extraction,"Linhan Zhang, Qian Chen, Wen Wang, Chong Deng, Shiliang Zhang, Bing Li, Wei Wang, Xin Cao",Linhan Zhang,English,"Keyphrase extraction (KPE) automatically extracts phrases in a document that provide a concise summary of the core content, which benefits downstream information retrieval and NLP tasks. Previous state-of-the-art (SOTA) methods select candidate keyphrases based on the similarity between learned representations of the candidates and the document. They suffer performance degradation on long documents due to discrepancy between sequence lengths which causes mismatch between representations of keyphrase candidates and the document. In this work, we propose a novel unsupervised embedding-based KPE approach, Masked Document Embedding Rank (MDERank), to address this problem by leveraging a mask strategy and ranking candidates by the similarity between embeddings of the source document and the masked document. We further develop a KPE-oriented BERT (KPEBERT) model by proposing a novel self-supervised contrastive learning method, which is more compatible to MDERank than vanilla BERT. Comprehensive evaluations on six KPE benchmarks demonstrate that the proposed MDERank outperforms state-of-the-art unsupervised KPE approach by average 1.80 $F1@15$ improvement. MDERank further benefits from KPEBERT and overall achieves average 3.53 $F1@15$ improvement over the SOTA SIFRank. Our code is available at \url{https://github.com/LinhanZ/mderank}.",Article,https://arxiv.org/abs/2110.06651,42,zhang2021mderank,"Key phrase extraction(KPE), embedding-based, BERT"
2020,AAAI,HAMNER: Headword Amplified Multi-span Distantly Supervised Method for Domain Specific Named Entity Recognition,"Shifeng Liu, Yifang Sun, Bing Li, Wei Wang, Xiang Zhao",Shifeng Liu,English,"To tackle Named Entity Recognition (NER) tasks, supervised methods need to obtain sufficient cleanly annotated data, which is labor and time consuming. On the contrary, distantly supervised methods acquire automatically annotated data using dictionaries to alleviate this requirement. Unfortunately, dictionaries hinder the effectiveness of distantly supervised methods for NER due to its limited coverage, especially in specific domains. In this paper, we aim at the limitations of the dictionary usage and mention boundary detection. We generalize the distant supervision by extending the dictionary with headword based non-exact matching. We apply a function to better weight the matched entity mentions. We propose a span-level model, which classifies all the possible spans then infers the selected spans with a proposed dynamic programming algorithm. Experiments on all three benchmark datasets demonstrate that our method outperforms previous state-of-the-art distantly supervised methods.",Conference paper,https://ojs.aaai.org/index.php/AAAI/article/view/6358,32,liu2020hamner,Named Entity Recognition(NER)
2021,Proceedings of the AAAI Conference on Artificial Intelligence,Improving the Efficiency and Effectiveness for BERT-based Entity Resolution,"Bing Li, Yukai Miao, Yaoshu Wang, Yifang Sun, Wei Wang",Bing Li,English,"BERT has set a new state-of-the-art performance on entity resolution (ER) task, largely owed to fine-tuning pre-trained language models and the deep pair-wise interaction. Albeit being remarkably effective, it comes with a steep increase in computational cost, as the deep-interaction requires to exhaustively compute every tuple pair to search for co-references. For ER task, it is often prohibitively expensive due to the large cardinality to be matched. To tackle this, we introduce a siamese network structure that independently encodes tuples using BERT but delays the pair-wise interaction via an enhanced alignment network. This siamese structure enables a dedicated blocking module to quickly filter out obviously dissimilar tuple pairs, and thus drastically reduces the cardinality of fine-grained matching. Further, the blocking and entity matching are integrated into a multi-task learning framework for facilitating both tasks. Extensive experiments on multiple datasets demonstrate that our model significantly outperforms state-of-the-art models (including BERT) in both efficiency and effectiveness.",Article,https://ojs.aaai.org/index.php/AAAI/article/view/17562,30,li2021improving,"BERT, Entity Resolution"
2020,SIGMOD,Monotonic Cardinality Estimation of Similarity Selection: A Deep Learning Approach,"Yaoshu Wang, Chuan Xiao, Jianbin Qin, Xin Cao, Yifang Sun, Wei Wang, Makoto Onizuka",Yaoshu Wang,English,"In this paper, we investigate the possibilities of utilizing deep learning for cardinality estimation of similarity selection. Answering this problem accurately and efficiently is essential to many data management applications, especially for query optimization. Moreover, in some applications the estimated cardinality is supposed to be consistent and interpretable. Hence a monotonic estimation w.r.t. the query threshold is preferred. We propose a novel and generic method that can be applied to any data type and distance function. Our method consists of a feature extraction model and a regression model. The feature extraction model transforms original data and threshold to a Hamming space, in which a deep learning-based regression model is utilized to exploit the incremental property of cardinality w.r.t. the threshold for both accuracy and monotonicity. We develop a training strategy tailored to our model as well as?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3318464.3380570,27,wang2020monotonic,"cardinality estimation,similarity selection,machine learning for data management"
2022,Applied Energy 326,A home energy management system incorporating data-driven uncertainty-aware user preference,"Yinyan Liu, Jin Ma, Xinjie Xing, Xinglu Liu, Wei Wang",Yinyan Liu,English,"Today, with the increase in the integration of renewable sources, the home energy management system (HEMS) has become a promising approach to improve grid energy efficiency and relieve network stress. Traditionally, complicated thermal models or passive participation of the users prevents HEMS from fully automating the involvement of demand-side energy management. In this paper, an advanced HEMS is proposed incorporating uncertainty-aware user preference. The energy consumption user behavior, including temporal and temperature habits, is firstly characterized in a data-driven way with non-intrusive load monitoring (NILM). To capture the potential uncertainties resulting from the characteristics of NILM modeling, a novel NILM model is developed with Bayesian theory. The NILM-based preference level is further integrated into the HEMS to schedule the appliances and respond the demand?…",Article,https://www.sciencedirect.com/science/article/pii/S0306261922011709,23,liu2022home,"Non-intrusive load monitoring,Bayesian neural network,Uncertainty Aware,user preference,home energy management system"
2020,ICDE,I/O Efficient Approximate Nearest Neighbour Search based on Learned Functions,"Mingjie Li, Ying Zhang, Yifang Sun, Wei Wang, Ivor W. Tsang, Xuemin Lin",Mingjie Li,English,"Approximate nearest neighbour search (ANNS) in high dimensional space is a fundamental problem in many applications, such as multimedia database, computer vision and information retrieval. Among many solutions, data-sensitive hashing-based methods are effective to this problem, yet few of them are designed for external storage scenarios and hence do not optimized for I/O efficiency during the query processing. In this paper, we introduce a novel data-sensitive indexing and query processing framework for ANNS with an emphasis on optimizing the I/O efficiency, especially, the sequential I/Os. The proposed index consists of several lists of point IDs, ordered by values that are obtained by learned hashing (i.e., mapping) functions on each corresponding data point. The functions are learned from the data and approximately preserve the order in the high-dimensional space. We consider two instantiations of?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9101839/,23,li2020efficient,Approximate Nearest Neighbour Search(ANNS)
2020,AAAI,Fine-Grained Named Entity Typing over Distantly Supervised Data Based on Refined Representation,"Muhammad Asif Ali, Yifang Sun, Bing Li, Wei Wang",Muhammad Asif Ali,English,"Fine-Grained Named Entity Typing (FG-NET) is a key component in Natural Language Processing (NLP). It aims at classifying an entity mention into a wide range of entity types. Due to a large number of entity types, distant supervision is used to collect training data for this task, which noisily assigns type labels to entity mentions irrespective of the context. In order to alleviate the noisy labels, existing approaches on FG-NET analyze the entity mentions entirely independent of each other and assign type labels solely based on mention's sentence-specific context. This is inadequate for highly overlapping and/or noisy type labels as it hinders information passing across sentence boundaries. For this, we propose an edge-weighted attentive graph convolution network that refines the noisy mention representations by attending over corpus-level contextual clues prior to the end classification. Experimental evaluation shows that the proposed model outperforms the existing research by a relative score of upto 10.2% and 8.3% for macro-f1 and micro-f1 respectively.",Conference paper,https://ojs.aaai.org/index.php/AAAI/article/view/6234,22,ali2020fine,None
2022,The VLDB Journal,On entity alignment at scale,"Weixin Zeng, Xiang Zhao, Xinyi Li, Jiuyang Tang, Wei Wang",Weixin Zeng,English,"Knowledge graph (KG), as an effective approach of organizing and storing data, has received growing attention over the last decade. A KG can hardly reach completeness since there are always a large amount of new data emerging. To increase the scale and coverage of KGs, a possible solution is to incorporate data from other KGs, and entity alignment (EA) plays a vital role during this process. EA is the task of detecting the entities that refer to the same real-world object but come from different KGs. Although a pile of approaches have been put forward to tackle this task, they are mostly evaluated on datasets in small size and cannot deal with large-scale data in practice. In this work, we study the task of EA at scale and put forward a novel solution that can manage large-scale KG pairs and meanwhile achieve promising alignment performance. First, we devise seed-oriented graph partition strategies to divide large?…",Article,https://link.springer.com/article/10.1007/s00778-021-00703-3,21,zeng2022entity,"Entity alignment,Scalable entity alignment,Reciprocal inference"
2022,Proceedings of the 2022 International Conference on Management of Data,Neural subgraph counting with wasserstein estimator,"Hanchen Wang, Rong Hu, Ying Zhang, Lu Qin, Wei Wang, Wenjie Zhang",Hanchen Wang,English,"Subgraph counting is a fundamental graph analysis task which has been widely used in many applications. As the problem of subgraph counting is NP-complete and hence intractable, approximate solutions have been widely studied, which fail to work with large and complex query graphs. Alternatively, Machine Learning techniques have been recently applied for this problem, yet the existing ML approaches either only support very small data graphs or cannot make full use of the data graph information, which inherently limits their scalability, estimation accuracies and robustness. In this paper, we propose a novel approximate subgraph counting algorithm, NeurSC, that can exploit and combine information from both the query graphs and the data graphs effectively and efficiently. It consists of two components: (1) an extraction module that adaptively generates simple yet representative substructures from data?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3514221.3526163,21,wang2022neural,"Subgraph Counting, Graph Neural Network(GNN)"
2021,Proceedings of the 44th International ACM SIGIR Conference on Research and?…,DAIR: A Query-Efficient Decision-based Attack on Image Retrieval Systems,"Mingyang Chen, Junda Lu, Yi Wang, Jianbin Qin, Wei Wang",Mingyang Chen,English,"There is an increasing interest in studying adversarial attacks on image retrieval systems. However, most of the existing attack methods are based on the white-box setting, where the attackers have access to all the model and database details, which is a strong assumption for practical attacks. The generic transfer-based attack also requires substantial resources yet the effect was shown to be unreliable. In this paper, we make the first attempt in proposing a query-efficient decision-based attack framework for the image retrieval (DAIR) to completely subvert the top-K retrieval results with human imperceptible perturbations. We propose an optimization-based method with a smoothed utility function to overcome the challenging discrete nature of the problem. To further improve the query efficiency, we propose a novel sampling method that can achieve the transferability between the surrogate and the target model?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3404835.3462887,21,chen2021dair,"Content-based Image Retrieval, Adversarial Attack in Deep Learning, Decision-based Attack in Deep Learning"
2021,Proceedings of the 2021 international conference on multimedia retrieval?…,A smart adversarial attack on deep hashing based image retrieval,"Junda Lu, Mingyang Chen, Yifang Sun, Wei Wang, Yi Wang, Xiaochun Yang",Junda Lu,English,"Deep hashing based retrieval models have been widely used in large-scale image retrieval systems. Recently, there has been a surging interest in studying the adversarial attack problem in deep hashing based retrieval models. However, the effectiveness of existing adversarial attacks is limited by their poor perturbation management, unawareness of ranking weight, and only laser-focusing on the attack image. These shortages lead to high perturbation costs yet low AP reductions. To overcome these shortages, we propose a novel adversarial attack framework to improve the effectiveness of adversarial attacks. Our attack designs a dimension-wise surrogate Hamming distance function to help with wiser perturbation management. Further, in generating adversarial examples, instead of focusing on a single image, we propose to collectively incorporate relevant images combined with an AP-oriented (average?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3460426.3463640,20,lu2021smart,"image retrieval, adversarial attack, deep hashing"
2020,EDBT,A Learning Based Approach to Predict Shortest-Path Distances,"Jianzhong Qi, Wei Wang, Rui Zhang, Zhuowei Zhao",Jianzhong Qi,English,,Conference paper,https://scholar.google.com/scholar?cluster=7445404443239290711&hl=en&oi=scholarr,19,qi2020learning,None
2022,2022 IEEE 38th International Conference on Data Engineering (ICDE),Reinforcement learning based query vertex ordering model for subgraph matching,"Hanchen Wang, Ying Zhang, Lu Qin, Wei Wang, Wenjie Zhang, Xuemin Lin",Hanchen Wang,English,"Subgraph matching is a fundamental problem in various fields that use graph structured data. Subgraph matching algorithms enumerate all isomorphic embeddings of a query graph    in a data graph G. An important branch of matching algorithms exploit the backtracking search approach which recursively extends intermediate results following a matching order of query vertices. It has been shown that the matching order plays a critical role in time efficiency of these backtracking based subgraph matching algorithms. In recent years, many advanced techniques for query vertex ordering (i.e., matching order generation) have been proposed to reduce the unpromising intermediate results according to the preset heuristic rules. In this paper, for the first time we apply the Reinforcement Learning (RL) and Graph Neural Networks (GNNs) techniques to generate the high-quality matching order for subgraph matching?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9835186/,17,wang2022reinforcement,"Subgraph Matching, Reinforcement Learning, Graph Neural Network(GNN)"
2021,Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data?…,High-Dimensional Similarity Query Processing for Data Science,"Jianbin Qin, Wei Wang, Chuan Xiao, Ying Zhang, Yaoshu Wang",Jianbin Qin,English,"Similarity query (a.k.a. nearest neighbor query) processing has been an active research topic for several decades. It is an essential procedure in a wide range of applications (e.g., classification & regression, deduplication, image retrieval, and recommender systems). Recently, representation learning and auto-encoding methods as well as pre-trained models have gained popularity. They basically deal with dense high-dimensional data, and this trend brings new opportunities and challenges to similarity query processing. Meanwhile, new techniques have emerged to tackle this long-standing problem theoretically and empirically.   This tutorial aims to provide a comprehensive review of high-dimensional similarity query processing for data science. It introduces solutions from a variety of research communities, including data mining (DM), database (DB), machine learning (ML), computer vision (CV), natural?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3447548.3470811,16,qin2021high,Similarity query
2020,Proceedings of the VLDB Endowment,Similarity query processing for high-dimensional data,"Jianbin Qin, Wei Wang, Chuan Xiao, Ying Zhang",Jianbin Qin,English,"Similarity query processing has been an active research topic for several decades. It is an essential procedure in a wide range of applications. Recently, embedding and auto-encoding methods as well as pre-trained models have gained popularity. They basically deal with high-dimensional data, and this trend brings new opportunities and challenges to similarity query processing for high-dimensional data. Meanwhile, new techniques have emerged to tackle this long-standing problem theoretically and empirically. In this tutorial, we summarize existing solutions, especially recent advancements from both database (DB) and machine learning (ML) communities, and analyze their strengths and weaknesses. We review exact and approximate methods such as cover tree, locality sensitive hashing, product quantization, and proximity graphs. We also discuss the selectivity estimation problem and show how researchers are bringing in state-of-the-art ML techniques to address the problem. By highlighting the strong connections between DB and ML, we hope that this tutorial provides an impetus towards new ML for DB solutions and vice versa.",Article,https://opus.lib.uts.edu.au/bitstream/10453/149587/2/p3437-qin.pdf,16,qin2020similarity,"Similarity Query Processing,  High-Dimensional Data"
2021,Proceedings of the 2021 International Conference on Management of Data,Consistent and flexible selectivity estimation for high-dimensional data,"Yaoshu Wang, Chuan Xiao, Jianbin Qin, Rui Mao, Makoto Onizuka, Wei Wang, Rui Zhang, Yoshiharu Ishikawa",Yaoshu Wang,English,"Selectivity estimation aims at estimating the number of database objects that satisfy a selection criterion. Answering this problem accurately and efficiently is essential to many applications, such as density estimation, outlier detection, query optimization, and data integration. The estimation problem is especially challenging for large-scale high-dimensional data due to the curse of dimensionality, the large variance of selectivity across different queries, and the need to make the estimator consistent (i.e., the selectivity is non-decreasing in the threshold). We propose a new deep learning-based model that learns a query-dependent piecewise linear function as selectivity estimator, which is flexible to fit the selectivity curve of any distance function and query object, while guaranteeing that the output is non-decreasing in the threshold. To improve the accuracy for large datasets, we propose to partition the dataset into?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3448016.3452772,15,wang2021consistent,"selectivity estimation,high-dimensional data,piecewise linear function, deep neural network"
2021,The VLDB Journal 30,EI-LSH: An early-termination driven I/O efficient incremental c-approximate nearest neighbor search,"Wanqi Liu, Hanchen Wang, Ying Zhang, Wei Wang, Lu Qin, Xuemin Lin",Wanqi Liu,English," Nearest neighbor in high-dimensional space has been widely used in various fields such as databases, data mining and machine learning. The problem has been well solved in low-dimensional space. However, when it comes to high-dimensional space, due to the curse of dimensionality, the problem is challenging. As a trade-off between accuracy and efficiency, c-approximate nearest neighbor (c-ANN) is considered instead of an exact NN search in high-dimensional space. A variety of c-ANN algorithms have been proposed, one of the important schemes for the c-ANN problem is called Locality-sensitive hashing (LSH), which projects a high-dimensional dataset into a low-dimensional dataset and can return a c-ANN with a constant probability. In this paper, we propose a new aggressive early-termination (ET) condition which stops the algorithm with LSH scheme earlier under the same theoretical?…",Article,https://link.springer.com/article/10.1007/s00778-020-00635-4,15,liu2021ei,"Locality-sensitive hashing,Approximate nearest neighbor search,Similarity search ,I/O efficient algorithm"
2021,Advances in Neural Information Processing Systems 34,Ks-gnn: Keywords search over incomplete graphs via graphs neural network,"Yu Hao, Xin Cao, Yufan Sheng, Yixiang Fang, Wei Wang",Yu Hao,English,"Keyword search is a fundamental task to retrieve information that is the most relevant to the query keywords. Keyword search over graphs aims to find subtrees or subgraphs containing all query keywords ranked according to some criteria. Existing studies all assume that the graphs have complete information. However, real-world graphs may contain some missing information (such as edges or keywords), thus making the problem much more challenging. To solve the problem of keyword search over incomplete graphs, we propose a novel model named KS-GNN based on the graph neural network and the auto-encoder. By considering the latent relationships and the frequency of different keywords, the proposed KS-GNN aims to alleviate the effect of missing information and is able to learn low-dimensional representative node embeddings that preserve both graph structure and keyword features. Our model can effectively answer keyword search queries with linear time complexity over incomplete graphs. The experiments on four real-world datasets show that our model consistently achieves better performance than state-of-the-art baseline methods in graphs having missing information.",Article,https://proceedings.neurips.cc/paper/2021/hash/0d7363894acdee742caf7fe4e97c4d49-Abstract.html,14,hao2021ks,"Keyword search,Graph Neural Network(GNN)"
2021,Proceedings of the AAAI Conference on Artificial Intelligence,Adversarial Defence by Diversified Simultaneous Training of Deep Ensembles,"Bo Huang, Zhiwei Ke, Yi Wang, Wei Wang, Linlin Shen, Feng Liu",Bo Huang,English,"Learning-based classifiers are susceptible to adversarial examples. Existing defence methods are mostly devised on individual classifiers. Recent studies showed that it is viable to increase adversarial robustness by promoting diversity over an ensemble of models. In this paper, we propose adversarial defence by encouraging ensemble diversity on learning high-level feature representations and gradient dispersion in simultaneous training of deep ensemble networks. We perform extensive evaluations under white-box and black-box attacks including transferred examples and adaptive attacks. Our approach achieves a significant gain of up to 52% in adversarial robustness, compared with the baseline and the state-of-the-art method on image benchmarks with complex data scenes. The proposed approach complements the defence paradigm of adversarial training, and can further boost the performance. The source code is available at https://github. com/ALIS-Lab/AAAI2021-PDD.",Article,https://ojs.aaai.org/index.php/AAAI/article/view/16955,14,huang2021adversarial,None
2022,Proceedings of the ACM SIGCOMM 2022 Conference,Software-defined network assimilation: bridging the last mile towards centralized network configuration management with NAssim,"Huangxun Chen, Yukai Miao, Li Chen, Haifeng Sun, Hong Xu, Libin Liu, Gong Zhang, Wei Wang",Huangxun Chen,English,"On-boarding new devices into an existing SDN network is a pain for network operations (NetOps) teams, because much expert effort is required to bridge the gap between the configuration models of the new devices and the unified data model in the SDN controller. In this work, we present an assistant framework NAssim, to help NetOps accelerate the process of assimilating a new device into a SDN network. Our solution features a unified parser framework to parse diverse device user manuals into preliminary configuration models, a rigorous validator that confirm the correctness of the models via formal syntax analysis, model hierarchy validation and empirical data validation, and a deep-learning-based mapping algorithm that uses state-of-the-art neural language processing techniques to produce human-comprehensible recommended mapping between the validated configuration model and the one in the SDN?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3544216.3544244,13,chen2022software,"Multi-Vendor Networks, Network ConfigurationManagement, Software-Defined Networks"
2023,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?…,Boosting Accuracy and Robustness of Student Models via Adaptive Adversarial Distillation,"Bo Huang, Mingyang Chen, Yi Wang, Junda Lu, Minhao Cheng, Wei Wang",Bo Huang,English,"Distilled student models in teacher-student architectures are widely considered for computational-effective deployment in real-time applications and edge devices. However, there is a higher risk of student models to encounter adversarial attacks at the edge. Popular enhancing schemes such as adversarial training have limited performance on compressed networks. Thus, recent studies concern about adversarial distillation (AD) that aims to inherit not only prediction accuracy but also adversarial robustness of a robust teacher model under the paradigm of robust optimization. In the min-max framework of AD, existing AD methods generally use fixed supervision information from the teacher model to guide the inner optimization for knowledge distillation which often leads to an overcorrection towards model smoothness. In this paper, we propose an adaptive adversarial distillation (AdaAD) that involves the teacher model in the knowledge optimization process in a way interacting with the student model to adaptively search for the inner results. Comparing with state-of-the-art methods, the proposed AdaAD can significantly boost both the prediction accuracy and adversarial robustness of student models in most scenarios. In particular, the ResNet-18 model trained by AdaAD achieves top-rank performance (54.23% robust accuracy) on RobustBench under AutoAttack.",Conference paper,http://openaccess.thecvf.com/content/CVPR2023/html/Huang_Boosting_Accuracy_and_Robustness_of_Student_Models_via_Adaptive_Adversarial_CVPR_2023_paper.html,11,huang2023boosting,None
2021,arXiv preprint arXiv:2109.02254,Sent2Span: span detection for PICO extraction in the biomedical text without span annotations,"Shifeng Liu, Yifang Sun, Bing Li, Wei Wang, Florence T Bourgeois, Adam G Dunn",Shifeng Liu,English,"The rapid growth in published clinical trials makes it difficult to maintain up-to-date systematic reviews, which requires finding all relevant trials. This leads to policy and practice decisions based on out-of-date, incomplete, and biased subsets of available clinical evidence. Extracting and then normalising Population, Intervention, Comparator, and Outcome (PICO) information from clinical trial articles may be an effective way to automatically assign trials to systematic reviews and avoid searching and screening - the two most time-consuming systematic review processes. We propose and test a novel approach to PICO span detection. The major difference between our proposed method and previous approaches comes from detecting spans without needing annotated span data and using only crowdsourced sentence-level annotations. Experiments on two datasets show that PICO span detection results achieve much higher results for recall when compared to fully supervised methods with PICO sentence detection at least as good as human annotations. By removing the reliance on expert annotations for span detection, this work could be used in human-machine pipeline for turning low-quality crowdsourced, and sentence-level PICO annotations into structured information that can be used to quickly assign trials to relevant systematic reviews.",Article,https://arxiv.org/abs/2109.02254,11,liu2021sent2span,PICO detection
2020,The VLDB Journal 29,Efficient query autocompletion with edit distance-based error tolerance,"Jianbin Qin, Chuan Xiao, Sheng Hu, Jie Zhang, Wei Wang, Yoshiharu Ishikawa, Koji Tsuda, Kunihiko Sadakane",Jianbin Qin,English," Query autocompletion is an important feature saving users many keystrokes from typing the entire query. In this paper, we study the problem of query autocompletion that tolerates errors in users’ input using edit distance constraints. Previous approaches index data strings in a trie, and continuously maintain all the prefixes of data strings whose edit distances from the query string are within the given threshold. The major inherent drawback of these approaches is that the number of such prefixes is huge for the first few characters of the query string and is exponential in the alphabet size. This results in slow query response even if the entire query approximately matches only few prefixes. We propose a novel neighborhood generation-based method to process error-tolerant query autocompletion. Our proposed method only maintains a small set of active nodes, thus saving both space and time to process the?…",Article,https://link.springer.com/article/10.1007/s00778-019-00595-4,11,qin2020efficient,"Query autocompletion ,Similarity Search, Database, Neighbourhood generation tree"
2022,arXiv preprint arXiv:2211.13873,Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition,"Yuxin Jiang, Linhan Zhang, Wei Wang",Yuxin Jiang,English,"Due to the absence of explicit connectives, implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis. The critical step for IDRR is to learn high-quality discourse relation representations between two arguments. Recent methods tend to integrate the whole hierarchical information of senses into discourse relation representations for multi-level sense recognition. Nevertheless, they insufficiently incorporate the static hierarchical structure containing all senses (defined as global hierarchy), and ignore the hierarchical sense label sequence corresponding to each instance (defined as local hierarchy). For the purpose of sufficiently exploiting global and local hierarchies of senses to learn better discourse relation representations, we propose a novel GlObal and Local Hierarchy-aware Contrastive Framework (GOLF), to model two kinds of hierarchies with the aid of multi-task learning and contrastive learning. Experimental results on PDTB 2.0 and PDTB 3.0 datasets demonstrate that our method remarkably outperforms current state-of-the-art models at all hierarchical levels. Our code is publicly available at https://github.com/YJiangcm/GOLF_for_IDRR",Article,https://arxiv.org/abs/2211.13873,10,jiang2022global,None
2023,IEEE Transactions on Pattern Analysis and Machine Intelligence,DifFormer: Multi-Resolutional Differencing Transformer With Dynamic Ranging for Time Series Analysis,"Bing Li, Wei Cui, Le Zhang, Ce Zhu, Wei Wang, Ivor Tsang, Joey Tianyi Zhou",Bing Li,English,"Time series analysis is essential to many far-reaching applications of data science and statistics including economic and financial forecasting, surveillance, and automated business processing. Though being greatly successful of Transformer in computer vision and natural language processing, the potential of employing it as the general backbone in analyzing the ubiquitous times series data has not been fully released yet. Prior Transformer variants on time series highly rely on task-dependent designs and pre-assumed “pattern biases”, revealing its insufficiency in representing nuanced seasonal, cyclic, and outlier patterns which are highly prevalent in time series. As a consequence, they can not generalize well to different time series analysis tasks. To tackle the challenges, we propose  DifFormer , an effective and efficient Transformer architecture that can serve as a workhorse for a variety of time-series analysis?…",Article,https://ieeexplore.ieee.org/abstract/document/10177239/,8,li2023difformer,"Transformer, deep time series analysis, deep forecasting"
2023,Nanophotonics,A deep neural network for general scattering matrix,"Yongxin Jing, Hongchen Chu, Bo Huang, Jie Luo, Wei Wang, Yun Lai",Yongxin Jing,English," The scattering matrix is the mathematical representation of the scattering characteristics of any scatterer. Nevertheless, except for scatterers with high symmetry like spheres or cylinders, the scattering matrix does not have any analytical forms and thus can only be calculated numerically, which requires heavy computation. Here, we have developed a well-trained deep neural network (DNN) that can calculate the scattering matrix of scatterers without symmetry at a speed thousands of times faster than that of finite element solvers. Interestingly, the scattering matrix obtained from the DNN inherently satisfies the fundamental physical principles, including energy conservation, time reversal and reciprocity. Moreover, inverse design based on the DNN is made possible by applying the gradient descent algorithm. Finally, we demonstrate an application of the DNN, which is to design scatterers with desired scattering?…",Article,https://www.degruyter.com/document/doi/10.1515/nanoph-2022-0770/html,8,jing2023deep,"deep neural network, inverse problem, scattering matrix"
2023,arXiv preprint arXiv:2304.12635,LMSFC: A Novel Multidimensional Index based on Learned Monotonic Space Filling Curves,"Jian Gao, Xin Cao, Xin Yao, Gong Zhang, Wei Wang",Jian Gao,English,"The recently proposed learned indexes have attracted much attention as they can adapt to the actual data and query distributions to attain better search efficiency. Based on this technique, several existing works build up indexes for multi-dimensional data and achieve improved query performance. A common paradigm of these works is to (i) map multi-dimensional data points to a one-dimensional space using a fixed space-filling curve (SFC) or its variant and (ii) then apply the learned indexing techniques. We notice that the first step typically uses a fixed SFC method, such as row-major order and z-order. It definitely limits the potential of learned multi-dimensional indexes to adapt variable data distributions via different query workloads. In this paper, we propose a novel idea of learning a space-filling curve that is carefully designed and actively optimized for efficient query processing. We also identify innovative offline and online optimization opportunities common to SFC-based learned indexes and offer optimal and/or heuristic solutions. Experimental results demonstrate that our proposed method, LMSFC, outperforms state-of-the-art non-learned or learned methods across three commonly used real-world datasets and diverse experimental settings.",Article,https://arxiv.org/abs/2304.12635,8,gao2023lmsfc,"space filling curves,query processing,index-based"
2022,IEEE Transactions on Knowledge and Data Engineering,Deep learning for approximate nearest neighbour search: A survey and future directions,"Mingjie Li, Yuan-Gen Wang, Peng Zhang, Hanpin Wang, Lisheng Fan, Enxia Li, Wei Wang",Mingjie Li,English,"Approximate nearest neighbour search (ANNS) in high-dimensional space is an essential and fundamental operation in many applications from many domains such as multimedia database, information retrieval and computer vision. With the rapidly growing volume of data and the dramatically increasing demands of users, traditional heuristic-based ANNS solutions have been facing great challenges in terms of both efficiency and accuracy. Inspired by the recent successes of deep learning in many fields, substantial efforts have been devoted to applying deep learning techniques to ANNS for learning to index and learning to search, resulting in numerous algorithms that achieve state-of-the-art performance compared with conventional methods. In this survey paper, we comprehensively review the different types of deep learning-based ANNS methods according to two learning paradigms:  learning to index  and?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9942356/,8,li2022deep,"Approximate nearest neighbour search, similarity search, high-dimensional space, learning to index, learning to search"
2021,arXiv preprint arXiv:2101.11212,Fine-Grained Named Entity Typing over Distantly Supervised Data via Refinement in Hyperbolic Space,"Muhammad Asif Ali, Yifang Sun, Bing Li, Wei Wang",Muhammad Asif Ali,English,None,Article,https://scholar.google.com/scholar?cluster=934781224416469013&hl=en&oi=scholarr,8,ali2021fine,"FG-NET, Hyperbolic Geometry, Distant Supervision, Graph Convolution"
2021,IEEE Transactions on Pattern Analysis and Machine Intelligence,GCP: Graph Encoder With Content-Planning for Sentence Generation From Knowledge Bases,"Bayu Distiawan Trisedya, Jianzhong Qi, Wei Wang, Rui Zhang",Bayu Distiawan Trisedya,English,"A knowledge base is a large repository of facts usually represented as triples, each consisting of a subject, a predicate, and an object. The triples together form a graph, i.e., a  knowledge graph . The triple representation in a knowledge graph offers a simple interface for applications to access the facts. However, this representation is not in a natural language form, which is difficult for humans to understand. We address this problem by proposing a system to translate a set of triples (i.e., a graph) into natural sentences. We take an encoder-decoder based approach. Specifically, we propose a  Graph encoder with C ontent- P lanning capability  ( GCP ) to encode an input graph. GCP not only works as an encoder but also serves as a content-planner by using an entity-order aware topological traversal to encode a graph. This way, GCP can capture the relationships between entities in a knowledge graph as well as providing?…",Article,https://ieeexplore.ieee.org/abstract/document/9565383/,7,trisedya2021gcp,"Natural language processing, triple-to-text generation, knowledge base"
2020,AAAI,A Recurrent Model for Collective Entity Linking with Adaptive Features,"Xiaoling Zhou, Yukai Miao, Wei Wang, Jianbin Qin",Xiaoling Zhou,English,"The vast amount of web data enables us to build knowledge bases with unprecedented quality and coverage. Named Entity Disambiguation (NED) is an important task that automatically resolves ambiguous mentions in free text to correct target entries in the knowledge base. Traditional machine learning based methods for NED were outperformed and made obsolete by the state-of-the-art deep learning based models. However, deep learning models are more complex, requiring large amount of training data and lengthy training and parameter tuning time. In this paper, we revisit traditional machine learning techniques and propose a light-weight, tuneable and time-efficient method without using deep learning or deep learning generated features. We propose novel adaptive features that focus on extracting discriminative features to better model similarities between candidate entities and the mention's context. We learn a local ranking model based on traditional and the new adaptive features based on the learning-to-rank framework. While arriving at linking decisions individually via the local model, our method also takes into consideration the correlation between decisions by running multiple recurrent global models, which can be deemed as a learned local search method. Our method attains performances comparable to the state-of-the-art deep learning-based methods on NED benchmark datasets while being significantly faster to train.",Conference paper,https://aaai.org/ojs/index.php/AAAI/article/view/5367,7,zhou2020recurrent,None
2021,Proceedings of the 30th ACM International Conference on Information?…,Relation prediction via graph neural network in heterogeneous information networks with missing type information,"Han Zhang, Yu Hao, Xin Cao, Yixiang Fang, Won-Yong Shin, Wei Wang",Han Zhang,English,"Relation prediction is a fundamental task in network analysis which aims to predict the relationship between two nodes. Thus, this differes from the traditional link prediction problem predicting whether a link exists between a pair of nodes, which can be viewed as a binary classification task. However, in the heterogeneous information network (HIN) which contains multiple types of nodes and multiple relations between nodes, the relation prediction task is more challenging. In addition, the HIN might have missing relation types on some edges and missing node types on some nodes, which makes the problem even harder. In this work, we propose RPGNN, a novel relation prediction model based on the graph neural network (GNN) and multi-task learning to solve this problem. Existing GNN models for HIN representation learning usually focus on the node classification/clustering task. They require the type information?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3459637.3482384,6,zhang2021relation,"Relation prediction, Heterogeneous information network, Graph neural network, Graph representation learning"
2020,arXiv preprint arXiv:2012.11083,A Note on Graph-Based Nearest Neighbor Search,"Hongya Wang, Zhizheng Wang, Wei Wang, Yingyuan Xiao, Zeng Zhao, Kaixiang Yang",Hongya Wang,English,"Nearest neighbor search has found numerous applications in machine learning, data mining and massive data processing systems. The past few years have witnessed the popularity of the graph-based nearest neighbor search paradigm because of its superiority over the space-partitioning algorithms. While a lot of empirical studies demonstrate the efficiency of graph-based algorithms, not much attention has been paid to a more fundamental question: why graph-based algorithms work so well in practice? And which data property affects the efficiency and how? In this paper, we try to answer these questions. Our insight is that ""the probability that the neighbors of a point o tends to be neighbors in the KNN graph"" is a crucial data property for query efficiency. For a given dataset, such a property can be qualitatively measured by clustering coefficient of the KNN graph. To show how clustering coefficient affects the performance, we identify that, instead of the global connectivity, the local connectivity around some given query q has more direct impact on recall. Specifically, we observed that high clustering coefficient makes most of the k nearest neighbors of q sit in a maximum strongly connected component (SCC) in the graph. From the algorithmic point of view, we show that the search procedure is actually composed of two phases - the one outside the maximum SCC and the other one in it, which is different from the widely accepted single or multiple paths search models. We proved that the commonly used graph-based search algorithm is guaranteed to traverse the maximum SCC once visiting any point in it. Our analysis reveals that high clustering?…",Article,https://arxiv.org/abs/2012.11083,6,wang2020note,None
2020,AAAI,Recursively Binary Modification Model for Nested Named Entity Recognition,"Bing Li, Shifeng Liu, Yifang Sun, Wei Wang, Xiang Zhao",Bing Li,English,"Recently, there has been an increasing interest in identifying named entities with nested structures. Existing models only make independent typing decisions on the entire entity span while ignoring strong modification relations between sub-entity types. In this paper, we present a novel Recursively Binary Modification model for nested named entity recognition. Our model utilizes the modification relations among sub-entities types to infer the head component on top of a Bayesian framework and uses entity head as a strong evidence to determine the type of the entity span. The process is recursive, allowing lower-level entities to help better model those on the outer-level. To the best of our knowledge, our work is the first effort that uses modification relation in nested NER task. Extensive experiments on four benchmark datasets demonstrate that our model outperforms state-of-the-art models in nested NER tasks, and delivers competitive results with state-of-the-art models in flat NER task, without relying on any extra annotations or NLP tools.",Conference paper,https://aaai.org/ojs/index.php/AAAI/article/view/6329,6,li2020recursively,"Named Entity Recognition(NER),nested entities"
2021,IEEE Transactions on Network Science and Engineering,A Single-to-Multi Network for Latency-Free Non-Intrusive Load Monitoring,"Yinyan Liu, Jing Qiu, Junda Lu, Wei Wang, Jin Ma",Yinyan Liu,English,"As one of the most important smart grid features, non-intrusive load monitoring (NILM) has become a practical technology for identifying the users’ energy consumption behavior. The conventional studies are usually based on the assumption that only one appliance is active or the signature database of all appliances is already known. Existing deep learning-based algorithms need to train a model for each target appliance. This paper, however, proposes an energy disaggregation network (EDNet) with deep encoder-decoder architecture to remove the unrealistic assumptions and reduce the size of the network to achieve latency-free NILM with only one model. Firstly, the blind source separation and mask mechanism used for speech recognition are creatively adopted for energy disaggregation. Then, the on/off states of each target appliance is detected based on the results of energy disaggregation. Finally, a?…",Article,https://ieeexplore.ieee.org/abstract/document/9645339/,5,liu2021single,"Non-intrusive Load Monitoring, Encoder-Decoder Neural Network, Energy Disaggregation, State Detection"
2020,ACM Transactions on the Web (TWEB),SMINT: Toward Interpretable and Robust Model Sharing for Deep Neural Networks,"Huijun Wu, Chen Wang, Richard Nock, Wei Wang, Jie Yin, Kai Lu, Liming Zhu",Huijun Wu,English,"Sharing a pre-trained machine learning model, particularly a deep neural network via prediction APIs, is becoming a common practice on machine learning as a service (MLaaS) platforms nowadays. Although deep neural networks (DNN) have shown remarkable successes in many tasks, they are also criticized for the lack of interpretability and transparency. Interpreting a shared DNN model faces two additional challenges compared with interpreting a general model. (1) Limited training data can be disclosed to users. (2) The internal structure of the models may not be available. These two challenges impede the application of most existing interpretability approaches, such as saliency maps or influence functions, for DNN models. Case-based reasoning methods have been used for interpreting decisions; however, how to select and organize the data points under the constraints of shared DNN models is not?…",Article,https://dl.acm.org/doi/abs/10.1145/3381833,5,wu2020smint,"Deep neural networks, model sharing, interpretability, decision boundary"
2023,IEEE Transactions on Industrial Informatics,Self-Supervised Feature Learning for Appliance Recognition in Non-Intrusive Load Monitoring,"Yinyan Liu, Lei Bai, Jin Ma, Wei Wang, Wanli Ouyang",Yinyan Liu,English,"Nonintrusive load monitoring (NILM) can monitor the operating state and energy consumption of electric appliances in a nonintrusive manner and provides a promising approach to improving electricity usage efficiency for residential and commercial buildings. Although machine learning (ML) methods are powerful and have significantly advanced the developments of NILM, they request a sizable amount of labeled data for model training. However, getting operational data of each electrical appliance in real life is challenging, so the requirements for labeled data limit the NLIM's practicality. To tackle this challenge, a novel multilayer momentum contrast (MLMoCo) learning mechanism is proposed for self-supervised feature representation learning. With only unlabeled aggregate load data, the proposed MLMoCo contrasts the augmented versions of the same sample (“positives”) with instances extracted from other?…",Article,https://ieeexplore.ieee.org/abstract/document/10138015/,3,liu2023self,"Feature learning, nonintrusive load monitoring (NILM), self-supervised learning, semisupervised learning"
2022,Computers & Chemical Engineering 163,Data predictive control of nonlinear process feature dynamics through latent variable behaviours,"Mengjie Zhao, Yitao Yan, Jie Bao, Wei Wang",Mengjie Zhao,English,"The paper presents a new big data-based approach to control of feature dynamics of continuous nonlinear chemical/industrial processes, based on the behavioural systems theory and deep learning tools. From time-series process data, the feature dynamics of a nonlinear process are extracted using an Autoencoder (AE), a type of artificial neural network. The feature dynamics are embedded in, and can be constructed from, a linear dynamic behaviour of latent variables. The latent variable dynamic space is described by a kernel representation and linearly maps the feature variable space. A Data Predictive Control (DPC) approach is developed to optimise the feature variables by controlling the latent variable dynamics using a system behaviour framework. Behaviour-based dissipativity conditions are adopted to deal with errors that arise in the latent and feature variable spaces during neural network training. A?…",Article,https://www.sciencedirect.com/science/article/pii/S0098135422001958,3,zhao2022data,"nonlinear processes, feature dynamics control, autoencoder, dissipativity theory, neural networks"
2020,IEEE Transactions on Knowledge and Data Engineering,Efficient regular expression matching based on positional inverted index,"Tao Qiu, Xiaochun Yang, Bin Wang, Wei Wang",Tao Qiu,English,"We study the efficient regular expression (regex) matching problem. Existing algorithms are the scanning-based algorithms which typically use an equivalent automaton compiled from the regex query to verify a document. Although some works propose various strategies to quickly jump to  candidate locations  in a document where a query result may appear, they still need to utilize the scanning-based method to verify these candidate locations. These methods become inefficient when there are still many candidate locations needed to be verified. In this article, we propose a novel approach to efficiently compute all matching positions for a regex query purely based on a positional   -gram inverted index. We propose a gram-driven NFA to represent the language of a regex and show all regex matching locations can be obtained by finding positions on   -grams of GNFA that satisfy certain positional constraints. Then?…",Article,https://ieeexplore.ieee.org/abstract/document/9093067/,3,qiu2020efficient,"Regular expression matching, positional inverted index, query plan"
2023,World Wide Web,Unbiased quasi-hyperbolic nesterov-gradient momentum-based optimizers for accelerating convergence,"Weiwei Cheng, Xiaochun Yang, Bin Wang, Wei Wang",Weiwei Cheng,English,"In the training process of deep learning models, one of the important steps is to choose an appropriate optimizer that directly determines the final performance of the model. Choosing the appropriate direction and step size (i.e. learning rate) of parameter update are decisive factors for optimizers. Previous gradient descent optimizers could be oscillated and fail to converge to a minimum point because they are only sensitive to the current gradient. Momentum-Based Optimizers (MBOs) have been widely adopted recently since they can relieve oscillation to accelerate convergence by using the exponentially decaying average of gradients to fine-tune the direction. However, we find that most of the existing MBOs are biased and inconsistent with the local fastest descent direction resulting in a high number of iterations. To accelerate convergence, we propose an Unbiased strategy to adjust the descent direction of a?…",Article,https://link.springer.com/article/10.1007/s11280-022-01086-3,1,cheng2023unbiased,"Optimizer, Momentum ,Accelerate convergence ,Unbiased"
2023,Proceedings of the ACM Web Conference 2023,Graph Self-supervised Learning with Augmentation-aware Contrastive Learning,"Dong Chen, Xiang Zhao, Wei Wang, Zhen Tan, Weidong Xiao",Dong Chen,English," Graph self-supervised learning aims to mine useful information from unlabeled graph data, and has been successfully applied to pre-train graph representations. Many existing approaches use contrastive learning to learn powerful embeddings by learning contrastively from two augmented graph views. However, none of these graph contrastive methods fully exploits the diversity of different augmentations, and hence is prone to overfitting and limited generalization ability of learned representations. In this paper, we propose a novel Graph Self-supervised Learning method with Augmentation-aware Contrastive Learning. Our method is based on the finding that the pre-trained model after adding augmentation diversity can achieve better generalization ability. To make full use of the information from the diverse augmentation method, this paper constructs new augmentation-aware prediction task which?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3543507.3583246,1,chen2023graph,"Graph Neural Networks,  Self-supervised Learning, Contrastive Learning"
2023,Proceedings of the VLDB Endowment,Expanding Reverse Nearest Neighbors,"Wentao Li, Maolin Cai, Min Gao, Dong Wen, Lu Qin, Wei Wang",Wentao Li,English,In a graph the reverse nearest neighbors RNN of vertex f refer to the set of vertices that consider f as their nearest neighbor When f represents a facility like a subway station its RNN comprises potential users who prefer the nearest facility In practice there may be underutilized facilities with small RNN sizes and relocating these facilities to expand their service can be costly or infeasible A more cost effective approach involves selectively upgrading some edges e g reducing their weights to expand the RNN sizes of underutilized facilities This motivates our research on the Expanding Reverse Nearest Neighbors ERNN problem which aims to maximize the RNN size of a target facility by upgrading a limited number of edges Solving the ERNN problem allows underutilized facilities to serve more users and alleviate the burden on other facilities Despite numerous potential applications ERNN is hard to solve It can be proven to be NP hard and APX hard and it exhibits non monotonic and non submodular properties To overcome these challenges we propose novel greedy algorithms that improve efficiency by minimizing the number of edges that need to be processed and the cost of processing each edge Experimental results demonstrate that the proposed algorithms achieve orders of magnitude speedup compared to the standard greedy algorithm while greatly expanding the RNN,Article,https://opus.lib.uts.edu.au/handle/10453/178393,0,liexpanding,None
2023,Proceedings of the 32nd ACM International Conference on Information and?…,CoSaR: Combating Label Noise Using Collaborative Sample Selection and Adversarial Regularization,"Xiaobo Zhang, Yutao Liu, Hao Wang, Wei Wang, Panpan Ni, Ji Zhang",Xiaobo Zhang,English,"Learning with noisy labels is nontrivial for deep learning models. Sample selection is a widely investigated research topic for handling noisy labels. However, most existing methods face challenges such as imprecise selection, a lack of global selection capabilities, and the need for tedious hyperparameter tuning. In this paper, we propose CoSaR (Collaborative Selection and adversarial Regularization ), a twin-networks based model that performs globally adaptive sample selection to tackle label noise. Specifically, the collaborative selection estimates the average distribution distances between predictions and generation labels through the collaboration of two networks to address the bias of the average distribution distances and the manual tuning of hyperparameters. Adversarial regularization is integrated into CoSaR to restrict the network's tendency to fit and memorize noisy labels, thereby enhancing its?…",Conference paper,https://dl.acm.org/doi/abs/10.1145/3583780.3614826,0,zhang2023cosar,"Label-noise learning, Collaborative selection, Adversarial regularization"
2023,International Conference on Advanced Data Mining and Applications,Cardinality Estimation of Subgraph Search Queries with Direction Learner,"Wenzhe Hou, Xiang Zhao, Wei Wang",Wenzhe Hou,English,"In recent years, graph data has been widely used in various fields, which has made graph data management a hot topic of research. Subgraph searching is a crucial operation for graph data management, but its NP-complete nature makes exact matching algorithms time-consuming. To address this problem, new methods have begun to employ deep learning techniques to produce approximate results in less time. However, the distinctiveness of representations of different type of nodes are still limited, and edge information are not well used. To address these limitations, a novel graph neural network architecture, Directional Embedding Network (), is proposed for cardinality estimation of subgraph search queries. It effectively captures node relationships to distinguish different nodes through direction discrepancy, and accurately represents edges with learnable structural information. Experimental results?…",Conference paper,https://link.springer.com/chapter/10.1007/978-3-031-46677-9_6,0,hou2023cardinality,"data management,subgraph matching ,cardinality estimation,ai4db"
2023,ICASSP 2023-2023 IEEE International Conference on Acoustics,Weighted Sampling for Masked Language Modeling,"Linhan Zhang, Qian Chen, Wen Wang, Chong Deng, Xin Cao, Kongzhang Hao, Yuxin Jiang, Wei Wang",Linhan Zhang,English,"Masked Language Modeling (MLM) is widely used to pretrain language models. The standard random masking strategy in MLM causes the pre-trained language models (PLMs) to be biased towards high-frequency tokens. Representation learning of rare tokens is poor and PLMs have limited performance on downstream tasks. To alleviate this frequency bias issue, we propose two simple and effective Weighted Sampling strategies for masking tokens based on token frequency and training loss. We apply these two strategies to BERT and obtain Weighted-Sampled BERT (WSBERT). Experiments on the Semantic Textual Similarity benchmark (STS) show that WSBERT significantly improves sentence embeddings over BERT. Combining WSBERT with calibration methods and prompt learning further improves sentence embeddings. We also investigate fine-tuning WSBERT on the GLUE benchmark and show that?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/10096946/,0,zhang2023weighted,"Weighted Sampling, Mask Language Model, Sentence Representation, GLUE Evaluation"
2021,arXiv preprint arXiv:2110.00696,Tao: A Learning Framework for Adaptive Nearest Neighbor Search using Static Features Only,"Kaixiang Yang, Hongya Wang, Bo Xu, Wei Wang, Yingyuan Xiao, Ming Du, Junfeng Zhou",Kaixiang Yang,English,"Approximate nearest neighbor (ANN) search is a fundamental problem in areas such as data management,information retrieval and machine learning. Recently, Li et al. proposed a learned approach named AdaptNN to support adaptive ANN query processing. In the middle of query execution, AdaptNN collects a number of runtime features and predicts termination condition for each individual query, by which better end-to-end latency is attained. Despite its efficiency, using runtime features complicates the learning process and leads to performance degradation. Radically different from AdaptNN, we argue that it is promising to predict termination condition before query exetution. Particularly, we developed Tao, a general learning framework for Terminating ANN queries Adaptively using Only static features. Upon the arrival of a query, Tao first maps the query to a local intrinsic dimension (LID) number, and then predicts the termination condition using LID instead of runtime features. By decoupling prediction procedure from query execution, Tao eliminates the laborious feature selection process involved in AdaptNN. Besides, two design principles are formulated to guide the application of Tao and improve the explainability of the prediction model. We integrate two state-of-the-art indexing approaches, i.e., IMI and HNSW, into Tao, and evaluate the performance over several million to billion-scale datasets. Experimental results show that, in addition to its simplicity and generality , Tao achieves up to 2.69x speedup even compared to its counterpart, at the same high accuracy targets.",Article,https://arxiv.org/abs/2110.00696,0,yang2021tao,"approximate nearest neighbor search, local intrinsic dimension, neural networks"
2021,arXiv preprint arXiv:2101.11212,FGNET-RH: Fine-Grained Named Entity Typing via Refinement in Hyperbolic Space,"Muhammad Asif Ali, Yifang Sun, Bing Li, Wei Wang",Muhammad Asif Ali,English,"Fine-Grained Named Entity Typing (FG-NET) aims at classifying the entity mentions into a wide range of entity types (usually hundreds) depending upon the context. While distant supervision is the most common way to acquire supervised training data, it brings in label noise, as it assigns type labels to the entity mentions irrespective of mentions context. In attempts to deal with the label noise, leading research on the FG-NET assumes that the fine-grained entity typing data possesses a euclidean nature, which restraints the ability of the existing models in combating the label noise. Given the fact that the fine-grained type hierarchy exhibits a hierarchical structure, it makes hyperbolic space a natural choice to model the FG-NET data. In this research, we propose FGNET-RH, a novel framework that benefits from the hyperbolic geometry in combination with the graph structures to perform entity typing in a performance-enhanced fashion. FGNET-RH initially uses LSTM networks to encode the mention in relation with its context, later it forms a graph to distill/refine the mention encodings in the hyperbolic space. Finally, the refined mention encoding is used for entity typing. Experimentation using different benchmark datasets shows that FGNET-RH improves the performance on FG-NET by up to 3.5-% in terms of strict accuracy.",Article,https://arxiv.org/abs/2101.11212,0,ali2021fgnet,"FG-NET, Hyperbolic Geometry, Label noise, Distant Supervision"
