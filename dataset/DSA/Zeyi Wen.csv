Year,Sources,Name,Authors,First Author,Chinese/English,Abstract,Venues,doi,Citation,Id,Keywords
2020,AAAI Conference on Artificial Intelligence (AAAI),Practical Federated Gradient Boosting Decision Trees,"Qinbin Li, Zeyi Wen, Bingsheng He",Qinbin Li,English,"Gradient Boosting Decision Trees (GBDTs) have become very successful in recent years, with many awards in machine learning and data mining competitions. There have been several recent studies on how to train GBDTs in the federated learning setting. In this paper, we focus on horizontal federated learning, where data samples with the same features are distributed among multiple parties. However, existing studies are not efficient or effective enough for practical use. They suffer either from the inefficiency due to the usage of costly data transformations such as secure sharing and homomorphic encryption, or from the low model accuracy due to differential privacy designs. In this paper, we study a practical federated environment with relaxed privacy constraints. In this environment, a dishonest party might obtain some information about the other parties' data, but it is still impossible for the dishonest party to derive the actual raw data of other parties. Specifically, each party boosts a number of trees by exploiting similarity information based on locality-sensitive hashing. We prove that our framework is secure without exposing the original record to other parties, while the computation overhead in the training process is kept low. Our experimental studies show that, compared with normal training with the local data of each party, our approach can significantly improve the predictive accuracy, and achieve comparable accuracy to the original GBDT with the data from all parties.",Conference paper,https://aaai.org/ojs/index.php/AAAI/article/view/5895,202,li2020practical,None
2020,AAAI Conference on Artificial Intelligence (AAAI),Privacy-Preserving Gradient Boosting Decision Trees,"Qinbin Li, Zhaomin Wu, Zeyi Wen, Bingsheng He",Qinbin Li,English,"The Gradient Boosting Decision Tree (GBDT) is a popular machine learning model for various tasks in recent years. In this paper, we study how to improve model accuracy of GBDT while preserving the strong guarantee of differential privacy. Sensitivity and privacy budget are two key design aspects for the effectiveness of differential private models. Existing solutions for GBDT with differential privacy suffer from the significant accuracy loss due to too loose sensitivity bounds and ineffective privacy budget allocations (especially across different trees in the GBDT model). Loose sensitivity bounds lead to more noise to obtain a fixed privacy level. Ineffective privacy budget allocations worsen the accuracy loss especially when the number of trees is large. Therefore, we propose a new GBDT training algorithm that achieves tighter sensitivity bounds and more effective noise allocations. Specifically, by investigating the property of gradient and the contribution of each tree in GBDTs, we propose to adaptively control the gradients of training data for each iteration and leaf node clipping in order to tighten the sensitivity bounds. Furthermore, we design a novel boosting framework to allocate the privacy budget between trees so that the accuracy loss can be further reduced. Our experiments show that our approach can achieve much better model accuracy than other baselines.",Conference paper,https://ojs.aaai.org/index.php/AAAI/article/view/5422,85,li2020privacy,None
2020,Journal of Machine Learning Research (JMLR),ThunderGBM: Fast GBDTs and Random Forests on GPUs,"Zeyi Wen, Hanfeng Liu, Jiashuai Shi, Bingsheng He, Qinbin Li, Jian Chen",Zeyi Wen,English,"Gradient Boosting Decision Trees (GBDTs) and Random Forests (RFs) have been used in many real-world applications. They are often a standard recipe for building state-of-the-art solutions to machine learning and data mining problems. However, training and prediction are very expensive computationally for large and high dimensional problems. This article presents an efficient and open source software toolkit called ThunderGBM which exploits the high-performance Graphics Processing Units (GPUs) for GBDTs and RFs. ThunderGBM supports classification, regression and ranking, and can run on single or multiple GPUs of a machine. Our experimental results show that ThunderGBM outperforms the existing libraries while producing similar models, and can handle high dimensional problems where existing GPU-based libraries fail. Documentation, examples, and more details about ThunderGBM are available at https://github.com/xtra-computing/thundergbm.",Article,https://www.jmlr.org/papers/v21/19-095.html,25,wen2020thundergbm,"Gradient Boosting Decision Trees, Random Forests, GPUs, Eciency"
2023,The 37th AAAI Conference on Artificial Intelligence (AAAI),Local Path Integration for Attribution,"Peiyu Yang, Naveed Akhtar, Zeyi Wen, Ajmal Mian",Peiyu Yang,English,"Path attribution methods are a popular tool to interpret a visual model's prediction on an input. They integrate model gradients for the input features over a path defined between the input and a reference, thereby satisfying certain desirable theoretical properties. However, their reliability hinges on the choice of the reference. Moreover, they do not exhibit weak dependence on the input, which leads to counter-intuitive feature attribution mapping. We show that path-based attribution can account for the weak dependence property by choosing the reference from the local distribution of the input. We devise a method to identify the local input distribution and propose a technique to stochastically integrate the model gradients over the paths defined by the references sampled from that distribution. Our local path integration (LPI) method is found to consistently outperform existing path attribution techniques when evaluated on deep visual models. Contributing to the ongoing search of reliable evaluation metrics for the interpretation methods, we also introduce DiffID metric that uses the relative difference between insertion and deletion games to alleviate the distribution shift problem faced by existing metrics. Our code is available at https://github. com/ypeiyu/LPI.",Conference paper,https://ojs.aaai.org/index.php/AAAI/article/view/25422,8,yang2023local,None
2022,IEEE International Parallel and Distributed Processing Symposium (IPDPS),POSTER:Fast Parallel Bayesian Network Structure Learning,"Jiantong Jiang, Zeyi Wen, Ajmal Mian",Jiantong Jiang,English,"Bayesian networks (BNs) are a widely used graphical model in machine learning for representing knowledge with uncertainty. The mainstream BN structure learning methods require performing a large number of conditional independence (CI) tests. The learning process is very time-consuming, especially for high-dimensional problems, which hinders the adoption of BNs to more applications. Existing works attempt to accelerate the learning process with parallelism, but face issues including load unbalancing, costly atomic operations and dominant parallel overhead. In this paper, we propose a fast solution named Fast-BNS on multi-core CPUs to enhance the efficiency of the BN structure learning. Fast-Bns is powered by a series of efficiency optimizations including (i) designing a dynamic work pool to monitor the processing of edges and to better schedule the workloads among threads, (ii) grouping the CI tests of?…",Conference paper,https://ieeexplore.ieee.org/abstract/document/9820657/,8,jiang2022fast,"Bayesian Networks, Inference, Junction Tree"
2021,IEEE Transactions on Parallel and Distributed Systems (TPDS),Parallel and Distributed Structured SVM Training,"Jiantong Jiang, Zeyi Wen, Zeke Wang, Bingsheng He, Jian Chen",Jiantong Jiang,English,"Structured Support Vector Machines (structured SVMs) are a fundamental machine learning algorithm, and have solid theoretical foundation and high effectiveness in applications such as natural language parsing and computer vision. However, training structured SVMs is very time-consuming, due to the large number of constraints and inferior convergence rates, especially for large training data sets. The high cost of training structured SVMs has hindered its adoption to new applications. In this article, we aim to improve the efficiency of structured SVMs by proposing a parallel and distributed solution (namely  FastSSVM ) for training structured SVMs building on top of MPI and OpenMP. FastSSVM exploits a series of optimizations (e.g., optimizations on data storage and synchronization) to efficiently use the resources of the nodes in a cluster and the cores of the nodes. Moreover, FastSSVM tackles the large?…",Article,https://ieeexplore.ieee.org/abstract/document/9502528/,7,jiang2021parallel,"Parallel and distributed training, structured machine learning, support vector machines"
2023,The International Conference on Learning Representations (ICLR),Re-calibrating Feature Attributions for Model Interpretation,"Peiyu Yang, Naveed Akhtar, Zeyi Wen, Mubarak Shah, Ajmal Mian",Peiyu Yang,English,"The ability to interpret machine learning models is critical for high-stakes applications. Due to its desirable theoretical properties, path integration is a widely used scheme for feature attribution to interpret model predictions. However, the methods implementing this scheme currently rely on absolute attribution scores to eventually provide sensible interpretations. This not only contradicts the premise that the features with larger attribution scores are more relevant to the model prediction, but also conflicts with the theoretical settings for which the desirable properties of the attributions are proven. We address this by devising a method to first compute an appropriate reference for the path integration scheme. This reference further helps in identifying valid interpolation points on a desired integration path. The reference is computed in a gradient ascending direction on the model's loss surface, while the interpolations are performed by analyzing the model gradients and variations between the reference and the input. The eventual integration is effectively performed along a non-linear path. Our scheme can be incorporated into the existing integral-based attribution methods. We also devise an effective sampling and integration procedure that enables employing our scheme with multi-reference path integration efficiently. We achieve a marked performance boost for a range of integral-based attribution methods on both local and global evaluation metrics by enhancing them with our scheme. Our extensive results also show improved sensitivity, sanity preservation and model robustness with the proposed re-calibration of the attribution techniques with our?…",Conference paper,https://openreview.net/forum?id=WUWJIV2Yxtp,4,yang2022re,None
2023,Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and?…,Fast Parallel Exact Inference on Bayesian Networks,"Jiantong Jiang, Zeyi Wen, Atif Mansoor, Ajmal Mian",Jiantong Jiang,English,"Bayesian networks (BNs) are attractive, because they are graphical and interpretable machine learning models. However, exact inference on BNs is time-consuming, especially for complex problems. To improve the efficiency, we propose a fast BN exact inference solution named Fast-BNI on multi-core CPUs. Fast-BNI enhances the efficiency of exact inference through hybrid parallelism that tightly integrates coarse- and fine-grained parallelism. We also propose techniques to further simplify the bottleneck operations of BN exact inference. Fast-BNI source code is freely available at https://github.com/jjiantong/FastBN.",Conference paper,https://dl.acm.org/doi/abs/10.1145/3572848.3577476,4,jiang2023fast,None
