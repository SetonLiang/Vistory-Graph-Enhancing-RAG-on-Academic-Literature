Year,Sources,Name,Authors,First Author,Chinese/English,Abstract,Venues,doi,Citation,Id,Keywords
2020,Proceedings of the IEEE/CVF conference on computer vision and pattern?¡­,Spsequencenet: Semantic segmentation network on 4d point clouds,"Hanyu Shi, Guosheng Lin, Hao Wang, Tzu-Yi Hung, Zhenhua Wang",Hanyu Shi,English,"Point clouds are useful in many applications like autonomous driving and robotics as they provide natural 3D information of the surrounding environments. While there are extensive research on 3D point clouds, scene understanding on 4D point clouds, a series of consecutive 3D point clouds frames, is an emerging topic and yet under-investigated. With 4D point clouds (3D point cloud videos), robotic systems could enhance their robustness by leveraging the temporal information from previous frames. However, the existing semantic segmentation methods on 4D point clouds suffer from low precision due to the spatial and temporal information loss in their network structures. In this paper, we propose SpSequenceNet to address this problem. The network is designed based on 3D sparse convolution. And we introduce two novel modules, a cross-frame global attention module and a cross-frame local interpolation module, to capture spatial and temporal information in 4D point clouds. We conduct extensive experiments on SemanticKITTI, and achieve the state-of-the-art result of 43.1% on mIoU, which is 1.5% higher than the previous best approach.",Conference paper,http://openaccess.thecvf.com/content_CVPR_2020/html/Shi_SpSequenceNet_Semantic_Segmentation_Network_on_4D_Point_Clouds_CVPR_2020_paper.html,102,shi2020spsequencenet,"4D point cloud,3D sparse convolution,state-of-the-art"
2021,Proceedings of the 29th ACM International Conference on Multimedia,Cycle-consistent inverse GAN for text-to-image synthesis,"Hao Wang, Guosheng Lin, Steven CH Hoi, Chunyan Miao",Hao Wang,English,"This paper investigates an open research task of text-to-image synthesis for automatically generating or manipulating images from text descriptions. Prevailing methods mainly take the textual descriptions as the conditional input for the GAN generation, and need to train different models for the text-guided image generation and manipulation tasks. In this paper, we propose a novel unified framework of Cycle-consistent Inverse GAN (CI-GAN) for both text-to-image generation and text-guided image manipulation tasks. Specifically, we first train a GAN model without text input, aiming to generate images with high diversity and quality. Then we learn a GAN inversion model to convert the images back to the GAN latent space and obtain the inverted latent codes for each image, where we introduce the cycle-consistency training to learn more robust and consistent inverted latent codes. We further uncover the semantics of?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3474085.3475226,48,wang2021cycle,"GAN, Text-to-image synthesis, Cycle consistency"
2021,IEEE Transactions on Multimedia 24,Cross-modal food retrieval: learning a joint embedding of food images and recipes with semantic consistency and attention mechanism,"Hao Wang, Doyen Sahoo, Chenghao Liu, Ke Shu, Palakorn Achananuparp, Ee-peng Lim, Steven CH Hoi",Hao Wang,English,"Food retrieval is an important task to perform analysis of food-related information, where we are interested in retrieving relevant information about the queried food item such as ingredients, cooking instructions, etc. In this paper, we investigate cross-modal retrieval between food images and cooking recipes. The goal is to learn an embedding of images and recipes in a common feature space, such that the corresponding image-recipe embeddings lie close to one another. Two major challenges in addressing this problem are 1) large intra-variance and small inter-variance across cross-modal food data; and 2) difficulties in obtaining discriminative recipe representations. To address these two problems, we propose Semantic-Consistent and Attention-based Networks (SCAN), which regularize the embeddings of the two modalities through aligning output semantic probabilities. Besides, we exploit a self-attention?¡­",Article,https://ieeexplore.ieee.org/abstract/document/9439970/,47,wang2021cross,"Deep learning, cross-modal retrieval, vision-and-language"
2020,Computer Vision¨CECCV 2020: 16th European Conference,Structure-aware generation network for recipe generation from images,"Hao Wang, Guosheng Lin, Steven CH Hoi, Chunyan Miao",Hao Wang,English," Sharing food has become very popular with the development of social media. For many real-world applications, people are keen to know the underlying recipes of a food item. In this paper, we are interested in automatically generating cooking instructions for food. We investigate an open research task of generating cooking instructions based on only food images and ingredients, which is similar to the image captioning task. However, compared with image captioning datasets, the target recipes are long-length paragraphs and do not have annotations on structure information. To address the above limitations, we propose a novel framework of Structure-aware Generation Network (SGN) to tackle the food recipe generation task. Our approach brings together several novel ideas in a systematic framework: (1) exploiting an unsupervised learning approach to obtain the sentence-level tree structure labels?¡­",Conference paper,https://link.springer.com/chapter/10.1007/978-3-030-58583-9_22,34,wang2020structure,"Structure Learning, Text Generation, Image-to-Text"
2022,IEEE Transactions on Pattern Analysis and Machine Intelligence,Learning structural representations for recipe generation and food retrieval,"Hao Wang, Guosheng Lin, Steven CH Hoi, Chunyan Miao",Hao Wang,English,"Food is significant to human daily life. In this paper, we are interested in learning structural representations for lengthy recipes, that can benefit the recipe generation and food cross-modal retrieval tasks. Different from the common vision-language data, here the food images contain mixed ingredients and target recipes are lengthy paragraphs, where we do not have annotations on structure information. To address the above limitations, we propose a novel method to unsupervisedly learn the sentence-level tree structures for the cooking recipes. Our approach brings together several novel ideas in a systematic framework: (1) exploiting an unsupervised learning approach to obtain the sentence-level tree structure labels before training; (2) generating trees of target recipes from images with the supervision of tree structure labels learned from (1); and (3) integrating the learned tree structures into the recipe generation?¡­",Article,https://ieeexplore.ieee.org/abstract/document/9793670/,21,wang2022learning,"Text generation, vision-and-language"
2023,arXiv preprint arXiv:2310.14985,Llm-based agent society investigation: Collaboration and confrontation in avalon gameplay,"Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao, Ee-Peng Lim, Hui Xiong, Hao Wang",Yihuai Lan,English,"This paper aims to investigate the open research problem of uncovering the social behaviors of LLM-based agents. To achieve this goal, we adopt Avalon, a representative communication game, as the environment and use system prompts to guide LLM agents to play the game. While previous studies have conducted preliminary investigations into gameplay with LLM agents, there lacks research on their social behaviors. In this paper, we present a novel framework designed to seamlessly adapt to Avalon gameplay. The core of our proposed framework is a multi-agent system that enables efficient communication and interaction among agents. We evaluate the performance of our framework based on metrics from two perspectives: winning the game and analyzing the social behaviors of LLM agents. Our results demonstrate the effectiveness of our framework in generating adaptive and intelligent agents and highlight the potential of LLM-based agents in addressing the challenges associated with dynamic social environment interaction. By analyzing the social behaviors of LLM agents from the aspects of both collaboration and confrontation, we provide insights into the research and applications of this domain.",Article,https://arxiv.org/abs/2310.14985,18,lan2023llm,"Large Language Models,multi-agent,collaboration,confrontation"
2023,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?¡­,TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision,"Jiacheng Wei*, Hao Wang*, Jiashi Feng, Guosheng Lin, Kim-Hui Yap",Jiacheng Wei*,English,"In this paper, we investigate an open research task of generating controllable 3D textured shapes from the given textual descriptions. Previous works either require ground truth caption labeling or extensive optimization time. To resolve these issues, we present a novel framework, TAPS3D, to train a text-guided 3D shape generator with pseudo captions. Specifically, based on rendered 2D images, we retrieve relevant words from the CLIP vocabulary and construct pseudo captions using templates. Our constructed captions provide high-level semantic supervision for generated 3D shapes. Further, in order to produce fine-grained textures and increase geometry diversity, we propose to adopt low-level image regularization to enable fake-rendered images to align with the real ones. During the inference phase, our proposed model can generate 3D textured shapes from the given text without any additional optimization. We conduct extensive experiments to analyze each of our proposed components and show the efficacy of our framework in generating high-fidelity 3D textured and text-relevant shapes.",Conference paper,http://openaccess.thecvf.com/content/CVPR2023/html/Wei_TAPS3D_Text-Guided_3D_Textured_Shape_Generation_From_Pseudo_Supervision_CVPR_2023_paper.html,17,wei2023taps3d,"Text-guided 3D textured shapes, semantic supervision"
2022,Proceedings of the 30th ACM International Conference on Multimedia,Paired cross-modal data augmentation for fine-grained image-to-text retrieval,"Hao Wang, Guosheng Lin, Steven Hoi, Chunyan Miao",Hao Wang,English,"This paper investigates an open research problem of generating text-image pairs to improve the training of fine-grained image-to-text cross-modal retrieval task, and proposes a novel framework for paired data augmentation by uncovering the hidden semantic information of StyleGAN2 model. Specifically, we first train a StyleGAN2 model on the given dataset. We then project the real images back to the latent space of StyleGAN2 to obtain the latent codes. To make the generated images manipulatable, we further introduce a latent space alignment module to learn the alignment between StyleGAN2 latent codes and the corresponding textual caption features. When we do online paired data augmentation, we first generate augmented text through random token replacement, then pass the augmented text into the latent space alignment module to output the latent codes, which are finally fed to StyleGAN2 to generate?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3503161.3547809,10,wang2022paired,Image-to-text retrieval
2022,IEEE Transactions on Image Processing 31,Cross-modal graph with meta concepts for video captioning,"Hao Wang, Guosheng Lin, Steven CH Hoi, Chunyan Miao",Hao Wang,English,"Video captioning targets interpreting the complex visual contents as text descriptions, which requires the model to fully understand video scenes including objects and their interactions. Prevailing methods adopt off-the-shelf object detection networks to give object proposals and use the attention mechanism to model the relations between objects. They often miss some undefined semantic concepts of the pretrained model and fail to identify exact predicate relationships between objects. In this paper, we investigate an open research task of generating text descriptions for the given videos, and propose Cross-Modal Graph (CMG) with meta concepts for video captioning. Specifically, to cover the useful semantic concepts in video captions, we weakly learn the corresponding visual regions for text descriptions, where the associated visual regions and textual words are named cross-modal meta concepts. We further?¡­",Article,https://ieeexplore.ieee.org/abstract/document/9844447/,7,wang2022cross,"Video captioning, vision-and-language"
2022,Pattern Recognition 126,Decomposing generation networks with structure prediction for recipe generation,"Hao Wang, Guosheng Lin, Steven CH Hoi, Chunyan Miao",Hao Wang,English,"Recipe generation from food images and ingredients is a challenging task, which requires the interpretation of the information from another modality. Different from the image captioning task, where the captions usually have one sentence, cooking instructions contain multiple sentences and have obvious structures. To help the model capture the recipe structure and avoid missing some cooking details, we propose a novel framework: Decomposing Generation Networks (DGN) with structure prediction, to get more structured and complete recipe generation outputs. Specifically, we split each cooking instruction into several phases, and assign different sub-generators to each phase. Our approach includes two novel ideas: (i) learning the recipe structures with the global structure prediction component and (ii) producing recipe phases in the sub-generator output component based on the predicted structure. Extensive?¡­",Article,https://www.sciencedirect.com/science/article/pii/S0031320322000590,2,wang2022decomposing,"Structure Learning, Text Generation, Image-to-Text"
2024,International Journal of Computer Vision,Maniclip: Multi-attribute face manipulation from text,"Hao Wang, Guosheng Lin, Ana Garc¨ªa del Molino, Anran Wang, Jiashi Feng, Zhiqi Shen",Hao Wang,English,"In this paper we present a novel multi-attribute face manipulation method based on textual descriptions. Previous text-based image editing methods either require test-time optimization for each individual image or are restricted to single attribute editing. Extending these methods to multi-attribute face image editing scenarios will introduce undesired excessive attribute change, eg, text-relevant attributes are overly manipulated and text-irrelevant attributes are also changed. In order to address these challenges and achieve natural editing over multiple face attributes, we propose a new decoupling training scheme where we use group sampling to get text segments from same attribute categories, instead of whole complex sentences. Further, to preserve other existing face attributes, we encourage the model to edit the latent code of each attribute separately via an entropy constraint. During the inference phase, our?¡­",Article,https://link.springer.com/article/10.1007/s11263-024-02088-6,6,wang2024maniclip,"multi-attribute face manipulation, textual description, texts"
2022,arXiv preprint arXiv:2207.14425,3D cartoon face generation with controllable expressions from a single GAN image,"Hao Wang, Guosheng Lin, Steven CH Hoi, Chunyan Miao",Hao Wang,English,"In this paper, we investigate an open research task of generating 3D cartoon face shapes from single 2D GAN generated human faces and without 3D supervision, where we can also manipulate the facial expressions of the 3D shapes. To this end, we discover the semantic meanings of StyleGAN latent space, such that we are able to produce face images of various expressions, poses, and lighting by controlling the latent codes. Specifically, we first finetune the pretrained StyleGAN face model on the cartoon datasets. By feeding the same latent codes to face and cartoon generation models, we aim to realize the translation from 2D human face images to cartoon styled avatars. We then discover semantic directions of the GAN latent space, in an attempt to change the facial expressions while preserving the original identity. As we do not have any 3D annotations for cartoon faces, we manipulate the latent codes to generate images with different poses and lighting, such that we can reconstruct the 3D cartoon face shapes. We validate the efficacy of our method on three cartoon datasets qualitatively and quantitatively.",Article,https://arxiv.org/abs/2207.14425,6,wang20223d,"3D Generation, Image Manipulation"
2024,International Journal of Computer Vision,Learning Temporal Variations for 4D Point Cloud Segmentation,"Hanyu Shi, Jiacheng Wei, Hao Wang, Fayao Liu, Guosheng Lin",Hanyu Shi,English,"LiDAR-based 3D scene perception is a fundamental and important task for autonomous driving. Most state-of-the-art methods on LiDAR-based 3D recognition tasks focus on single-frame 3D point cloud data, ignoring temporal information. We argue that the temporal information across the frames provides crucial knowledge for 3D scene perceptions, especially in the driving scenario. In this paper, we focus on spatial and temporal variations to better explore temporal information across 3D frames. We design a temporal variation-aware interpolation module and a temporal voxel-point refinement module to capture the temporal variation in the 4D point cloud. The temporal variation-aware interpolation generates local features from the previous and current frames by capturing spatial coherence and temporal variation information. The temporal voxel-point refinement module builds a temporal graph on the 3D point?¡­",Article,https://link.springer.com/article/10.1007/s11263-024-02149-w,0,shi2024learning,"4D point cloud, Semantic segmentation, Scene Understanding"
2022,Drones,Smart decision-support system for pig farming,"Hao Wang, Boyang Li, Haoming Zhong, Ahong Xu, Yingjie Huang, Jingfu Zou, Yuanyuan Chen, Pengcheng Wu, Yiqiang Chen, Cyril Leung, Chunyan Miao",Hao Wang,English,"There are multiple participants, such as farmers, wholesalers, retailers, financial institutions, etc., involved in the modern food production process. All of these participants and stakeholders have a shared goal, which is to gather information on the food production process so that they can make appropriate decisions to increase productivity and reduce risks. However, real-time data collection and analysis continue to be difficult tasks, particularly in developing nations, where agriculture is the primary source of income for the majority of the population. In this paper, we present a smart decision-support system for pig farming. Specifically, we first adopt rail-based unmanned vehicles to capture pigsty images. We then conduct image stitching to avoid double-counting pigs so that we can use image segmentation method to give precise masks for each pig. Based on the segmentation masks, the pig weights can be estimated, and data can be integrated in our developed mobile app. The proposed system enables the above participants and stakeholders to have real-time data and intelligent analysis reports to help their decision-making.",Article,https://www.mdpi.com/2504-446X/6/12/389,3,wang2022smart,"smart agriculture, pig farming"
2023,arXiv preprint arXiv:2312.14871,BrainVis: Exploring the Bridge between Brain and Visual Signals via Image Reconstruction,"Honghao Fu, Zhiqi Shen, Jing Jih Chin, Hao Wang",Honghao Fu,English,"Analyzing and reconstructing visual stimuli from brain signals effectively advances understanding of the human visual system. However, the EEG signals are complex and contain a amount of noise. This leads to substantial limitations in existing works of visual stimuli reconstruction from EEG, such as difficulties in aligning EEG embeddings with the fine-grained semantic information and a heavy reliance on additional large self-collected dataset for training. To address these challenges, we propose a novel approach called BrainVis. Firstly, we divide the EEG signals into various units and apply a self-supervised approach on them to obtain EEG time-domain features, in an attempt to ease the training difficulty. Additionally, we also propose to utilize the frequency-domain features to enhance the EEG representations. Then, we simultaneously align EEG time-frequency embeddings with the interpolation of the coarse and fine-grained semantics in the CLIP space, to highlight the primary visual components and reduce the cross-modal alignment difficulty. Finally, we adopt the cascaded diffusion models to reconstruct images. Our proposed BrainVis outperforms state of the arts in both semantic fidelity reconstruction and generation quality. Notably, we reduce the training data scale to 10% of the previous work.",Article,https://arxiv.org/abs/2312.14871,1,fu2023brainvis,"visual stimuli,brain signals,EEG,CLIP"
2024,arXiv preprint arXiv:2406.09779,OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst,"Jingtao Cao, Zheng Zhang, Hongru Wang, Bin Liang, Hao Wang, Kam-Fai Wong",Jingtao Cao,English,"Memes, which rapidly disseminate personal opinions and positions across the internet, also pose significant challenges in propagating social bias and prejudice. This study presents a novel approach to detecting harmful memes, particularly within the multicultural and multilingual context of Singapore. Our methodology integrates image captioning, Optical Character Recognition (OCR), and Large Language Model (LLM) analysis to comprehensively understand and classify harmful memes. Utilizing the BLIP model for image captioning, PP-OCR and TrOCR for text recognition across multiple languages, and the Qwen LLM for nuanced language understanding, our system is capable of identifying harmful content in memes created in English, Chinese, Malay, and Tamil. To enhance the system's performance, we fine-tuned our approach by leveraging additional data labeled using GPT-4V, aiming to distill the understanding capability of GPT-4V for harmful memes to our system. Our framework achieves top-1 at the public leaderboard of the Online Safety Prize Challenge hosted by AI Singapore, with the AUROC as 0.7749 and accuracy as 0.7087, significantly ahead of the other teams. Notably, our approach outperforms previous benchmarks, with FLAVA achieving an AUROC of 0.5695 and VisualBERT an AUROC of 0.5561.",Article,https://arxiv.org/abs/2406.09779,0,cao2024ospc,"Large Language Models, Multimodal Detection, Harmful Memes"
2024,arXiv preprint arXiv:2405.04103,COM3D: Leveraging Cross-View Correspondence and Cross-Modal Mining for 3D Retrieval,"Hao Wu, Ruochong LI, Hao Wang, Hui Xiong",Hao Wu,English,"In this paper, we investigate an open research task of cross-modal retrieval between 3D shapes and textual descriptions. Previous approaches mainly rely on point cloud encoders for feature extraction, which may ignore key inherent features of 3D shapes, including depth, spatial hierarchy, geometric continuity, etc. To address this issue, we propose COM3D, making the first attempt to exploit the cross-view correspondence and cross-modal mining to enhance the retrieval performance. Notably, we augment the 3D features through a scene representation transformer, to generate cross-view correspondence features of 3D shapes, which enrich the inherent features and enhance their compatibility with text matching. Furthermore, we propose to optimize the cross-modal matching process based on the semi-hard negative example mining method, in an attempt to improve the learning efficiency. Extensive quantitative and qualitative experiments demonstrate the superiority of our proposed COM3D, achieving state-of-the-art results on the Text2Shape dataset.",Article,https://arxiv.org/abs/2405.04103,0,wu2024com3d,"Scene representation transformer, 3D retrieval"
2024,arXiv preprint arXiv:2403.17411,PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language Models,"Jinyi Li, Yihuai Lan, Lei Wang, Hao Wang",Jinyi Li,English,"Prompt compression is an innovative method for efficiently condensing input prompts while preserving essential information. To facilitate quick-start services, user-friendly interfaces, and compatibility with common datasets and metrics, we present the Prompt Compression Toolkit (PCToolkit). This toolkit is a unified plug-and-play solution for compressing prompts in Large Language Models (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and metrics for comprehensive performance evaluation. PCToolkit boasts a modular design, allowing for easy integration of new datasets and metrics through portable and user-friendly interfaces. In this paper, we outline the key components and functionalities of PCToolkit. We conducted evaluations of the compressors within PCToolkit across various natural language tasks, including reconstruction, summarization, mathematical problem-solving, question answering, few-shot learning, synthetic tasks, code completion, boolean expressions, multiple choice questions, and lies recognition.",Article,https://arxiv.org/abs/2403.17411,0,li2024pctoolkit,"prompt compression,plug-and-play,large language models, natural language tasks"
2024,arXiv preprint arXiv:2402.17971,All in a Single Image: Large Multimodal Models are In-Image Learners,"Lei Wang, Wanyu Xu, Zhiqiang Hu, Yihuai Lan, Shan Dong, Hao Wang, Roy Ka-Wei Lee, Ee-Peng Lim",Lei Wang,English,"This paper introduces a new in-context learning (ICL) mechanism called In-Image Learning (IL) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V. Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, IL consolidates all information into one image and primarily leverages image processing, understanding, and reasoning abilities. This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text. To further combine the strengths of different ICL methods, we introduce an automatic strategy to select the appropriate ICL method for a data example in a given task. We conducted experiments on MathVista and Hallusionbench to test the effectiveness of IL in complex multimodal reasoning tasks and mitigating language hallucination and visual illusion. Additionally, we explored the impact of image resolution, the number of demonstration examples, and their positions on the effectiveness of IL. Our code is publicly available at https://github.com/AGI-Edgerunners/IIL.",Article,https://arxiv.org/abs/2402.17971,0,wang2024all,"in-context learning,GPT-4V, In-Image Learning,large multimodal model"
