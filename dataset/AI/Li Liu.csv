Year,Sources,Name,Authors,First Author,Chinese/English,Abstract,Venues,doi,Citation,Id,Keywords
2021,Proceedings of the IEEE/CVF international conference on computer vision?¡­,The ninth visual object tracking vot2021 challenge results,"Matej Kristan, Ji?¨ª Matas, Ale? Leonardis, Michael Felsberg, Roman Pflugfelder, Joni-Kristian K?m?r?inen, Hyung Jin Chang, Martin Danelljan, Luka Cehovin, Alan Luke?i?, Ondrej Drbohlav, Jani K?pyl?, Gustav H?ger, Song Yan, Jinyu Yang, Zhongqun Zhang, Gustavo Fern¨¢ndez",Matej Kristan,English,"The Visual Object Tracking challenge VOT2021 is the ninth annual tracker benchmarking activity organized by the VOT initiative. Results of 71 trackers are presented; many are state-of-the-art trackers published at major com-puter vision conferences or in journals in recent years. The VOT2021 challenge was composed of four sub-challenges focusing on different tracking domains:(i) VOT-ST2021 challenge focused on short-term tracking in RGB,(ii) VOT-RT2021 challenge focused on"" real-time"" short-term track-ing in RGB,(iii) VOT-LT2021 focused on long-term track-ing, namely coping with target disappearance and reap-pearance and (iv) VOT-RGBD2021 challenge focused on long-term tracking in RGB and depth imagery. The VOT-ST2021 dataset was refreshed, while VOT-RGBD2021 in-troduces a training dataset and sequestered dataset for win-ner identification. The source code for most of the trackers, the datasets, the evaluation kit and the results along with the source code for most trackers are publicly available at the challenge website.",Conference paper,http://openaccess.thecvf.com/content/ICCV2021W/VOT/html/Kristan_The_Ninth_Visual_Object_Tracking_VOT2021_Challenge_Results_ICCVW_2021_paper.html,114,kristan2021ninth,"Training,Location awareness,Visualization,Computer vision,Target tracking,Correlation,Focusing"
2022,ECCV 2022,Data-free Backdoor Removal based on Channel Lipschitzness,"Runkai Zheng, Rongjun Tang, Jianze Li, Li Liu",Runkai Zheng,English,"Recent studies have shown that Deep Neural Networks (DNNs) are vulnerable to the backdoor attacks, which leads to malicious behaviors of DNNs when specific triggers are attached to the input images. It was further demonstrated that the infected DNNs possess a collection of channels, which are more sensitive to the backdoor triggers compared with normal channels. Pruning these channels was then shown to be effective in mitigating the backdoor behaviors. To locate those channels, it is natural to consider their Lipschitzness, which measures their sensitivity against worst-case perturbations on the inputs. In this work, we introduce a novel concept called Channel Lipschitz Constant (CLC), which is defined as the Lipschitz constant of the mapping from the input images to the output of each channel. Then we provide empirical evidences to show the strong correlation between an Upper bound of the CLC (UCLC?¡­",Article,https://link.springer.com/chapter/10.1007/978-3-031-20065-6_11,75,zheng2022data,"Deep neural network, Backdoor defense, Lipschitz constant, Model pruning"
2023,arXiv preprint arXiv:2305.08196,A comprehensive survey on segment anything model for vision and beyond,"Chunhui Zhang, Li Liu, Yawen Cui, Guanjie Huang, Weilin Lin, Yiqian Yang, Yuehong Hu",Chunhui Zhang,English,"Artificial intelligence (AI) is evolving towards artificial general intelligence, which refers to the ability of an AI system to perform a wide range of tasks and exhibit a level of intelligence similar to that of a human being. This is in contrast to narrow or specialized AI, which is designed to perform specific tasks with a high degree of efficiency. Therefore, it is urgent to design a general class of models, which we term foundation models, trained on broad data that can be adapted to various downstream tasks. The recently proposed segment anything model (SAM) has made significant progress in breaking the boundaries of segmentation, greatly promoting the development of foundation models for computer vision. To fully comprehend SAM, we conduct a survey study. As the first to comprehensively review the progress of segmenting anything task for vision and beyond based on the foundation model of SAM, this work focuses on its applications to various tasks and data types by discussing its historical development, recent progress, and profound impact on broad applications. We first introduce the background and terminology for foundation models including SAM, as well as state-of-the-art methods contemporaneous with SAM that are significant for segmenting anything task. Then, we analyze and summarize the advantages and limitations of SAM across various image processing applications, including software scenes, real-world scenes, and complex scenes. Importantly, many insights are drawn to guide future research to develop more versatile foundation models and improve the architecture of SAM. We also summarize massive other amazing?¡­",Conference paper,https://arxiv.org/abs/2305.08196,48,zhang2023comprehensive,"Survey, Artificial General Intelligence, Foundation Models, Segment Anything, Open Source Projects"
2022,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?¡­,Boosting black-box attack with partially transferred conditional adversarial distribution,"Yan Feng, Baoyuan Wu, Yanbo Fan, Li Liu, Zhifeng Li, Shu-Tao Xia",Yan Feng,English,"This work studies black-box adversarial attacks against deep neural networks (DNNs), where the attacker can only access the query feedback returned by the attacked DNN model, while other information such as model parameters or the training datasets are unknown. One promising approach to improve attack performance is utilizing the adversarial transferability between some white-box surrogate models and the target model (ie, the attacked model). However, due to the possible differences on model architectures and training datasets between surrogate and target models, dubbed"" surrogate biases"", the contribution of adversarial transferability to improving the attack performance may be weakened. To tackle this issue, we innovatively propose a black-box attack method by developing a novel mechanism of adversarial transferability, which is robust to the surrogate biases. The general idea is transferring partial parameters of the conditional adversarial distribution (CAD) of surrogate models, while learning the untransferred parameters based on queries to the target model, to keep the flexibility to adjust the CAD of the target model on any new benign sample. Extensive experiments on benchmark datasets and attacking against real-world API demonstrate the superior attack performance of the proposed method.",Conference paper,http://openaccess.thecvf.com/content/CVPR2022/html/Feng_Boosting_Black-Box_Attack_With_Partially_Transferred_Conditional_Adversarial_Distribution_CVPR_2022_paper.html,36,feng2022boosting,Adversarial attack and defense
2021,ICASSP 2021-2021 IEEE International Conference on Acoustics,Self-supervised depth estimation via implicit cues from videos,"Jianrong Wang, Ge Zhang, Zhenyu Wu, Xuewei Li, Li Liu",Jianrong Wang,English,"In self-supervised monocular depth estimation, the depth discontinuity and motion objects' artifacts are still challenging problems. Existing self-supervised methods usually utilize two views to train the depth estimation network and use one single view to make predictions. Compared with static views, abundant dynamic properties between video frames are beneficial to refining depth estimation, especially for dynamic objects. In this work, we improve the self-supervised learning framework for depth estimation using consecutive frames from monocular and stereo videos. The main idea is to exploit an implicit depth cue extractor which leverages dynamic and static cues to generate useful depth proposals. These cues can predict distinguishable motion contours and geometric scene structures. Moreover, a new high-dimensional attention module is proposed to extract a clear global transformation, which effectively?¡­",Conference paper,https://ieeexplore.ieee.org/abstract/document/9413407/,6,wang2021self,"Self-supervised learning, monocular depth estimation, implicit cues, monocular video"
2021,MICCAI-International Conference on Medical Image Computing and Computer?¡­,Uscl: Pretraining deep ultrasound image diagnosis model through video contrastive representation learning,"Yixiong Chen, Chunhui Zhang, Li Liu, Cheng Feng, Changfeng Dong, Yongfang Luo, Xiang Wan",Yixiong Chen,English," Most deep neural networks (DNNs) based ultrasound (US) medical image analysis models use pretrained backbones (e.g., ImageNet) for better model generalization. However, the domain gap between natural and medical images causes an inevitable performance bottleneck. To alleviate this problem, an US dataset named US-4 is constructed for direct pretraining on the same domain. It contains over 23,000 images from four US video sub-datasets. To learn robust features from US-4, we propose an US semi-supervised contrastive learning method, named USCL, for pretraining. In order to avoid high similarities between negative pairs as well as mine abundant visual features from limited US videos, USCL adopts a sample pair generation method to enrich the feature involved in a single step of contrastive optimization. Extensive experiments on several downstream tasks show the superiority of USCL?¡­",Conference paper,https://link.springer.com/chapter/10.1007/978-3-030-87237-3_60,41,chen2021uscl,"Ultrasound,Pretrained model,Contrastive learning."
2021,IEEE Transactions on Multimedia 23,Re-synchronization using the hand preceding model for multi-modal fusion in automatic continuous cued speech recognition,"Li Liu, Gang Feng, Denis Beautemps, Xiao-Ping Zhang",Li Liu,English,"Cued Speech (CS) is an augmented lip reading system complemented by hand coding, and it is very helpful to the deaf people. Automatic CS recognition can help communications between the deaf people and others. Due to the asynchronous nature of lips and hand movements, fusion of them in automatic CS recognition is a challenging problem. In this work, we propose a novel re-synchronization procedure for multi-modal fusion, which aligns the hand features with lips feature. It is realized by delaying hand position and hand shape with their optimal hand preceding time which is derived by investigating the temporal organizations of hand position and hand shape movements in CS. This re-synchronization procedure is incorporated into a practical continuous CS recognition system that combines convolutional neural network (CNN) with multi-stream hidden markov model (MSHMM). A significant improvement of?¡­",Article,https://ieeexplore.ieee.org/abstract/document/9016100/,40,liu2020re,"Cued speech, multi-modal fusion, re-synchronization procedure, automatic CS recognition, CNN, MSHMM"
2020,ICTAI-2020 IEEE 32nd International Conference on Tools with Artificial?¡­,Semi-supervised active learning for COVID-19 lung ultrasound multi-symptom classification,"Lei Liu, Wentao Lei, Li Liu, Xiang Wan, Yongfang Luo, Cheng Feng",Lei Liu,English,"Ultrasound (US) is a non-invasive yet effective medical diagnostic imaging technique for the COVID-19 global pandemic. However, due to complex feature behaviors and expensive annotations of US images, it is difficult to apply Artificial Intelligence (AI) assisting approaches for the lung's multi-symptom (multi-label) classification. To overcome these difficulties, we propose a novel semi-supervised Two-Stream Active Learning (TSAL) method to model complicated features and reduce labeling costs in an iterative manner. The core component of TSAL is the multi-label learning mechanism, in which label correlation information is used to design a multi-label margin (MLM) strategy and a confidence validation for automatically selecting informative samples and confident labels. In this framework, a multi-symptom multi-label (MSML) classification network is proposed to learn discriminative features of lung symptoms?¡­",Conference paper,https://ieeexplore.ieee.org/abstract/document/9288321/,32,liu2020semi,"COVID-19, Ultrasound Imaging, Multi-Label Classification, Active Learning, Semi-Supervised Learning"
2022,Neurips 2022,Pre-activation Distributions Expose Backdoor Neurons,"Runkai Zheng, Rongjun Tang, Jianze Li, Li Liu",Runkai Zheng,English,"Convolutional neural networks (CNN) can be manipulated to perform specific behaviors when encountering a particular trigger pattern without affecting the performance on normal samples, which is referred to as backdoor attack. The backdoor attack is usually achieved by injecting a small proportion of poisoned samples into the training set, through which the victim trains a model embedded with the designated backdoor. In this work, we demonstrate that backdoor neurons are exposed by their pre-activation distributions, where populations from benign data and poisoned data show significantly different moments. This property is shown to be attack-invariant and allows us to efficiently locate backdoor neurons. On this basis, we make several proper assumptions on the neuron activation distributions, and propose two backdoor neuron detection strategies based on (1) the differential entropy of the neurons, and (2) the Kullback-Leibler divergence between the benign sample distribution and a poisoned statistics based hypothetical distribution. Experimental results show that our proposed defense strategies are both efficient and effective against various backdoor attacks.",Conference paper,https://proceedings.neurips.cc/paper_files/paper/2022/hash/76917808731dae9e6d62c2a7a6afb542-Abstract-Conference.html,25,zheng2022pre,"Chemical activation, Convolutional neural networks, Population statistics,Activation distribution, Backdoors, Differential entropy, Kullback Leibler divergence, Neuron detection, Performance, Property, Sample distributions, Training sets"
2023,arXiv preprint arXiv:2302.09457 1,"Adversarial machine learning: A systematic survey of backdoor attack, weight attack and adversarial example","Baoyuan Wu, Li Liu, Zihao Zhu, Qingshan Liu, Zhaofeng He, Siwei Lyu",Baoyuan Wu,English,,Article,https://scholar.google.com/scholar?cluster=12056669301010787640&hl=en&oi=scholarr,22,wu2023adversarial,"Adversarial machine learning, training-time adversarial attack, deployment-time adversarial attack, inference-time adversarial attack"
2021,IEEE Transactions on Multimedia 24,Adaptive semantic-spatio-temporal graph convolutional network for lip reading,"Changchong Sheng, Xinzhong Zhu, Huiying Xu, Matti Pietik?inen, Li Liu",Changchong Sheng,English,"The goal of this work is to recognize words, phrases, and sentences being spoken by a talking face without given the audio. Current deep learning approaches for lip reading focus on exploring the appearance and optical flow information of videos. However, these methods do not fully exploit the characteristics of lip motion. In addition to appearance and optical flow, the mouth contour deformation usually conveys significant information that is complementary to others. However, the modeling of dynamic mouth contour has received little attention than that of appearance and optical flow. In this work, we propose a novel model of dynamic mouth contours called Adaptive Semantic-Spatio-Temporal Graph Convolution Network (ASST-GCN), to go beyond previous methods by automatically learning both the spatial and temporal information from videos. To combine the complementary information from appearance and?¡­",Article,https://ieeexplore.ieee.org/abstract/document/9514437/,21,sheng2021adaptive,"Lip reading, semantic-spatio-temporal, adaptive graph convolution network, two-stream"
2022,TPAMI,WebUAV-3M: A Benchmark Unveiling the Power of Million-Scale Deep UAV Tracking,"Chunhui Zhang, Guanjie Huang, Li Liu, Shan Huang, Yinan Yang, Xiang Wan, Shiming Ge, Dacheng Tao",Chunhui Zhang,English,"Unmanned aerial vehicle (UAV) tracking is of great significance for a wide range of applications, such as delivery and agriculture. Previous benchmarks in this area mainly focused on small-scale tracking problems while ignoring the amounts of data, types of data modalities, diversities of target categories and scenarios, and evaluation protocols involved, greatly hiding the massive power of deep UAV tracking. In this article, we propose WebUAV-3M, the largest public UAV tracking benchmark to date, to facilitate both the development and evaluation of deep UAV trackers. WebUAV-3M contains over 3.3 million frames across 4,500 videos and offers 223 highly diverse target categories. Each video is densely annotated with bounding boxes by an efficient and scalable semi-automatic target annotation (SATA) pipeline. Importantly, to take advantage of the complementary superiority of language and audio, we enrich?¡­",Article,https://ieeexplore.ieee.org/abstract/document/10004511/,17,zhang2022webuav,"Benchmark, language and audio annotation, semi-automatic labeling, scenario constraint evaluation, UAV tracking"
2021,Interspeech 2021,Cross-Modal Knowledge Distillation Method for Automatic Cued Speech Recognition,"Jianrong Wang, Ziyue Tang, Xuewei Li, Mei Yu, Qiang Fang, Li Liu",Jianrong Wang,English,"Cued Speech (CS) is a visual communication system for the deaf or hearing impaired people. It combines lip movements with hand cues to obtain a complete phonetic repertoire. Current deep learning based methods on automatic CS recognition suffer from a common problem, which is the data scarcity. Until now, there are only two public single speaker datasets for French (238 sentences) and British English (97 sentences). In this work, we propose a cross-modal knowledge distillation method with teacher-student structure, which transfers audio speech information to CS to overcome the limited data problem. Firstly, we pretrain a teacher model for CS recognition with a large amount of open source audio speech data, and simultaneously pretrain the feature extractors for lips and hands using CS data. Then, we distill the knowledge from teacher model to the student model with frame-level and sequence-level distillation strategies. Importantly, for frame-level, we exploit multi-task learning to weigh losses automatically, to obtain the balance coefficient. Besides, we establish a five-speaker British English CS dataset for the first time. The proposed method is evaluated on French and British English CS datasets, showing superior CS recognition performance to the state-of-the-art (SOTA) by a large margin.",Article,https://arxiv.org/abs/2106.13686,17,wang2021cross,"Cued Speech, Cross-modal knowledge distillation, Teacher-student structure, Cued Speech recognition"
2023,arXiv preprint arXiv:2305.10843,X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models,"Yixiong Chen, Li Liu, Chris Ding",Yixiong Chen,English,"This paper introduces a novel explainable image quality evaluation approach called X-IQE, which leverages visual large language models (LLMs) to evaluate text-to-image generation methods by generating textual explanations. X-IQE utilizes a hierarchical Chain of Thought (CoT) to enable MiniGPT-4 to produce self-consistent, unbiased texts that are highly correlated with human evaluation. It offers several advantages, including the ability to distinguish between real and generated images, evaluate text-image alignment, and assess image aesthetics without requiring model training or fine-tuning. X-IQE is more cost-effective and efficient compared to human evaluation, while significantly enhancing the transparency and explainability of deep image quality evaluation models. We validate the effectiveness of our method as a benchmark using images generated by prevalent diffusion models. X-IQE demonstrates similar performance to state-of-the-art (SOTA) evaluation methods on COCO Caption, while overcoming the limitations of previous evaluation models on DrawBench, particularly in handling ambiguous generation prompts and text recognition in generated images. Project website: https://github.com/Schuture/Benchmarking-Awesome-Diffusion-Models",Article,https://arxiv.org/abs/2305.10843,15,chen2023x,"DrawBench, Chain-of-Thought, Image Aesthetics, Text Recognition, MiniGPT-4, Text-to-Image Generation, Text-image Alignment"
2022,ICASSP 2022-2022 IEEE International Conference on Acoustics,Residual-Guided Personalized Speech Synthesis based on Face Image,"Jianrong Wang, Zixuan Wang, Xiaosheng Hu, Xuewei Li, Qiang Fang, Li Liu",Jianrong Wang,English,"Previous works derive personalized speech features by training the model on a large dataset composed of his/her audio sounds. It was reported that face information has a strong link with the speech sound. Thus in this work, we innovatively extract personalized speech features from human faces to synthesize personalized speech using neural vocoder. A Face-based Residual Personalized Speech Synthesis Model (FR-PSS) containing a speech encoder, a speech synthesizer and a face encoder is designed for PSS. In this model, by designing two speech priors, a residual-guided strategy is introduced to guide the face feature to approach the true speech feature in the training. Moreover, considering the error of feature¡¯s absolute values and their directional bias, we formulate a novel tri-item loss function for face encoder. Experimental results show that the speech synthesized by our model is comparable to the?¡­",Conference paper,https://ieeexplore.ieee.org/abstract/document/9746808/,12,wang2022residual,"Personalized speech synthesis, Speech prior, Residual, Attention mechanism"
2021,2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI),Multi-modal active learning for automatic liver fibrosis diagnosis based on ultrasound shear wave elastography,"Lufei Gao, Ruisong Zhou, Changfeng Dong, Cheng Feng, Zhen Li, Xiang Wan, Li Liu",Lufei Gao,English,"With the development of radiomics, noninvasive diagnosis like ultrasound (US) imaging plays a very important role in automatic liver fibrosis diagnosis (ALFD). Due to the noisy data, expensive annotations of US images, the application of Artificial Intelligence (AI) assisting approaches encounters a bottleneck. Besides, the use of single-modal US data limits the further improve of the classification results. In this work, we innovatively propose a multi-modal fusion network with active learning (MMFN-AL) for ALFD to exploit the information of multiple modalities, eliminate the noisy data and reduce the annotation cost. Four image modalities including US and three types of shear wave elastography (SWEs) are exploited. A new dataset containing these modalities from 214 candidates is collected and pre-processed, with the labels obtained from the liver biopsy results. Experimental results show that our proposed method?¡­",Conference paper,https://ieeexplore.ieee.org/abstract/document/9434170/,12,gao2021multi,"Liver fibrosis diagnosis, Shear wave elastography, Active learning, Attention, Multi-modal fusion"
2023,arXiv preprint arXiv:2306.00816,"Versatile Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers","Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Baoyuan Wu",Ruotong Wang,English,"Deep neural networks (DNNs) can be manipulated to exhibit specific behaviors when exposed to specific trigger patterns, without affecting their performance on benign samples, dubbed \textit{backdoor attack}. Currently, implementing backdoor attacks in physical scenarios still faces significant challenges. Physical attacks are labor-intensive and time-consuming, and the triggers are selected in a manual and heuristic way. Moreover, expanding digital attacks to physical scenarios faces many challenges due to their sensitivity to visual distortions and the absence of counterparts in the real world. To address these challenges, we define a novel trigger called the \textbf{V}isible, \textbf{S}emantic, \textbf{S}ample-Specific, and \textbf{C}ompatible (VSSC) trigger, to achieve effective, stealthy and robust simultaneously, which can also be effectively deployed in the physical scenario using corresponding objects. To implement the VSSC trigger, we propose an automated pipeline comprising three modules: a trigger selection module that systematically identifies suitable triggers leveraging large language models, a trigger insertion module that employs generative models to seamlessly integrate triggers into images, and a quality assessment module that ensures the natural and successful insertion of triggers through vision-language models. Extensive experimental results and analysis validate the effectiveness, stealthiness, and robustness of the VSSC trigger. It can not only maintain robustness under visual distortions but also demonstrates strong practicality in the physical scenario. We hope that the proposed VSSC trigger and implementation approach?¡­",Article,https://arxiv.org/abs/2306.00816,11,wang2023versatile,"Backdoor attacks, physical attacks"
2023,Proceedings of the 46th International ACM SIGIR Conference on Research and?¡­,Fedads: A benchmark for privacy-preserving cvr estimation with vertical federated learning,"Penghui Wei, Hongjian Dou, Shaoguo Liu, Rongjun Tang, Li Liu, Liang Wang, Bo Zheng",Penghui Wei,English,"Conversion rate (CVR) estimation aims to predict the probability of conversion event after a user has clicked an ad. Typically, online publisher has user browsing interests and click feedbacks, while demand-side advertising platform collects users' post-click behaviors such as dwell time and conversion decisions. To estimate CVR accurately and protect data privacy better, vertical federated learning (vFL) is a natural solution to combine two sides' advantages for training models, without exchanging raw data. Both CVR estimation and applied vFL algorithms have attracted increasing research attentions. However, standardized and systematical evaluations are missing: due to the lack of standardized datasets, existing studies adopt public datasets to simulate a vFL setting via hand-crafted feature partition, which brings challenges to fair comparison. We introduce FedAds, the first benchmark for CVR estimation with?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3539618.3591909,10,wei2023fedads,"Ad Ranking, Vertical Federated Learning, Deep Generative Model"
2022,IEEE Transactions on Medical Imaging,Generating and Weighting Semantically Consistent Sample Pairs for Ultrasound Contrastive Learning,"Yixiong Chen, Chunhui Zhang, Chris HQ Ding, Li Liu",Yixiong Chen,English,"Well-annotated medical datasets enable deep neural networks (DNNs) to gain strong power in extracting lesion-related features. Building such large and well-designed medical datasets is costly due to the need for high-level expertise. Model pre-training based on ImageNet is a common practice to gain better generalization when the data amount is limited. However, it suffers from the domain gap between natural and medical images. In this work, we pre-train DNNs on ultrasound (US) domains instead of ImageNet to reduce the domain gap in medical US applications. To learn US image representations based on unlabeled US videos, we propose a novel meta-learning-based contrastive learning method, namely Meta Ultrasound Contrastive Learning (Meta-USCL). To tackle the key challenge of obtaining semantically consistent sample pairs for contrastive learning, we present a positive pair generation module?¡­",Article,https://ieeexplore.ieee.org/abstract/document/9980429/,10,chen2022generating,"Computer aided diagnosis, neural networks, contrastive learning, meta-learning, medical ultrasound, pneumonia, COVID-19, breast tumor"
2021,Hepatology International,Diagnosis of Significant Liver Fibrosis in Patients With Chronic Hepatitis B By Using a Deep Learning-Based Data Integration Network,"Zhong Liu, Huiying Wen, Ziqi Zhu, Qinyuan Li, Li Liu, Tianjiao Li, Wencong Xu, Chao Hou, Bin Huang, Zhiyan Li, Changfeng Dong, Xin Chen",Zhong Liu,English,"Background and aimsChronic hepatitis B virus (CHB) infection remains a major global health burden and the non-invasive and accurate diagnosis of significant liver fibrosis (¡Ý?F2) in CHB patients is clinically very important. This study aimed to assess the potential of the joint use of ultrasound images of liver parenchyma, liver stiffness values, and patients¡¯ clinical parameters in a deep learning model to improve the diagnosis of?¡Ý?F2 in CHB patients.MethodsOf 527 CHB patients who underwent US examination, liver elastography and biopsy, 284 eligible patients were included. We developed a deep learning-based data integration network (DI-Net) to fuse the information of ultrasound images of liver parenchyma, liver stiffness values and patients¡¯ clinical parameters for diagnosing?¡Ý?F2 in CHB patients. The performance of DI-Net was cross-validated in a main cohort (n?=?155) of the included patients and?¡­",Article,https://link.springer.com/article/10.1007/s12072-021-10294-4,10,liu2022diagnosis,"Fatty liver disease,Liver fibrosis, Significant liver fibrosis,Chronic hepatitis B ,Ultrasonography , Liver elastography ,Deep learning , Feature fusion , Non-invasive fibrosis staging ,Fibrosis biomarker"
2023,arXiv preprint arXiv:2312.08890,Defenses in adversarial machine learning: A survey,"Baoyuan Wu, Shaokui Wei, Mingli Zhu, Meixi Zheng, Zihao Zhu, Mingda Zhang, Hongrui Chen, Danni Yuan, Li Liu, Qingshan Liu",Baoyuan Wu,English,"Adversarial phenomenon has been widely observed in machine learning (ML) systems, especially in those using deep neural networks, describing that ML systems may produce inconsistent and incomprehensible predictions with humans at some particular cases. This phenomenon poses a serious security threat to the practical application of ML systems, and several advanced attack paradigms have been developed to explore it, mainly including backdoor attacks, weight attacks, and adversarial examples. For each individual attack paradigm, various defense paradigms have been developed to improve the model robustness against the corresponding attack paradigm. However, due to the independence and diversity of these defense paradigms, it is difficult to examine the overall robustness of an ML system against different kinds of attacks.This survey aims to build a systematic review of all existing defense paradigms from a unified perspective. Specifically, from the life-cycle perspective, we factorize a complete machine learning system into five stages, including pre-training, training, post-training, deployment, and inference stages, respectively. Then, we present a clear taxonomy to categorize and review representative defense methods at each individual stage. The unified perspective and presented taxonomies not only facilitate the analysis of the mechanism of each defense paradigm but also help us to understand connections and differences among different defense paradigms, which may inspire future research to develop more advanced, comprehensive defenses.",Article,https://arxiv.org/abs/2312.08890,8,wu2023defenses,"Machine learning, computer vision, adversarial machine learning, backdoor learning, backdoor defense, weight defense, adversarial examples, adversarial defense"
2023,International Conference on Medical Image Computing and Computer-Assisted?¡­,Metalr: Meta-tuning of learning rates for transfer learning in medical imaging,"Yixiong Chen, Li Liu, Jingxian Li, Hua Jiang, Chris Ding, Zongwei Zhou",Yixiong Chen,English,"In medical image analysis, transfer learning is a powerful method for deep neural networks (DNNs) to generalize on limited medical data. Prior efforts have focused on developing pre-training algorithms on domains such as lung ultrasound, chest X-ray, and liver CT to bridge domain gaps. However, we find that model fine-tuning also plays a crucial role in adapting medical knowledge to target tasks. The common fine-tuning method is manually picking transferable layers (e.g., the last few layers) to update, which is labor-expensive. In this work, we propose a meta-learning-based learning rate (LR) tuner, named MetaLR, to make different layers automatically co-adapt to downstream tasks based on their transferabilities across domains. MetaLR learns LRs for different layers in an online manner, preventing highly transferable layers from forgetting their medical representation abilities and driving less transferable?¡­",Conference paper,https://link.springer.com/chapter/10.1007/978-3-031-43907-0_67,3,chen2023metalr,"Medical image analysis,Meta-learning, Transfer learning."
2022,ICASSP 2022  preprint arXiv:2212.01083,Cross-Modal Mutual Learning for Cued Speech Recognition,"Lei Liu, Li Liu",Lei Liu,English,"Automatic Cued Speech Recognition (ACSR) provides an intelligent human-machine interface for visual communications, where the Cued Speech (CS) system utilizes lip movements and hand gestures to code spoken language for hearing-impaired people. Previous ACSR approaches often utilize direct feature concatenation as the main fusion paradigm. However, the asynchronous modalities (i.e., lip, hand shape and hand position) in CS may cause interference for feature concatenation. To address this challenge, we propose a transformer based cross-modal mutual learning framework to prompt multi-modal interaction. Compared with the vanilla self-attention, our model forces modality-specific information of different modalities to pass through a modality-invariant codebook, concatenating linguistic representations with tokens of each modality. Then the shared linguistic knowledge is used to re-synchronize?¡­",Article,https://ieeexplore.ieee.org/abstract/document/10095271/,8,liu2023cross,"Mandarin Chinese Cued Speech, Multi-modal Transformer, Linguistic Representation"
2022,ICASSP 2022-2022 IEEE International Conference on Acoustics,Acoustic-to-Articulatory Inversion Based on Speech Decomposition and Auxiliary Feature,"Jianrong Wang, Jinyu Liu, Longxuan Zhao, Shanyu Wang, Ruiguo Yu, Li Liu",Jianrong Wang,English,"Acoustic-to-articulatory inversion (AAI) is to obtain the movement of articulators from speech signals. Until now, achieving a speaker-independent AAI remains a challenge given the limited data. Besides, most current works only use audio speech as input, causing an inevitable performance bottleneck. To solve these problems, firstly, we pre-train a speech decomposition network to decompose audio speech into speaker embedding and content embedding as the new personalized speech features to adapt to the speaker-independent case. Secondly, to further improve the AAI, we propose a novel auxiliary feature network to estimate the lip auxiliary features from the above personalized speech features. Experimental results on three public datasets show that, compared with the state-of-the-art only using the audio speech feature, the proposed method reduces the average RMSE by 0.25 and increases the average?¡­",Conference paper,https://ieeexplore.ieee.org/abstract/document/9746125/,7,wang2022acoustic,"Acoustic-to-articulatory inversion, Speech , decomposition, Personalized speech feature, Auxiliary feature, Speaker-independent"
2022,Proceedings of the Asian Conference on Computer Vision,Hico: Hierarchical contrastive learning for ultrasound video model pretraining,"Chunhui Zhang, Yixiong Chen, Li Liu, Qiong Liu, Xi Zhou",Chunhui Zhang,English,"The self-supervised ultrasound (US) video model pretraining can use a small amount of labeled data to achieve one of the most promising results on US diagnosis. However, it does not take full advantage of multi-level knowledge for learning deep neural networks (DNNs), and thus is difficult to learn transferable feature representations. This work proposes a hierarchical contrastive learning (HiCo) method to improve the transferability for the US video model pretraining. HiCo introduces both peer-level semantic alignment and cross-level semantic alignment to facilitate the interaction between different semantic levels, which can effectively accelerate the convergence speed, leading to better generalization and adaptation of the learned model. Additionally, a softened objective function is implemented by smoothing the hard labels, which can alleviate the negative effect caused by local similarities of images between different classes. Experiments with HiCo on five datasets demonstrate its favorable results over state-of-the-art approaches. The source code of this work is publicly available at https://github. com/983632847/HiCo.",Conference paper,https://openaccess.thecvf.com/content/ACCV2022/html/Zhang_HiCo_Hierarchical_Contrastive__Learning_for_Ultrasound_Video_Model_Pretraining_ACCV_2022_paper.html,7,zhang2022hico,"contrastive learning, deep neural networks, cross-level semantic alignment,hard labels"
2023,Interspeech preprint arXiv:2306.03594,Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks,"Jianrong Wang, Yaxin Zhao, Li Liu, Tianyi Xu, Qi Li, Sen Li",Jianrong Wang,English,"Given an audio clip and a reference face image, the goal of the talking head generation is to generate a high-fidelity talking head video. Although some audio-driven methods of generating talking head videos have made some achievements in the past, most of them only focused on lip and audio synchronization and lack the ability to reproduce the facial expressions of the target person. To this end, we propose a talking head generation model consisting of a Memory-Sharing Emotion Feature extractor (MSEF) and an Attention-Augmented Translator based on U-net (AATU). Firstly, MSEF can extract implicit emotional auxiliary features from audio to estimate more accurate emotional face landmarks.~Secondly, AATU acts as a translator between the estimated landmarks and the photo-realistic video frames. Extensive qualitative and quantitative experiments have shown the superiority of the proposed method to the previous works. Codes will be made publicly available.",Article,https://arxiv.org/abs/2306.03594,6,wang2023emotional,"Audio-driven Talking Head Generation, Emotion, Memory-sharing, U-net, Attention"
2023,ICASSP 2023-2023 IEEE International Conference on Acoustics,Memory-augmented contrastive learning for talking head generation,"Jianrong Wang, Yaxin Zhao, Hongkai Fan, Tianyi Xu, Qi Li, Sen Li, Li Liu",Jianrong Wang,English,"Given one reference facial image and a piece of speech as input, talking head generation aims to synthesize a realistic-looking talking head video. However, generating a lip-synchronized video with natural head movements is challenging. The same speech clip can generate multiple possible lip and head movements, that is, there is no one-to-one mapping relationship between them. To overcome this problem, we propose a Speech Feature Extractor (SFE) based on memory-augmented self-supervised contrastive learning, which introduces the memory module to store multiple different speech mapping results. In addition, we introduce the Mixed Density Networks (MDN) into the landmark regression task to generate multiple predicted facial landmarks. Extensive qualitative and quantitative experiments show that the quality of our facial animation is significantly superior to that of the state-of-the-art (SOTA). The?¡­",Conference paper,https://ieeexplore.ieee.org/abstract/document/10096593/,6,wang2023memory,"Talking head generation, Contrastive learning, Memory bank, Mixture density networks"
2023,arXiv preprint arXiv:2302.09457,Attacks in adversarial machine learning: A systematic survey from the life-cycle perspective,"Baoyuan Wu, Zihao Zhu, Li Liu, Qingshan Liu, Zhaofeng He, Siwei Lyu",Baoyuan Wu,English,"Adversarial machine learning (AML) studies the adversarial phenomenon of machine learning, which may make inconsistent or unexpected predictions with humans. Some paradigms have been recently developed to explore this adversarial phenomenon occurring at different stages of a machine learning system, such as backdoor attack occurring at the pre-training, in-training and inference stage; weight attack occurring at the post-training, deployment and inference stage; adversarial attack occurring at the inference stage. However, although these adversarial paradigms share a common goal, their developments are almost independent, and there is still no big picture of AML. In this work, we aim to provide a unified perspective to the AML community to systematically review the overall progress of this field. We firstly provide a general definition about AML, and then propose a unified mathematical framework to covering existing attack paradigms. According to the proposed unified framework, we build a full taxonomy to systematically categorize and review existing representative methods for each paradigm. Besides, using this unified framework, it is easy to figure out the connections and differences among different attack paradigms, which may inspire future researchers to develop more advanced attack paradigms. Finally, to facilitate the viewing of the built taxonomy and the related literature in adversarial machine learning, we further provide a website, \ie, \url{http://adversarial-ml.com}, where the taxonomies and literature will be continuously updated.",Article,https://arxiv.org/abs/2302.09457,6,wu2023attacks,"Adversarial machine learning, training-time adversarial attack, deployment-time adversarial attack, inference-time adversarial attack"
2022,Ann Transl Med 2022;10(6):271,Ultrasound score combined with liver stiffness measurement by sound touch elastography for staging liver fibrosis in patients with chronic hepatitis B: a clinical prospective study,"Kun Huang, Qinyuan Li, Weimei Zeng, Xin Chen, Li Liu, Xiang Wan, Cheng Feng, Zhiyan Li, Zhong Liu, Changfeng Dong",Kun Huang,English,"BackgroundA noninvasive and precise diagnosis of liver fibrosis in patients with chronic hepatitis B (CHB) is crucial for establishing the optimal time and strategy of therapy and for predicting treatment response. This study aimed to assess the diagnostic performance of ultrasound (US) score and liver stiffness measurement (LSM) of sound touch elastography (STE) in diagnosing liver fibrosis stages and to investigate whether combining these methods would improve liver fibrosis staging.MethodsUS and STE examinations were performed in CHB patients included. Liver biopsy was used as a reference standard. A diagnostic marker with the optimal linear combination (LC) of US score and LSM of STE, namely LC marker, was established for noninvasive assessment of liver fibrosis stages. The diagnostic performance of the LC marker was evaluated by using receiver operating characteristic (ROC) curves and the?¡­",Article,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9011233/,6,huang2022ultrasound,"Ultrasound, sound touch elastography (STE), liver stiffness measurement (LSM), chronic hepatitis B (CHB), liver fibrosis staging"
2021,Interspeech 2021: https://arxiv.org/abs/2106.14016,An Attention Self-supervised Contrastive Learning based Three-stage Model for Hand Shape Feature Representation in Cued Speech,"Jianrong Wang, Nan Gu, Mei Yu, Xuewei Li, Qiang Fang, Li Liu",Jianrong Wang,English,"Cued Speech (CS) is a communication system for deaf people or hearing impaired people, in which a speaker uses it to aid a lipreader in phonetic level by clarifying potentially ambiguous mouth movements with hand shape and positions. Feature extraction of multi-modal CS is a key step in CS recognition. Recent supervised deep learning based methods suffer from noisy CS data annotations especially for hand shape modality. In this work, we first propose a self-supervised contrastive learning method to learn the feature representation of image without using labels. Secondly, a small amount of manually annotated CS data are used to fine-tune the first module. Thirdly, we present a module, which combines Bi-LSTM and self-attention networks to further learn sequential features with temporal and contextual information. Besides, to enlarge the volume and the diversity of the current limited CS datasets, we build a new British English dataset containing 5 native CS speakers. Evaluation results on both French and British English datasets show that our model achieves over 90% accuracy in hand shape recognition. Significant improvements of 8.75% (for French) and 10.09% (for British English) are achieved in CS phoneme recognition correctness compared with the state-of-the-art.",Article,https://arxiv.org/abs/2106.14016,6,wang2021attention,"Cued Speech, Self-supervised contrastive learn- ing, Self-attention network, Hand shape recognition"
2023,ACM MM preprint arXiv:2308.03432,Cuing Without Sharing: A Federated Cued Speech Recognition Framework via Mutual Knowledge Distillation,"Yuxuan Zhang, Lei Liu, Li Liu",Yuxuan Zhang,English,"Cued Speech (CS) is a visual coding tool to encode spoken languages at the phonetic level, which combines lip-reading and hand gestures to effectively assist communication among people with hearing impairments. The Automatic CS Recognition (ACSR) task aims to recognize CS videos into linguistic texts, which involves both lips and hands as two distinct modalities conveying complementary information. However, the traditional centralized training approach poses potential privacy risks due to the use of facial and gesture videos in CS data. To address this issue, we propose a new Federated Cued Speech Recognition (FedCSR) framework to train an ACSR model over the decentralized CS data without sharing private information. In particular, a mutual knowledge distillation method is proposed to maintain cross-modal semantic consistency of the Non-IID CS data, which ensures learning a unified feature?¡­",Article,https://dl.acm.org/doi/abs/10.1145/3581783.3612134,5,zhang2023cuing,"Cued Speech, Federated Learning, Data Privacy"
2022,ISCSLP 2022,Objective Hand Complexity Comparison between Two Mandarin Chinese Cued Speech Systems,"Li Liu, Gang Feng, Xiaoxi Ren, Xianping Ma",Li Liu,English,"Recently, a pilot Mandarin Chinese Cued Speech (MCCS) system, called MCCS-1 was proposed with a main characteristic that each vowel is coded by only one specific hand position, without using any hand slides to code diphthongs. Indeed, hand slides are also used in some other languages of CS to code diphthongs. In order to demonstrate that the MCCS-1 system possesses real advantages over systems using hand slides, in this work, we first propose a novel MCCS-2 by introducing hand slides to code diphthongs, and a ¡°push out move¡± for ending consonants [n] or [ng] of nasalized vowels. Then, we present a multi-parameter hand complexity measure method to compare MCCS-1 and MCCS-2 by measuring three kinematic parameters, which are the time duration of words realization, hand move trajectory length and average speed of hand movements. Moreover, the first MCCS corpus for these two systems?¡­",Article,https://ieeexplore.ieee.org/abstract/document/10037814/,5,liu2022objective,"Mandarin Chinese Cued Speech, hand slides, kinematic parameters, time duration, trajectory length"
2021,https://arxiv.org/abs/2011.10951,Learning Class Unique Features in Fine-Grained Visual Classification,"Runkai Zheng, Zhijia Yu, Yinqi Zhang, Chris Ding, Hei Victor Cheng, Li Liu",Runkai Zheng,English,"A major challenge in Fine-Grained Visual Classification (FGVC) is distinguishing various categories with high inter-class similarity by learning the feature that differentiate the details. Conventional cross entropy trained Convolutional Neural Network (CNN) fails this challenge as it may suffer from producing inter-class invariant features in FGVC. In this work, we innovatively propose to regularize the training of CNN by enforcing the uniqueness of the features to each category from an information theoretic perspective. To achieve this goal, we formulate a minimax loss based on a game theoretic framework, where a Nash equilibria is proved to be consistent with this regularization objective. Besides, to prevent from a feasible solution of minimax loss that may produce redundant features, we present a Feature Redundancy Loss (FRL) based on normalized inner product between each selected feature map pair to complement the proposed minimax loss. Superior experimental results on several influential benchmarks along with visualization show that our method gives full play to the performance of the baseline model without additional computation and achieves comparable results with state-of-the-art models.",Article,https://arxiv.org/abs/2011.10951,5,zheng2020learning,"Fine-Grained Visual Classification,Convolutional Neural Netword"
2024,IEEE/ACM Transactions on Audio,Computation and parameter efficient multi-modal fusion transformer for cued speech recognition,"Lei Liu, Li Liu, Haizhou Li",Lei Liu,English,"Cued Speech (CS) is a pure visual coding method used by hearing-impaired people that combines lip reading with several specific hand shapes to make the spoken language visible. Automatic CS recognition (ACSR) seeks to transcribe visual cues of speech into text, which can help hearing-impaired people to communicate effectively. The visual information of CS contains lip reading and hand cueing, thus the fusion of them plays an important role in ACSR. However, most previous fusion methods struggle to capture the global dependency present in long sequence inputs of multi-modal CS data. As a result, these methods generally fail to learn the effective cross-modal relationships that contribute to the fusion. Recently, attention-based transformers have been a prevalent idea for capturing the global dependency over the long sequence in multi-modal fusion, but existing multi-modal fusion transformers suffer from both?¡­",Article,https://ieeexplore.ieee.org/abstract/document/10432942/,4,liu2024computation,"Transformer, cross-attention, automatic speech recognition, computation and parameter efficient"
2021,2020 25th International Conference on Pattern Recognition (ICPR),Three-dimensional lip motion network for text-independent speaker recognition,"Jianrong Wang, Tong Wu, Shanyu Wang, Mei Yu, Qiang Fang, Ju Zhang, Li Liu",Jianrong Wang,English,"Lip motion reflects behavior characteristics of speakers, and thus can be used as a new kind of biometrics in speaker recognition. In the literature, lots of works used two-dimensional (2D) lip images to recognize speaker in a text-dependent context. However, 2D lip easily suffers from various face orientations. To this end, in this work, we present a novel end-to-end 3D lip motion Network (3LMNet) by utilizing the sentence-level 3D lip motion (S3DLM) to recognize speakers in both the text-independent and text-dependent contexts. A new regional feedback module (RFM) is proposed to obtain attentions in different lip regions. Besides, prior knowledge of lip motion is investigated to complement RFM, where landmark-level and frame-level features are merged to form a better feature representation. Moreover, we present two methods, i.e., coordinate transformation and face posture correction to pre-process the LSD-AV?¡­",Conference paper,https://ieeexplore.ieee.org/abstract/document/9413218/,4,wang2021three,"Adversarial Robustness, Multimodal Learning, Zero-Shot Learning, Vision-Language Models"
2023,ACM MM preprint arXiv:2307.03373,All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment,"Chunhui Zhang, Xin Sun, Li Liu, Yiqian Yang, Qiong Liu, Xi Zhou, Yanfeng Wang",Chunhui Zhang,English,"Current mainstream vision-language (VL) tracking framework consists of three parts,i.e., a visual feature extractor, a language feature extractor, and a fusion model. To pursue better performance, a natural modus operandi for VL tracking is employing customized and heavier unimodal encoders, and multi-modal fusion models. Albeit effective, existing VL trackers separate feature extraction and feature integration, resulting in extracted features that lack semantic guidance and have limited target-aware capability in complex scenarios, e.g., similar distractors and extreme illumination. In this work, inspired by the recent success of exploring foundation models with unified architecture for both natural language and computer vision tasks, we propose an All-in-One framework, which learns joint feature extraction and interaction by adopting a unified transformer backbone. Specifically, we mix raw vision and language?¡­",Article,https://dl.acm.org/doi/abs/10.1145/3581783.3611803,3,zhang2023all,"Unified vision-language tracking, multi-modal alignment"
2023,Interspeech,A Novel Interpretable and Generalizable Re-synchronization Model for Cued Speech based on a Multi-Cuer Corpus,"Lufei Gao, Shan Huang, Li Liu",Lufei Gao,English,"Cued Speech (CS) is a multi-modal visual coding system combining lip reading with several hand cues at the phonetic level to make the spoken language visible to the hearing impaired. Previous studies solved asynchronous problems between lip and hand movements by a cuer\footnote{The people who perform Cued Speech are called the cuer.}-dependent piecewise linear model for English and French CS. In this work, we innovatively propose three statistical measure on the lip stream to build an interpretable and generalizable model for predicting hand preceding time (HPT), which achieves cuer-independent by a proper normalization. Particularly, we build the first Mandarin CS corpus comprising annotated videos from five speakers including three normal and two hearing impaired individuals. Consequently, we show that the hand preceding phenomenon exists in Mandarin CS production with significant differences between normal and hearing impaired people. Extensive experiments demonstrate that our model outperforms the baseline and the previous state-of-the-art methods.",Article,https://arxiv.org/abs/2306.02596,3,gao2023novel,"Mandarin Cued Speech, Asynchronous multi-modality, Hand preceding time, Hearing-impaired"
2023,ICASSP 2023-2023 IEEE International Conference on Acoustics,Two-Stream Joint-Training for Speaker Independent Acoustic-to-Articulatory Inversion,"Jianrong Wang, Jinyu Liu, Xuewei Li, Mei Yu, Jie Gao, Qiang Fang, Li Liu",Jianrong Wang,English,"Acoustic-to-articulatory inversion (AAI) aims to estimate the parameters of articulators from speech audio. There are two common challenges in AAI, which are the limited data and the unsatisfactory performance in speaker independent scenario. Most current works focus on extracting features directly from speech and ignoring the importance of phoneme information which may limit the performance of AAI. To this end, we propose a novel network called SPN that uses two different streams to carry out the AAI task. Firstly, to improve the performance of speaker-independent experiment, we propose a new phoneme stream network to estimate the articulatory parameters as the phoneme features. To the best of our knowledge, this is the first work that extracts the speaker-independent features from phonemes to improve the performance of AAI. Secondly, in order to better represent the speech information, we train a?¡­",Conference paper,https://ieeexplore.ieee.org/abstract/document/10095994/,3,wang2023two,"Acoustic-to-articulatory inversion, Local features, Global features, Speaker-independent, Phoneme stream"
2023,Proceedings of the IEEE/CVF International Conference on Computer Vision?¡­,Global Balanced Experts for Federated Long-Tailed Learning,"Yaopei Zeng, Lei Liu, Li Liu, Li Shen, Shaoguo Liu, Baoyuan Wu",Yaopei Zeng,English,"Federated learning (FL) is a prevalent distributed machine learning approach that enables collaborative training of a global model across multiple devices without sharing local data. However, the presence of long-tailed data can negatively deteriorate the model's performance in real-world FL applications. Moreover, existing re-balance strategies are less effective for the federated long-tailed issue when directly utilizing local label distribution as the class prior at the clients' side. To this end, we propose a novel Global Balanced Multi-Expert (GBME) framework to optimize a balanced global objective, which does not require additional information beyond the standard FL pipeline. In particular, a proxy is derived from the accumulated gradients uploaded by the clients after local training, and is shared by all clients as the class prior for re-balance training. Such a proxy can also guide the client grouping to train a multi-expert model, where the knowledge from different clients can be aggregated via the ensemble of different experts corresponding to different client groups. To further strengthen the privacy-preserving ability, we present a GBME-p algorithm with a theoretical guarantee to prevent privacy leakage from the proxy. Extensive experiments on long-tailed decentralized datasets demonstrate the effectiveness of GBME and GBME-p, both of which show superior performance to state-of-the-art methods.",Conference paper,http://openaccess.thecvf.com/content/ICCV2023/html/Zeng_Global_Balanced_Experts_for_Federated_Long-Tailed_Learning_ICCV_2023_paper.html,3,zeng2023global,"Training,Performance evaluation,Privacy,Differential privacy,Computer vision,Federated learning,Pipelines"
2022,ECAI 2022,TAOTF: A Two-stage Approximately Orthogonal Training Framework in Deep Neural Networks,"Taoyong Cui, Jianze Li, Yuhan Dong, Li Liu",Taoyong Cui,English,"The orthogonality constraints, including the hard and soft ones, have been used to normalize the weight matrices of Deep Neural Network (DNN) models, especially the Convolutional Neural Network (CNN) and Vision Transformer (ViT), to reduce model parameter redundancy and improve training stability. However, the robustness to noisy data of these models with constraints is not always satisfactory. In this work, we propose a novel two-stage approximately orthogonal training framework (TAOTF) to find a trade-off between the orthogonal solution space and the main task solution space to solve this problem in noisy data scenarios. In the first stage, we propose a novel algorithm called polar decomposition-based orthogonal initialization (PDOI) to find a good initialization for the orthogonal optimization. In the second stage, unlike other existing methods, we apply soft orthogonal constraints for all layers of DNN?¡­",Article,https://ebooks.iospress.nl/doi/10.3233/FAIA230310,3,cui2023taotf,"Deep neural network, Convolutional neural network, Vision Transformer, trade-off, orthogonal training"
2023,Interspeech preprint arXiv:2306.02263,MAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information,"Jianrong Wang, Yuchen Huo, Li Liu, Tianyi Xu, Qi Li, Sen Li",Jianrong Wang,English,"Audio-visual speech recognition (AVSR) gains increasing attention from researchers as an important part of human-computer interaction. However, the existing available Mandarin audio-visual datasets are limited and lack the depth information. To address this issue, this work establishes the MAVD, a new large-scale Mandarin multimodal corpus comprising 12,484 utterances spoken by 64 native Chinese speakers. To ensure the dataset covers diverse real-world scenarios, a pipeline for cleaning and filtering the raw text material has been developed to create a well-balanced reading material. In particular, the latest data acquisition device of Microsoft, Azure Kinect is used to capture depth information in addition to the traditional audio signals and RGB images during data acquisition. We also provide a baseline experiment, which could be used to evaluate the effectiveness of the dataset. The dataset and code will be released at https://github.com/SpringHuo/MAVD.",Article,https://arxiv.org/abs/2306.02263,2,wang2023mavd,"Audio-Visual Speech Recognition, Mandarin Audio-Visual Corpus, Azure Kinect, Depth Information"
2021,https://arxiv.org/abs/2107.03758,Investigate the Essence of Long-Tailed Recognition from a Unified Perspective,"Lei Liu, Li Liu",Lei Liu,English,"As the data scale grows, deep recognition models often suffer from long-tailed data distributions due to the heavy imbalanced sample number across categories. Indeed, real-world data usually exhibit some similarity relation among different categories (eg, pigeons and sparrows), called category similarity in this work. It is doubly difficult when the imbalance occurs between such categories with similar appearances. However, existing solutions mainly focus on the sample number to re-balance data distribution. In this work, we systematically investigate the essence of the long-tailed problem from a unified perspective. Specifically, we demonstrate that long-tailed recognition suffers from both sample number and category similarity. Intuitively, using a toy example, we first show that sample number is not the unique influence factor for performance dropping of long-tailed recognition. Theoretically, we demonstrate that (1) category similarity, as an inevitable factor, would also influence the model learning under long-tailed distribution via similar samples,(2) using more discriminative representation methods (eg, self-supervised learning) for similarity reduction, the classifier bias can be further alleviated with greatly improved performance. Extensive experiments on several long-tailed datasets verify the rationality of our theoretical analysis, and show that based on existing state-of-the-arts (SOTAs), the performance could be further improved by similarity reduction. Our investigations highlight the essence behind the long-tailed problem, and claim several feasible directions for future work.",Article,https://www.researchgate.net/profile/Li-Liu-219/publication/353116853_Investigate_the_Essence_of_Long-Tailed_Recognition_from_a_Unified_Perspective/links/60ef957d9541032c6d3eaa44/Investigate-the-Essence-of-Long-Tailed-Recognition-from-a-Unified-Perspective.pdf,2,liu2021investigate,"state-of-the-arts,long-tailed distribution,long-tailed recognition"
2020,CoRR,Towards Class-Specific Unit,"Runkai Zheng, Zhijia Yu, Yinqi Zhang, Chris Ding, Hei Victor Cheng, Li Liu",Runkai Zheng,English,"Class selectivity is an attribute of a unit in deep neural networks, which characterizes the discriminative ability of units to a specific class. Intuitively, decisions made by several highly selective units are more interpretable since it is easier to be traced back to the origin while that made by complex combinations of lowly selective units are more difficult to interpret. In this work, we develop a novel way to directly train highly selective units, through which we are able to examine the performance of a network that only rely on highly selective units. Specifically, we train the network such that all the units in the penultimate layer only response to one specific class, which we named as class-specific unit. By innovatively formulating the problem using mutual information, we find that in such a case, the output of the model has a special form that all the probabilities over non-target classes are uniformly distributed. We then propose a minimax loss based on a game theoretic framework to achieve the goal. Nash equilibria are proved to exist and the outcome is consistent with our regularization objective. Experimental results show that the model trained with the proposed objective outperforms models trained with baseline objective among all the tasks we test. Our results shed light on the role of class-specific units by indicating that they can be directly used for decisions without relying on low selective units.",Article,https://www.academia.edu/download/109551417/2011.10951v1.pdf,2,zheng2020towards,"class selectivity, Nash equilibria, class-specfic"
2024,arXiv preprint arXiv:2405.19818,WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark,"Chunhui Zhang, Li Liu, Guanjie Huang, Hao Wen, Xi Zhou, Yanfeng Wang",Chunhui Zhang,English,"Underwater object tracking (UOT) is a foundational task for identifying and tracing submerged entities in underwater video sequences. However, current UOT datasets suffer from limitations in scale, diversity of target categories and scenarios covered, hindering the training and evaluation of modern tracking algorithms. To bridge this gap, we take the first step and introduce WebUOT-1M, \ie, the largest public UOT benchmark to date, sourced from complex and realistic underwater environments. It comprises 1.1 million frames across 1,500 video clips filtered from 408 target categories, largely surpassing previous UOT datasets, \eg, UVOT400. Through meticulous manual annotation and verification, we provide high-quality bounding boxes for underwater targets. Additionally, WebUOT-1M includes language prompts for video sequences, expanding its application areas, \eg, underwater vision-language tracking. Most existing trackers are tailored for open-air environments, leading to performance degradation when applied to UOT due to domain gaps. Retraining and fine-tuning these trackers are challenging due to sample imbalances and limited real-world underwater datasets. To tackle these challenges, we propose a novel omni-knowledge distillation framework based on WebUOT-1M, incorporating various strategies to guide the learning of the student Transformer. To the best of our knowledge, this framework is the first to effectively transfer open-air domain knowledge to the UOT model through knowledge distillation, as demonstrated by results on both existing UOT datasets and the newly proposed WebUOT-1M. Furthermore, we?¡­",Article,https://arxiv.org/abs/2405.19818,1,zhang2024webuot,"Underwater object tracking, underwater video sequence, benchmark, knowledge distillation"
2024,arXiv preprint arXiv:2405.14200,Awesome Multi-modal Object Tracking,"Chunhui Zhang, Li Liu, Hao Wen, Xi Zhou, Yanfeng Wang",Chunhui Zhang,English,"Multi-modal object tracking (MMOT) is an emerging field that combines data from various modalities, \eg vision (RGB), depth, thermal infrared, event, language and audio, to estimate the state of an arbitrary object in a video sequence. It is of great significance for many applications such as autonomous driving and intelligent surveillance. In recent years, MMOT has received more and more attention. However, existing MMOT algorithms mainly focus on two modalities (\eg RGB+depth, RGB+thermal infrared, and RGB+language). To leverage more modalities, some recent efforts have been made to learn a unified visual object tracking model for any modality. Additionally, some large-scale multi-modal tracking benchmarks have been established by simultaneously providing more than two modalities, such as vision-language-audio (\eg WebUAV-3M) and vision-depth-language (\eg UniMod1K). To track the latest progress in MMOT, we conduct a comprehensive investigation in this report. Specifically, we first divide existing MMOT tasks into five main categories, \ie RGBL tracking, RGBE tracking, RGBD tracking, RGBT tracking, and miscellaneous (RGB+X), where X can be any modality, such as language, depth, and event. Then, we analyze and summarize each MMOT task, focusing on widely used datasets and mainstream tracking algorithms based on their technical paradigms (\eg self-supervised learning, prompt learning, knowledge distillation, generative models, and state space models). Finally, we maintain a continuously updated paper list for MMOT at https://github.com/983632847/Awesome-Multimodal-Object-Tracking.",Article,https://arxiv.org/abs/2405.14200,1,zhang2024awesome,"Multi-modal object tracking, RGBL tracking, RGBE tracking, RGBD tracking, RGBT tracking, Miscellaneous (RGB+X)"
2024,IJCAI 2024: arXiv preprint arXiv:2404.19277,Bridge to Non-Barrier Communication: Gloss-Prompted Fine-grained Cued Speech Gesture Generation with Diffusion Model,"Wentao Lei, Li Liu, Jun Wang",Wentao Lei,English,"Cued Speech (CS) is an advanced visual phonetic encoding system that integrates lip reading with hand codings, enabling people with hearing impairments to communicate efficiently. CS video generation aims to produce specific lip and gesture movements of CS from audio or text inputs. The main challenge is that given limited CS data, we strive to simultaneously generate fine-grained hand and finger movements, as well as lip movements, meanwhile the two kinds of movements need to be asynchronously aligned. Existing CS generation methods are fragile and prone to poor performance due to template-based statistical models and careful hand-crafted pre-processing to fit the models. Therefore, we propose a novel Gloss-prompted Diffusion-based CS Gesture generation framework (called GlossDiff). Specifically, to integrate additional linguistic rules knowledge into the model. we first introduce a bridging instruction called \textbf{Gloss}, which is an automatically generated descriptive text to establish a direct and more delicate semantic connection between spoken language and CS gestures. Moreover, we first suggest rhythm is an important paralinguistic feature for CS to improve the communication efficacy. Therefore, we propose a novel Audio-driven Rhythmic Module (ARM) to learn rhythm that matches audio speech. Moreover, in this work, we design, record, and publish the first Chinese CS dataset with four CS cuers. Extensive experiments demonstrate that our method quantitatively and qualitatively outperforms current state-of-the-art (SOTA) methods. We release the code and data at https://glossdiff.github.io/.",Article,https://arxiv.org/abs/2404.19277,1,lei2024bridge,"state-of-the-arts,Cued Speech,diffusion model, gesture generation"
2023,arXiv preprint arXiv:2312.06230,Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks,"Danni Yuan, Shaokui Wei, Mingda Zhang, Li Liu, Baoyuan Wu",Danni Yuan,English,"This work focuses on defending against the data poisoning based backdoor attacks, which bring in serious security threats to deep neural networks (DNNs). Specifically, given a untrustworthy training dataset, we aim to filter out potential poisoned samples, \ie, poisoned sample detection (PSD). The key solution for this task is to find a discriminative metric between clean and poisoned samples, even though there is no information about the potential poisoned samples (\eg, the attack method, the poisoning ratio). In this work, we develop an innovative detection approach from the perspective of the gradient \wrt activation (\ie, activation gradient direction, AGD) of each sample in the backdoored model trained on the untrustworthy dataset. We present an interesting observation that the circular distribution of AGDs among all samples of the target class is much more dispersed than that of one clean class. Motivated by this observation, we firstly design a novel metric called Cosine similarity Variation towards Basis Transition (CVBT) to measure the circular distribution's dispersion of each class. Then, we design a simple yet effective algorithm with identifying the target class(es) using outlier detection on CVBT scores of all classes, followed by progressively filtering of poisoned samples according to the cosine similarities of AGDs between every potential sample and a few additional clean samples. Extensive experiments under various settings verify that given very few clean samples of each class, the proposed method could filter out most poisoned samples, while avoiding filtering out clean samples, verifying its effectiveness on the PSD task. Codes are?¡­",Article,https://arxiv.org/abs/2312.06230,1,yuan2023activation,"poisoned sample detection,backdoor attacks,gradients,activation, deep neural network"
2023,arXiv preprint arXiv:2308.08849,A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation,"Li Liu, Lufei Gao, Wentao Lei, Fengji Ma, Xiaotian Lin, Jinting Wang",Li Liu,English,"Body language (BL) refers to the non-verbal communication expressed through physical movements, gestures, facial expressions, and postures. It is a form of communication that conveys information, emotions, attitudes, and intentions without the use of spoken or written words. It plays a crucial role in interpersonal interactions and can complement or even override verbal communication. Deep multi-modal learning techniques have shown promise in understanding and analyzing these diverse aspects of BL. The survey emphasizes their applications to BL generation and recognition. Several common BLs are considered i.e., Sign Language (SL), Cued Speech (CS), Co-speech (CoS), and Talking Head (TH), and we have conducted an analysis and established the connections among these four BL for the first time. Their generation and recognition often involve multi-modal approaches. Benchmark datasets for BL research are well collected and organized, along with the evaluation of SOTA methods on these datasets. The survey highlights challenges such as limited labeled data, multi-modal learning, and the need for domain adaptation to generalize models to unseen speakers or languages. Future research directions are presented, including exploring self-supervised learning techniques, integrating contextual information from other modalities, and exploiting large-scale pre-trained multi-modal models. In summary, this survey paper provides a comprehensive understanding of deep multi-modal learning for various BL generations and recognitions for the first time. By analyzing advancements, challenges, and future directions, it serves as a?¡­",Conference paper,https://arxiv.org/abs/2308.08849,1,liu2023survey,"Deep Multi-modal Learning, Body Language, Sign Language, Cued Speech, Co-speech, Talking Head, Recognition and Generation"
2023,ICASSP 2023-2023 IEEE International Conference on Acoustics,Spatio-Temporal Structure Consistency for Semi-Supervised Medical Image Classification,"Wentao Lei, Lei Liu, Li Liu",Wentao Lei,English,"Intelligent medical diagnosis has shown remarkable progress on the large-scale datasets with full annotations. However, very few labeled images are available due to significantly expensive annotations by experts. To efficiently leverage abundant unlabeled data, we propose a novel Spatio-Temporal Structure Consistent (STSC) learning framework to combine both spatial and temporal structure consistency. Specifically, a gram matrix is derived to capture the structural similarity among different training samples in the representation space. At the spatial level, our framework explicitly enforces the consistency of structural similarity among different samples under perturbations. At the temporal level, we desire to maintain the consistency of the structural similarity in different training iterations by digging out the stable sub-structures in a relation graph. Experiments on two medical image datasets (i.e., ISIC 2018 and?¡­",Conference paper,https://ieeexplore.ieee.org/abstract/document/10095930/,1,lei2023spatio,"Medical Image Classification, Semi-supervised Learning, Spatial-temporal Structure Consistency"
2023,,Label-distribution-agnostic ensemble learning on federated long-tailed data,"Yaopei Zeng, Lei Liu, Baoyuan Wu, ShaoGuo Liu, Li Liu",Yaopei Zeng,English,"Federated Learning (FL) is a distributed machine learning paradigm that enables devices to collaboratively train a shared model. However, the long-tailed distribution in nature deteriorates the performance of the global model, which is difficult to address due to data heterogeneity, e.g., local clients may exhibit diverse imbalanced class distributions. Moreover, existing re-balance strategies generally utilize label distribution as the class prior, which may conflict with the privacy requirement of FL. To this end, we propose a Label-Distribution-Agnostic Ensemble (LDAE) learning framework to integrate heterogeneous data distributions using multiple experts, which targets to optimize a balanced global objective under privacy protection. In particular, we derive a privacy-preserving proxy from the model updates of clients to guide the grouping and updating of multiple experts. Knowledge from clients can be aggregated via implicit interactions among different expert groups. We theoretically and experimentally demonstrate that (1) there is a global objective gap between global and local re-balance strategies\footnote{The local re-balance strategy means that each client utilizes re-balance methods based on the local label distribution, while the global re-balance strategy applies re-balance methods using global label distribution as the class-wise prior.} and (2) with protecting data privacy, the proxy can be used as an alternative to label distribution for existing class prior based re-balance strategies. Extensive experiments on long-tailed decentralized datasets demonstrate the effectiveness of our method, showing superior performance to state-of-the-art?¡­",Conference paper,https://openreview.net/forum?id=l4f-zJqY2s,1,zeng2023label,None
2022,arXiv preprint arXiv:2212.00399,Rethinking Two Consensuses of the Transferability in Deep Learning,"Yixiong Chen, Jingxian Li, Chris Ding, Li Liu",Yixiong Chen,English,"Deep transfer learning (DTL) has formed a long-term quest toward enabling deep neural networks (DNNs) to reuse historical experiences as efficiently as humans. This ability is named knowledge transferability. A commonly used paradigm for DTL is firstly learning general knowledge (pre-training) and then reusing (fine-tuning) them for a specific target task. There are two consensuses of transferability of pre-trained DNNs: (1) a larger domain gap between pre-training and downstream data brings lower transferability; (2) the transferability gradually decreases from lower layers (near input) to higher layers (near output). However, these consensuses were basically drawn from the experiments based on natural images, which limits their scope of application. This work aims to study and complement them from a broader perspective by proposing a method to measure the transferability of pre-trained DNN parameters. Our experiments on twelve diverse image classification datasets get similar conclusions to the previous consensuses. More importantly, two new findings are presented, i.e., (1) in addition to the domain gap, a larger data amount and huge dataset diversity of downstream target task also prohibit the transferability; (2) although the lower layers learn basic image features, they are usually not the most transferable layers due to their domain sensitivity.",Article,https://arxiv.org/abs/2212.00399,1,chen2022rethinking,"deep neural networks,deep transfer learning,transferability"
2024,arXiv preprint arXiv:2407.08428,"A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights","Wentao Lei, Jinting Wang, Fengji Ma, Guanjie Huang, Li Liu",Wentao Lei,English,"Human video generation is a dynamic and rapidly evolving task that aims to synthesize 2D human body video sequences with generative models given control conditions such as text, audio, and pose. With the potential for wide-ranging applications in film, gaming, and virtual communication, the ability to generate natural and realistic human video is critical. Recent advancements in generative models have laid a solid foundation for the growing interest in this area. Despite the significant progress, the task of human video generation remains challenging due to the consistency of characters, the complexity of human motion, and difficulties in their relationship with the environment. This survey provides a comprehensive review of the current state of human video generation, marking, to the best of our knowledge, the first extensive literature review in this domain. We start with an introduction to the fundamentals of human video generation and the evolution of generative models that have facilitated the field's growth. We then examine the main methods employed for three key sub-tasks within human video generation: text-driven, audio-driven, and pose-driven motion generation. These areas are explored concerning the conditions that guide the generation process. Furthermore, we offer a collection of the most commonly utilized datasets and the evaluation metrics that are crucial in assessing the quality and realism of generated videos. The survey concludes with a discussion of the current challenges in the field and suggests possible directions for future research. The goal of this survey is to offer the research community a clear and holistic view of the?¡­",Conference paper,https://arxiv.org/abs/2407.08428,0,lei2024comprehensive,"Human video generation, Digital human, Virtual avatar, Diffusion model, Generative methods, Survey"
2024,arXiv preprint arXiv:2405.20291,Unveiling and Mitigating Backdoor Vulnerabilities based on Unlearning Weight Changes and Backdoor Activeness,"Weilin Lin, Li Liu, Shaokui Wei, Jianze Li, Hui Xiong",Weilin Lin,English,"The security threat of backdoor attacks is a central concern for deep neural networks (DNNs). Recently, without poisoned data, unlearning models with clean data and then learning a pruning mask have contributed to backdoor defense. Additionally, vanilla fine-tuning with those clean data can help recover the lost clean accuracy. However, the behavior of clean unlearning is still under-explored, and vanilla fine-tuning unintentionally induces back the backdoor effect. In this work, we first investigate model unlearning from the perspective of weight changes and gradient norms, and find two interesting observations in the backdoored model: 1) the weight changes between poison and clean unlearning are positively correlated, making it possible for us to identify the backdoored-related neurons without using poisoned data; 2) the neurons of the backdoored model are more active (i.e., larger changes in gradient norm) than those in the clean model, suggesting the need to suppress the gradient norm during fine-tuning. Then, we propose an effective two-stage defense method. In the first stage, an efficient Neuron Weight Change (NWC)-based Backdoor Reinitialization is proposed based on observation 1). In the second stage, based on observation 2), we design an Activeness-Aware Fine-Tuning to replace the vanilla fine-tuning. Extensive experiments, involving eight backdoor attacks on three benchmark datasets, demonstrate the superior performance of our proposed method compared to recent state-of-the-art backdoor defense approaches.",Article,https://arxiv.org/abs/2405.20291,0,lin2024unveiling,"deep neural networks,poisoned data,backdoor defense,cleaning,backdoored model, state-of-the-art"
2024,arXiv preprint arXiv:2405.17678,TIMA: Text-Image Mutual Awareness for Balancing Zero-Shot Adversarial Robustness and Generalization Ability,"Fengji Ma, Li Liu, Hei Victor Cheng",Fengji Ma,English,"This work addresses the challenge of achieving zero-shot adversarial robustness while preserving zero-shot generalization in large-scale foundation models, with a focus on the popular Contrastive Language-Image Pre-training (CLIP). Although foundation models were reported to have exceptional zero-shot generalization, they are highly vulnerable to adversarial perturbations. Existing methods achieve a comparable good tradeoff between zero-shot adversarial robustness and generalization under small adversarial perturbations. However, they fail to achieve a good tradeoff under large adversarial perturbations. To this end, we propose a novel Text-Image Mutual Awareness (TIMA) method that strikes a balance between zero-shot adversarial robustness and generalization. More precisely, we propose an Image-Aware Text (IAT) tuning mechanism that increases the inter-class distance of text embeddings by incorporating the Minimum Hyperspherical Energy (MHE). Simultaneously, fixed pre-trained image embeddings are used as cross-modal auxiliary supervision to maintain the similarity between the MHE-tuned and original text embeddings by the knowledge distillation, preserving semantic information between different classes. Besides, we introduce a Text-Aware Image (TAI) tuning mechanism, which increases inter-class distance between image embeddings during the training stage by Text-distance based Adaptive Margin (TAM). Similarly, a knowledge distillation is utilized to retain the similarity between fine-tuned and pre-trained image embeddings. Extensive experimental results demonstrate the effectiveness of our approach?¡­",Article,https://arxiv.org/abs/2405.17678,0,ma2024tima,"Adversarial Robustness, Multimodal Learning, Zero-Shot Learning, Vision-Language Models"
2024,ICASSP 2024-2024 IEEE International Conference on Acoustics,Leveraging Noisy Labels of Nearest Neighbors for Label Correction and Sample Selection,"Hua Jiang, Yixiong Chen, Li Liu, Xiaoguang Han, Xiao-Ping Zhang",Hua Jiang,English,"Dealing with noisy labels (LNL) emerges as a critical challenge when applying deep learning (DL) in practical settings. Previous methodologies primarily concentrated on harnessing model predictions to mitigate the impact of noisy labels. Nevertheless, their efficacy is strongly contingent on the accuracy of model predictions, a factor that cannot be assured in the context of LNL. Our empirical analysis shows that in noisy datasets, the spatial information of latent feature representation combined with original noisy labels is more robust than the methods using model predictions. To mitigate the unreliability introduced by model predictions, we propose a novel Feature Representation method, which utilizes noisy labels of nearest neighbors for label Correction and sample Selection (FRCS). Extensive experiments on various benchmark datasets demonstrate the superiority of FRCS compared with SOTA methods. Our?¡­",Conference paper,https://ieeexplore.ieee.org/abstract/document/10446280/,0,jiang2024leveraging,"Noisy label, Label correction, Sample selection, Feature Representation"
2024,arXiv preprint arXiv:2401.15002,BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning,"Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, Chao Shen",Baoyuan Wu,English,"As an emerging and vital topic for studying deep neural networks' vulnerability (DNNs), backdoor learning has attracted increasing interest in recent years, and many seminal backdoor attack and defense algorithms are being developed successively or concurrently, in the status of a rapid arms race. However, mainly due to the diverse settings, and the difficulties of implementation and reproducibility of existing works, there is a lack of a unified and standardized benchmark of backdoor learning, causing unfair comparisons, and unreliable conclusions (e.g., misleading, biased or even false conclusions). Consequently, it is difficult to evaluate the current progress and design the future development roadmap of this literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. Our benchmark makes three valuable contributions to the research community. 1) We provide an integrated implementation of state-of-the-art (SOTA) backdoor learning algorithms (currently including 16 attack and 27 defense algorithms), based on an extensible modular-based codebase. 2) We conduct comprehensive evaluations of 12 attacks against 16 defenses, with 5 poisoning ratios, based on 4 models and 4 datasets, thus 11,492 pairs of evaluations in total. 3) Based on above evaluations, we present abundant analysis from 8 perspectives via 18 useful analysis tools, and provide several inspiring insights about backdoor learning. We hope that our efforts could build a solid foundation of backdoor learning to facilitate researchers to investigate existing algorithms, develop more innovative algorithms, and explore?¡­",Article,https://arxiv.org/abs/2401.15002,0,wu2024backdoorbench,"Backdoor learning, backdoor attacks, backdoor defense, benchmark"
2024,arXiv preprint arXiv:2401.13578,WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition,"Zhengyao Song, Yongqiang Li, Danni Yuan, Li Liu, Shaokui Wei, Baoyuan Wu",Zhengyao Song,English,"This work explores an emerging security threat against deep neural networks (DNNs) based image classification, i.e., backdoor attack. In this scenario, the attacker aims to inject a backdoor into the model by manipulating training data, such that the backdoor could be activated by a particular trigger and bootstraps the model to make a target prediction at inference. Currently, most existing data poisoning-based attacks struggle to achieve success at low poisoning ratios, increasing the risk of being defended by defense methods. In this paper, we propose a novel frequency-based backdoor attack via Wavelet Packet Decomposition (WPD), WPD decomposes the original image signal to a spectrogram that contains frequency information with different semantic meanings. We leverage WPD to statistically analyze the frequency distribution of the dataset to infer the key frequency regions the DNNs would focus on, and the trigger information is only injected into the key frequency regions. Our method mainly includes three parts: 1) the selection of the poisoning frequency regions in spectrogram; 2) trigger generation; 3) the generation of the poisoned dataset. Our method is stealthy and precise, evidenced by the 98.12% Attack Success Rate (ASR) on CIFAR-10 with the extremely low poisoning ratio 0.004% (i.e., only 2 poisoned samples among 50,000 training samples) and can bypass most existing defense methods. Besides, we also provide visualization analyses to explain why our method works.",Article,https://arxiv.org/abs/2401.13578,0,song2024wpda,"Backdoor attack, wavelet packet transform, low poisoning ratios"
2023,arXiv preprint arXiv:2310.03363,Realistic Speech-to-Face Generation with Speech-Conditioned Latent Diffusion Model with Face Prior,"Jinting Wang, Li Liu, Jun Wang, Hei Victor Cheng",Jinting Wang,English,"Speech-to-face generation is an intriguing area of research that focuses on generating realistic facial images based on a speaker's audio speech. However, state-of-the-art methods employing GAN-based architectures lack stability and cannot generate realistic face images. To fill this gap, we propose a novel speech-to-face generation framework, which leverages a Speech-Conditioned Latent Diffusion Model, called SCLDM. To the best of our knowledge, this is the first work to harness the exceptional modeling capabilities of diffusion models for speech-to-face generation. Preserving the shared identity information between speech and face is crucial in generating realistic results. Therefore, we employ contrastive pre-training for both the speech encoder and the face encoder. This pre-training strategy facilitates effective alignment between the attributes of speech, such as age and gender, and the corresponding facial characteristics in the face images. Furthermore, we tackle the challenge posed by excessive diversity in the synthesis process caused by the diffusion model. To overcome this challenge, we introduce the concept of residuals by integrating a statistical face prior to the diffusion process. This addition helps to eliminate the shared component across the faces and enhances the subtle variations captured by the speech condition. Extensive quantitative, qualitative, and user study experiments demonstrate that our method can produce more realistic face images while preserving the identity of the speaker better than state-of-the-art methods. Highlighting the notable enhancements, our method demonstrates significant gains in all metrics on?¡­",Article,https://arxiv.org/abs/2310.03363,0,wang2023realistic,"Speech-to-face generation,state-of-the-art,GAN, diffusion model"
2022,ICONIP 2022,MVNet: Memory Assistance and Vocal Reinforcement Network for Speech Enhancement,"Jianrong Wang, Xiaomin Li, Xuewei Li, Yu Mei, Qiang Fang, Li Liu",Jianrong Wang,English,"Speech enhancement improves speech quality and promotes the performance of various downstream tasks. However, most current speech enhancement work was mainly devoted to improving the performance of downstream automatic speech recognition (ASR), only a relatively small amount of work focused on the automatic speaker verification (ASV) task. In this work, we propose a MVNet consisted of a memory assistance module which improves the performance of downstream ASR and a vocal reinforcement module to boosts the performance of ASV. In addition, we design a new loss function to improve speaker vocal similarity. Experimental results on the Libri2mix dataset show that our method outperforms baseline methods in several metrics, including speech quality, intelligibility, and speaker vocal similarity.",Article,https://link.springer.com/chapter/10.1007/978-3-031-30108-7_9,0,wang2022mvnet,"Speech enhancement, Complex network, Speaker similarity, Memory assistance, Vocal reinforcement"
