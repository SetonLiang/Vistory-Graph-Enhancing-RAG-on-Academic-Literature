Year,Sources,Name,Authors,First Author,Chinese/English,Abstract,Venues,doi,Citation,Id,Keywords
2021,International conference on machine learning,Delving into deep imbalanced regression,"Yuzhe Yang, Kaiwen Zha, Yingcong Chen, Hao Wang, Dina Katabi",Yuzhe Yang,English,"Real-world data often exhibit imbalanced distributions, where certain target values have significantly fewer observations. Existing techniques for dealing with imbalanced data focus on targets with categorical indices, ie, different classes. However, many tasks involve continuous targets, where hard boundaries between classes do not exist. We define Deep Imbalanced Regression (DIR) as learning from such imbalanced data with continuous targets, dealing with potential missing data for certain target values, and generalizing to the entire target range. Motivated by the intrinsic difference between categorical and continuous label space, we propose distribution smoothing for both labels and features, which explicitly acknowledges the effects of nearby targets, and calibrates both label and learned feature distributions. We curate and benchmark large-scale DIR datasets from common real-world tasks in computer vision, natural language processing, and healthcare domains. Extensive experiments verify the superior performance of our strategies. Our work fills the gap in benchmarks and techniques for practical imbalanced regression problems. Code and data are available at: https://github. com/YyzHarry/imbalanced-regression.",Conference paper,http://proceedings.mlr.press/v139/yang21m.html,270,yang2021delving,"deep imbalanced regression, benchmark, imbalanced data"
2020,Proceedings of the IEEE/CVF conference on computer vision and pattern?¡­,Spatial-temporal graph convolutional network for video-based person re-identification,"Jinrui Yang, Wei-Shi Zheng, Qize Yang, Ying-Cong Chen, Qi Tian",Jinrui Yang,English,"While video-based person re-identification (Re-ID) has drawn increasing attention and made great progress in recent years, it is still very challenging to effectively overcome the occlusion problem and the visual ambiguity problem for visually similar negative samples. On the other hand, we observe that different frames of a video can provide complementary information for each other, and the structural information of pedestrians can provide extra discriminative cues for appearance features. Thus, modeling the temporal relations of different frames and the spatial relations within a frame has the potential for solving the above problems. In this work, we propose a novel Spatial-Temporal Graph Convolutional Network (STGCN) to solve these problems. The STGCN includes two GCN branches, a spatial one and a temporal one. The spatial branch extracts structural information of a human body. The temporal branch mines discriminative cues from adjacent frames. By jointly optimizing these branches, our model extracts robust spatial-temporal information that is complementary with appearance information. As shown in the experiments, our model achieves state-of-the-art results on MARS and DukeMTMC-VideoReID datasets.",Conference paper,http://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Spatial-Temporal_Graph_Convolutional_Network_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.html,216,yang2020spatial,"person re-identification,graph convolutional network"
2022,Nature medicine,Artificial intelligence-enabled detection and assessment of Parkinson¡¯s disease using nocturnal breathing signals,"Yuzhe Yang, Yuan Yuan, Guo Zhang, Hao Wang, Ying-Cong Chen, Yingcheng Liu, Christopher G Tarolli, Daniel Crepeau, Jan Bukartyk, Mithri R Junna, Aleksandar Videnovic, Terry D Ellis, Melissa C Lipford, Ray Dorsey, Dina Katabi",Yuzhe Yang,English,"There are currently no effective biomarkers for diagnosing Parkinson¡¯s disease (PD) or tracking its progression. Here, we developed an artificial intelligence (AI) model to detect PD and track its progression from nocturnal breathing signals. The model was evaluated on a large dataset comprising 7,671 individuals, using data from several hospitals in the United States, as well as multiple public datasets. The AI model can detect PD with an area-under-the-curve of 0.90 and 0.85 on held-out and external test sets, respectively. The AI model can also estimate PD severity and progression in accordance with the Movement Disorder Society Unified Parkinson¡¯s Disease Rating Scale (R= 0.94, P= 3.6¡Á 10¨C25). The AI model uses an attention layer that allows for interpreting its predictions with respect to sleep and electroencephalogram. Moreover, the model can assess PD in the home setting in a touchless manner, by?¡­",Article,https://www.nature.com/articles/s41591-022-01932-x%3E).,135,yang2022artificial,None
2022,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?¡­,Representation compensation networks for continual semantic segmentation,"Chang-Bin Zhang, Jia-Wen Xiao, Xialei Liu, Ying-Cong Chen, Ming-Ming Cheng",Chang-Bin Zhang,English,"In this work, we study the continual semantic segmentation problem, where the deep neural networks are required to incorporate new classes continually without catastrophic forgetting. We propose to use a structural re-parameterization mechanism, named representation compensation (RC) module, to decouple the representation learning of both old and new knowledge. The RC module consists of two dynamically evolved branches with one frozen and one trainable. Besides, we design a pooled cube knowledge distillation strategy on both spatial and channel dimensions to further enhance the plasticity and stability of the model. We conduct experiments on two challenging continual semantic segmentation scenarios, continual class segmentation and continual domain segmentation. Without any extra computational overhead and parameters during inference, our method outperforms state-of-the-art performance. The code is available at https://github. com/zhangchbin/RCIL.",Conference paper,http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Representation_Compensation_Networks_for_Continual_Semantic_Segmentation_CVPR_2022_paper.html,82,zhang2022representation,"re-parameterization,knowledge distillation"
2020,Computer Vision¨CECCV 2020: 16th European Conference,Vcnet: A robust approach to blind image inpainting,"Yi Wang, Ying-Cong Chen, Xin Tao, Jiaya Jia",Yi Wang,English," Blind inpainting is a task to automatically complete visual contents without specifying masks for missing areas in an image. Previous work assumes known missing-region-pattern, limiting the application scope. We instead relax the assumption by defining a new blind inpainting setting, making training a neural system robust against various unknown missing region patterns. Specifically, we propose a two-stage visual consistency network (VCN) to estimate where to fill (via masks) and generate what to fill. In this procedure, the unavoidable potential mask prediction errors lead to severe artifacts in the subsequent repairing. To address it, our VCN predicts semantically inconsistent regions first, making mask prediction more tractable. Then it repairs these estimated missing regions using a new spatial normalization, making VCN robust to mask prediction errors. Semantically convincing and visually compelling?¡­",Conference paper,https://link.springer.com/chapter/10.1007/978-3-030-58595-2_45,79,wang2020vcnet,"blind image inpainting,PyTorch"
2021,Proceedings of the IEEE/CVF international conference on computer vision?¡­,Learning to know where to see: A visibility-aware approach for occluded person re-identification,"Jinrui Yang, Jiawei Zhang, Fufu Yu, Xinyang Jiang, Mengdan Zhang, Xing Sun, Ying-Cong Chen, Wei-Shi Zheng",Jinrui Yang,English,"Person re-identification (ReID) has gained an impressive progress in recent years. However, the occlusion is still a common and challenging problem for recent ReID methods. Several mainstream methods utilize extra cues (eg, human pose information) to distinguish human parts from obstacles to alleviate the occlusion problem. Although achieving inspiring progress, these methods severely rely on the fine-grained extra cues, and are sensitive to the estimation error in the extra cues. In this paper, we show that existing methods may degrade if the extra information is sparse or noisy. Thus we propose a simple yet effective method that is robust to sparse and noisy pose information. This is achieved by discretizing pose information to the visibility label of body parts, so as to suppress the influence of occluded regions. We show in our experiments that leveraging pose information in this way is more effective and robust. Besides, our method can be embedded into most person ReID models easily. Extensive experiments validate the effectiveness of our model on common occluded person ReID datasets.",Conference paper,http://openaccess.thecvf.com/content/ICCV2021/html/Yang_Learning_To_Know_Where_To_See_A_Visibility-Aware_Approach_for_ICCV_2021_paper.html,64,yang2021learning,Person Re-identification
2021,Proceedings of the IEEE/CVF International Conference on Computer Vision?¡­,Image synthesis via semantic composition,"Yi Wang, Lu Qi, Ying-Cong Chen, Xiangyu Zhang, Jiaya Jia",Yi Wang,English,"In this paper, we present a novel approach to synthesize realistic images based on their semantic layouts. It hypothesizes that for objects with similar appearance, they share similar representation. Our method establishes dependencies between regions according to their appearance correlation, yielding both spatially variant and associated representations. Conditioning on these features, we propose a dynamic weighted network constructed by spatially conditional computation (with both convolution and normalization). More than preserving semantic distinctions, the given dynamic network strengthens semantic relevance, benefiting global structure and detail synthesis. We demonstrate that our method gives the compelling generation performance qualitatively and quantitatively with extensive experiments on benchmarks.",Conference paper,http://openaccess.thecvf.com/content/ICCV2021/html/Wang_Image_Synthesis_via_Semantic_Composition_ICCV_2021_paper.html,58,wang2021image,"image synthesis,semantic composition"
2022,European conference on computer vision,RC-MVSNet: Unsupervised multi-view stereo with neural rendering,"Di Chang, Alja? Bo?i?, Tong Zhang, Qingsong Yan, Yingcong Chen, Sabine S¨¹sstrunk, Matthias Nie?ner",Di Chang,English,"Finding accurate correspondences among different views is the Achilles¡¯ heel of unsupervised Multi-View Stereo (MVS). Existing methods are built upon the assumption that corresponding pixels share similar photometric features. However, multi-view images in real scenarios observe non-Lambertian surfaces and experience occlusions. In this work, we propose a novel approach with neural rendering (RC-MVSNet) to solve such ambiguity issues of correspondences among views. Specifically, we impose a depth rendering consistency loss to constrain the geometry features close to the object surface to alleviate occlusions. Concurrently, we introduce a reference view synthesis loss to generate consistent supervision, even for non-Lambertian surfaces. Extensive experiments on DTU and Tanks &Temples benchmarks demonstrate that our RC-MVSNet approach achieves state-of-the-art performance over?¡­",Conference paper,https://link.springer.com/chapter/10.1007/978-3-031-19821-2_38,49,chang2022rc,"End-to-end Unsupervised Multi-View Stereo, Neural Rendering, Depth Estimation"
2020,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?¡­,Attentive normalization for conditional image generation,"Yi Wang, Ying-Cong Chen, Xiangyu Zhang, Jian Sun, Jiaya Jia",Yi Wang,English,"Traditional convolution-based generative adversarial networks synthesize images based on hierarchical local operations, where long-range dependency relation is implicitly modeled with a Markov chain. It is still not sufficient for categories with complicated structures. In this paper, we characterize long-range dependence with attentive normalization (AN), which is an extension to traditional instance normalization. Specifically, the input feature map is softly divided into several regions based on its internal semantic similarity, which are respectively normalized. It enhances consistency between distant regions with semantic correspondence. Compared with self-attention GAN, our attentive normalization does not need to measure the correlation of all locations, and thus can be directly applied to large-size feature maps without much computational burden. Extensive experiments on class-conditional image generation and semantic inpainting verify the efficacy of our proposed module.",Conference paper,http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Attentive_Normalization_for_Conditional_Image_Generation_CVPR_2020_paper.html,47,wang2020attentive,"generative adversarial networks, self-attention GAN,image generation"
2024,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?¡­,Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching,"Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, Yingcong Chen",Yixun Liang,English,The recent advancements in text-to-3D generation mark a significant milestone in generative models unlocking new possibilities for creating imaginative 3D assets across various real-world scenarios. While recent advancements in text-to-3D generation have shown promise they often fall short in rendering detailed and high-quality 3D models. This problem is especially prevalent as many methods base themselves on Score Distillation Sampling (SDS). This paper identifies a notable deficiency in SDS that it brings inconsistent and low-quality updating direction for the 3D model causing the over-smoothing effect. To address this we propose a novel approach called Interval Score Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes interval-based score matching to counteract over-smoothing. Furthermore we incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline. Extensive experiments show that our model largely outperforms the state-of-the-art in quality and training efficiency.,Conference paper,https://openaccess.thecvf.com/content/CVPR2024/html/Liang_LucidDreamer_Towards_High-Fidelity_Text-to-3D_Generation_via_Interval_Score_Matching_CVPR_2024_paper.html,45,liang2024luciddreamer," text-to-3D generation,Score Distillation Sampling"
2022,European Conference on Computer Vision,DecoupleNet: Decoupled network for domain adaptive semantic segmentation,"Xin Lai, Zhuotao Tian, Xiaogang Xu, Yingcong Chen, Shu Liu, Hengshuang Zhao, Liwei Wang, Jiaya Jia",Xin Lai,English,"Unsupervised domain adaptation in semantic segmentation alleviates the reliance on expensive pixel-wise annotation. It uses a labeled source domain dataset as well as unlabeled target domain images to learn a segmentation network. In this paper, we observe two main issues of existing domain-invariant learning framework. (1) Being distracted by the feature distribution alignment, the network cannot focus on the segmentation task. (2) Fitting source domain data well would compromise the target domain performance. To address these issues, we propose DecoupleNet to alleviate source domain overfitting and let the final model focus more on the segmentation task. Also, we put forward Self-Discrimination (SD) and introduce an auxiliary classifier to learn more discriminative target domain features with pseudo labels. Finally, we propose Online Enhanced Self-Training (OEST) to contextually enhance the quality?¡­",Conference paper,https://link.springer.com/chapter/10.1007/978-3-031-19827-4_22,45,lai2022decouplenet,"Unsupervised Domain Adaptation,Semantic Segmentation"
2021,IEEE Transactions on Pattern Analysis and Machine Intelligence,Pointins: Point-based instance segmentation,"Lu Qi, Yi Wang, Yukang Chen, Ying-Cong Chen, Xiangyu Zhang, Jian Sun, Jiaya Jia",Lu Qi,English,"In this paper, we explore the mask representation in instance segmentation with Point-of-Interest (PoI) features. Differentiating multiple potential instances within a single PoI feature is challenging, because learning a high-dimensional mask feature for each instance using vanilla convolution demands a heavy computing burden. To address this challenge, we propose an instance-aware convolution. It decomposes this mask representation learning task into two tractable modules as instance-aware weights and instance-agnostic features. The former is to parametrize convolution for producing mask features corresponding to different instances, improving mask learning efficiency by avoiding employing several independent convolutions. Meanwhile, the latter serves as mask templates in a single point. Together, instance-aware mask features are computed by convolving the template with dynamic weights, used for the?¡­",Article,https://ieeexplore.ieee.org/abstract/document/9444808/,34,qi2021pointins,"Instance Segmentation, Single-Point Feature"
2020,Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,Domain Adaptive Image-to-image Translation,"Ying-Cong Chen, Xiaogang Xu, Jiaya Jia",Ying-Cong Chen,English,"Unpaired image-to-image translation (I2I) has achieved great success in various applications. However, its generalization capacity is still an open question. In this paper, we show that existing I2I models do not generalize well for samples outside the training domain. The cause is twofold. First, an I2I model may not work well when testing samples are beyond its valid input domain. Second, results could be unreliable if the expected output is far from what the model is trained. To deal with these issues, we propose the Domain Adaptive Image-To-Image translation (DAI2I) framework that adapts an I2I model for out-of-domain samples. Our framework introduces two sub-modules--one maps testing samples to the valid input domain of the I2I model, and the other transforms the output of I2I model to expected results. Extensive experiments manifest that our framework improves the capacity of existing I2I models, allowing them to handle samples that are distinctively different from their primary targets.",Conference paper,http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Domain_Adaptive_Image-to-Image_Translation_CVPR_2020_paper.html,30,chen2020domain,image-to-image translation
2023,Proceedings of the IEEE/CVF International Conference on Computer Vision?¡­,Ref-neus: Ambiguity-reduced neural implicit surface learning for multi-view reconstruction with reflection,"Wenhang Ge, Tao Hu, Haoyu Zhao, Shu Liu, Ying-Cong Chen",Wenhang Ge,English,"Neural implicit surface learning has shown significant progress in multi-view 3D reconstruction, where an object is represented by multilayer perceptrons that provide continuous implicit surface representation and view-dependent radiance. However, current methods often fail to accurately reconstruct reflective surfaces, leading to severe ambiguity. To overcome this issue, we propose Ref-NeuS, which aims to reduce ambiguity by attenuating the effect of reflective surfaces. Specifically, we utilize an anomaly detector to estimate an explicit reflection score with the guidance of multi-view context to localize reflective surfaces. Afterward, we design a reflection-aware photometric loss that adaptively reduces ambiguity by modeling rendered color as a Gaussian distribution, with the reflection score representing the variance. We show that together with a reflection direction-dependent radiance, our model achieves high-quality surface reconstruction on reflective surfaces and outperforms the state-of-the-arts by a large margin. Besides, our model is also comparable on general surfaces.",Conference paper,http://openaccess.thecvf.com/content/ICCV2023/html/Ge_Ref-NeuS_Ambiguity-Reduced_Neural_Implicit_Surface_Learning_for_Multi-View_Reconstruction_with_ICCV_2023_paper.html,24,ge2023ref,None
2023,Proceedings of the IEEE/CVF conference on computer vision and pattern?¡­,Neuron structure modeling for generalizable remote physiological measurement,"Hao Lu, Zitong Yu, Xuesong Niu, Ying-Cong Chen",Hao Lu,English,"Remote photoplethysmography (rPPG) technology has drawn increasing attention in recent years. It can extract Blood Volume Pulse (BVP) from facial videos, making many applications like health monitoring and emotional analysis more accessible. However, as the BVP signal is easily affected by environmental changes, existing methods struggle to generalize well for unseen domains. In this paper, we systematically address the domain shift problem in the rPPG measurement task. We show that most domain generalization methods do not work well in this problem, as domain labels are ambiguous in complicated environmental changes. In light of this, we propose a domain-label-free approach called NEuron STructure modeling (NEST). NEST improves the generalization capacity by maximizing the coverage of feature space during training, which reduces the chance for under-optimized feature activation during inference. Besides, NEST can also enrich and enhance domain invariant features across multi-domain. We create and benchmark a large-scale domain generalization protocol for the rPPG measurement task. Extensive experiments show that our approach outperforms the state-of-the-art methods on both cross-dataset and intra-dataset settings.",Conference paper,http://openaccess.thecvf.com/content/CVPR2023/html/Lu_Neuron_Structure_Modeling_for_Generalizable_Remote_Physiological_Measurement_CVPR_2023_paper.html,24,lu2023neuron,"Remote photoplethysmography (rPPG),Blood Volume Pulse (BVP),benchmark"
2024,arXiv preprint arXiv:2405.08816,The robodrive challenge: Drive anytime anywhere in any condition,"Lingdong Kong, Shaoyuan Xie, Hanjiang Hu, Yaru Niu, Wei Tsang Ooi, Benoit R Cottereau, Lai Xing Ng, Yuexin Ma, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu, Weichao Qiu, Wei Zhang, Xu Cao, Hao Lu, Ying-Cong Chen, Caixin Kang, Xinning Zhou, Chengyang Ying, Wentao Shang, Xingxing Wei, Yinpeng Dong, Bo Yang, Shengyin Jiang, Zeliang Ma, Dengyi Ji, Haiwen Li, Xingliang Huang, Yu Tian, Genghua Kou, Fan Jia, Yingfei Liu, Tiancai Wang, Ying Li, Xiaoshuai Hao, Yifan Yang, Hui Zhang, Mengchuan Wei, Yi Zhou, Haimei Zhao, Jing Zhang, Jinke Li, Xiao He, Xiaoqiang Cheng, Bingyang Zhang, Lirong Zhao, Dianlei Ding, Fangsheng Liu, Yixiang Yan, Hongming Wang, Nanfei Ye, Lun Luo, Yubo Tian, Yiwei Zuo, Zhe Cao, Yi Ren, Yunfan Li, Wenjie Liu, Xun Wu, Yifan Mao, Ming Li, Jian Liu, Jiayang Liu, Zihan Qin, Cunxi Chu, Jialei Xu, Wenbo Zhao, Junjun Jiang, Xianming Liu, Ziyan Wang, Chiwei Li, Shilong Li, Chendong Yuan, Songyue Yang, Wentao Liu, Peng Chen, Bin Zhou, Yubo Wang, Chi Zhang, Jianhang Sun, Hai Chen, Xiao Yang, Lizhong Wang, Dongyi Fu, Yongchun Lin, Huitong Yang, Haoang Li, Yadan Luo, Xianjing Cheng, Yong Xu",Lingdong Kong,English,"In the realm of autonomous driving, robust perception under out-of-distribution conditions is paramount for the safe deployment of vehicles. Challenges such as adverse weather, sensor malfunctions, and environmental unpredictability can severely impact the performance of autonomous systems. The 2024 RoboDrive Challenge was crafted to propel the development of driving perception technologies that can withstand and adapt to these real-world variabilities. Focusing on four pivotal tasks -- BEV detection, map segmentation, semantic occupancy prediction, and multi-view depth estimation -- the competition laid down a gauntlet to innovate and enhance system resilience against typical and atypical disturbances. This year's challenge consisted of five distinct tracks and attracted 140 registered teams from 93 institutes across 11 countries, resulting in nearly one thousand submissions evaluated through our servers. The competition culminated in 15 top-performing solutions, which introduced a range of innovative approaches including advanced data augmentation, multi-sensor fusion, self-supervised learning for error correction, and new algorithmic strategies to enhance sensor robustness. These contributions significantly advanced the state of the art, particularly in handling sensor inconsistencies and environmental variability. Participants, through collaborative efforts, pushed the boundaries of current technologies, showcasing their potential in real-world scenarios. Extensive evaluations and analyses provided insights into the effectiveness of these solutions, highlighting key trends and successful strategies for improving the resilience of?¡­",Article,https://arxiv.org/abs/2405.08816,16,kong2024robodrive,"autonomous driving,robust perception,driving perception"
2021,IEEE Transactions on Pattern Analysis and Machine Intelligence,Text-guided human image manipulation via image-text shared space,"Xiaogang Xu, Ying-Cong Chen, Xin Tao, Jiaya Jia",Xiaogang Xu,English,"Text is a new way to guide human image manipulation. Albeit natural and flexible, text usually suffers from inaccuracy in spatial description, ambiguity in the description of appearance, and incompleteness. We in this paper address these issues. To overcome inaccuracy, we use structured information (e.g., poses) to help identify correct location to manipulate, by disentangling the control of appearance and spatial structure. Moreover, we learn the image-text shared space with derived disentanglement to improve accuracy and quality of manipulation, by separating relevant and irrelevant editing directions for the textual instructions in this space. Our model generates a series of manipulation results by moving source images in this space with different degrees of editing strength. Thus, to reduce the ambiguity in text, our model generates sequential output for manual selection. In addition, we propose an efficient?¡­",Article,https://ieeexplore.ieee.org/abstract/document/9444850/,12,xu2021text,"Human image manipulation, adversarial generative networks, image and text"
2022,European Conference on Computer Vision,Semi-supervised monocular 3d object detection by multi-view consistency,"Qing Lian, Yanbo Xu, Weilong Yao, Yingcong Chen, Tong Zhang",Qing Lian,English,"The success of monocular 3D object detection highly relies on considerable labeled data, which is costly to obtain. To alleviate the annotation effort, we propose MVC-MonoDet, the first semi-supervised training framework that improves Monocular 3D object detection by enforcing multi-view consistency. In particular, a box-level regularization and an object-level regularization are designed to enforce the consistency of 3D bounding box predictions of the detection model across unlabeled multi-view data (stereo or video). The box-level regularizer requires the model to consistently estimate 3D boxes in different views so that the model can learn cross-view invariant features for 3D detection. The object-level regularizer employs an object-wise photometric consistency loss that mitigates 3D box estimation error through structure-from-motion (SFM). A key innovation in our approach to effectively utilize these consistency?¡­",Conference paper,https://link.springer.com/chapter/10.1007/978-3-031-20074-8_41,11,lian2022semi,"Monocular 3D Object Detection, Semi-supervised Training, Structure From Motion"
2023,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?¡­,Lift3d: Synthesize 3d training data by lifting 2d gan to 3d generative radiance field,"Leheng Li, Qing Lian, Luozhou Wang, Ningning Ma, Ying-Cong Chen",Leheng Li,English,"This work explores the use of 3D generative models to synthesize training data for 3D vision tasks. The key requirements of the generative models are that the generated data should be photorealistic to match the real-world scenarios, and the corresponding 3D attributes should be aligned with given sampling labels. However, we find that the recent NeRF-based 3D GANs hardly meet the above requirements due to their designed generation pipeline and the lack of explicit 3D supervision. In this work, we propose Lift3D, an inverted 2D-to-3D generation framework to achieve the data generation objectives. Lift3D has several merits compared to prior methods:(1) Unlike previous 3D GANs that the output resolution is fixed after training, Lift3D can generalize to any camera intrinsic with higher resolution and photorealistic output.(2) By lifting well-disentangled 2D GAN to 3D object NeRF, Lift3D provides explicit 3D information of generated objects, thus offering accurate 3D annotations for downstream tasks. We evaluate the effectiveness of our framework by augmenting autonomous driving datasets. Experimental results demonstrate that our data generation framework can effectively improve the performance of 3D object detectors. Code: len-li. github. io/lift3d-web",Conference paper,http://openaccess.thecvf.com/content/CVPR2023/html/Li_Lift3D_Synthesize_3D_Training_Data_by_Lifting_2D_GAN_to_CVPR_2023_paper.html,9,li2023lift3d,"3D GANs,Neural Radiance Fields, 2D-to-3D generation"
2023,Proceedings of the 31st ACM International Conference on Multimedia,Resolve domain conflicts for generalizable remote physiological measurement,"Weiyu Sun, Xinyu Zhang, Hao Lu, Ying Chen, Yun Ge, Xiaolin Huang, Jie Yuan, Yingcong Chen",Weiyu Sun,English,"Remote photoplethysmography (rPPG) technology has become increasingly popular due to its non-invasive monitoring of various physiological indicators, making it widely applicable in multimedia interaction, healthcare, and emotion analysis. Existing rPPG methods utilize multiple datasets for training to enhance the generalizability of models. However, they often overlook the underlying conflict issues in the rPPG field, such as (1) label conflict resulting from different phase delays between physiological signal labels and face videos at the instance level, and (2) attribute conflict stemming from distribution shifts caused by head movements, illumination changes, skin types, etc. To address this, we introduce the DOmain-HArmonious framework (DOHA). Specifically, we first propose a harmonious phase strategy to eliminate uncertain phase delays and preserve the temporal variation of physiological signals. Next, we?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3581783.3612265,5,sun2023resolve,"physiological signal estimation, rPPG, multimedia application,semantic segmentation"
2023,arXiv preprint arXiv:2310.03337,Denoising diffusion step-aware models,"Shuai Yang, Yukang Chen, Luozhou Wang, Shu Liu, Yingcong Chen",Shuai Yang,English,"Denoising Diffusion Probabilistic Models (DDPMs) have garnered popularity for data generation across various domains. However, a significant bottleneck is the necessity for whole-network computation during every step of the generative process, leading to high computational overheads. This paper presents a novel framework, Denoising Diffusion Step-aware Models (DDSM), to address this challenge. Unlike conventional approaches, DDSM employs a spectrum of neural networks whose sizes are adapted according to the importance of each generative step, as determined through evolutionary search. This step-wise network variation effectively circumvents redundant computational efforts, particularly in less critical steps, thereby enhancing the efficiency of the diffusion model. Furthermore, the step-aware design can be seamlessly integrated with other efficiency-geared diffusion models such as DDIMs and latent diffusion, thus broadening the scope of computational savings. Empirical evaluations demonstrate that DDSM achieves computational savings of 49% for CIFAR-10, 61% for CelebA-HQ, 59% for LSUN-bedroom, 71% for AFHQ, and 76% for ImageNet, all without compromising the generation quality. Our code and models will be publicly available.",Article,https://arxiv.org/abs/2310.03337,5,yang2023denoising,"Denoising Diffusion Step-aware Models,diffusion model"
2023,Proceedings of the IEEE/CVF International Conference on Computer Vision?¡­,Not all steps are created equal: Selective diffusion distillation for image manipulation,"Luozhou Wang, Shuai Yang, Shu Liu, Ying-cong Chen",Luozhou Wang,English,"Conditional diffusion models have demonstrated impressive performance in image manipulation tasks. The general pipeline involves adding noise to the image and then denoising it. However, this method faces a trade-off problem: adding too much noise affects the fidelity of the image while adding too little affects its editability. This largely limits their practical applicability. In this paper, we propose a novel framework, Selective Diffusion Distillation (SDD), that ensures both the fidelity and editability of images. Instead of directly editing images with a diffusion model, we train a feedforward image manipulation network under the guidance of the diffusion model. Besides, we propose an effective indicator to select the semantic-related timestep to obtain the correct semantic guidance from the diffusion model. This approach successfully avoids the dilemma caused by the diffusion process. Our extensive experiments demonstrate the advantages of our framework.",Conference paper,http://openaccess.thecvf.com/content/ICCV2023/html/Wang_Not_All_Steps_are_Created_Equal_Selective_Diffusion_Distillation_for_ICCV_2023_paper.html,5,wang2023not,"diffusion model,image manipulation"
2023,arXiv preprint arXiv:2309.01351,Adv3D: generating 3D adversarial examples in driving scenarios with nerf,"Leheng Li, Qing Lian, Ying-Cong Chen",Leheng Li,English,"Deep neural networks (DNNs) have been proven extremely susceptible to adversarial examples, which raises special safety-critical concerns for DNN-based autonomous driving stacks (i.e., 3D object detection). Although there are extensive works on image-level attacks, most are restricted to 2D pixel spaces, and such attacks are not always physically realistic in our 3D world. Here we present Adv3D, the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic appearances and 3D accurate generation, yielding a more realistic and realizable adversarial example. We train our adversarial NeRF by minimizing the surrounding objects' confidence predicted by 3D detectors on the training set. Then we evaluate Adv3D on the unseen validation set and show that it can cause a large performance reduction when rendering NeRF in any sampled pose. To generate physically realizable adversarial examples, we propose primitive-aware sampling and semantic-guided regularization that enable 3D patch attacks with camouflage adversarial texture. Experimental results demonstrate that the trained adversarial NeRF generalizes well to different poses, scenes, and 3D detectors. Finally, we provide a defense method to our attacks that involves adversarial training through data augmentation. Project page: https://len-li.github.io/adv3d-web",Article,https://arxiv.org/abs/2309.01351,4,li2023adv3d,"deep neural networks,Neural Radiance Fields,3D adversarial"
2023,,Dual-balancing for multi-task learning,"Baijiong Lin, Weisen Jiang, Feiyang Ye, Yu Zhang, Pengguang Chen, Ying-Cong Chen, Shu Liu, James Kwok",Baijiong Lin,English,"Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task balancing problem remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Dual-Balancing Multi-Task Learning (DB-MTL) method to alleviate the task balancing problem from both loss and gradient perspectives. Specifically, DB-MTL ensures loss-scale balancing by performing a logarithm transformation on each task loss, and guarantees gradient-magnitude balancing via normalizing all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the state-of-the-art performance of DB-MTL.",Conference paper,https://openreview.net/forum?id=8FhwHJGUPZ,4,lin2023dual,"Multi-Task Learning,Dual-Balancing Multi-Task Learning"
2023,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?¡­,Real-time 6K Image Rescaling with Rate-distortion Optimization,"Chenyang Qi, Xin Yang, Ka Leong Cheng, Ying-Cong Chen, Qifeng Chen",Chenyang Qi,English,"The task of image rescaling aims at embedding an high-resolution (HR) image into a low-resolution (LR) one that can contain embedded information for HR image reconstruction. Existing image rescaling methods do not optimize the LR image file size and recent flow-based rescaling methods are not real-time yet for HR image reconstruction (eg, 6K). To address these two challenges, we propose a novel framework (HyperThumbnail) for real-time 6K rate-distortion-aware image rescaling. Our HyperThumbnail first embeds an HR image into a JPEG LR image (thumbnail) by an encoder with our proposed learnable JPEG quantization module, which optimizes the file size of the embedding LR JPEG image. Then, an efficient decoder reconstructs a high-fidelity HR (6K) image from the LR one in real time. Extensive experiments demonstrate that our framework outperforms previous image rescaling baselines in both rate-distortion performance and is much faster than prior work in HR image reconstruction speed.",Conference paper,http://openaccess.thecvf.com/content/CVPR2023/html/Qi_Real-Time_6K_Image_Rescaling_With_Rate-Distortion_Optimization_CVPR_2023_paper.html,4,qi2023real,"image reconstruction,rate-distortion,real-time image rescaling"
2023,IEEE Journal of Biomedical and Health Informatics,Hierarchical Style-Aware Domain Generalization for Remote Physiological Measurement,"Jiyao Wang, Hao Lu, Ange Wang, Yingcong Chen, Dengbo He",Jiyao Wang,English,"The utilization of remote photoplethysmography (rPPG) technology has gained attention in recent years due to its ability to extract blood volume pulse (BVP) from facial videos, making it accessible for various applications such as health monitoring and emotional analysis. However, the BVP signal is susceptible to complex environmental changes or individual differences, causing existing methods to struggle in generalizing for unseen domains. This article addresses the domain shift problem in rPPG measurement and shows that most domain generalization methods fail to work well in this problem due to ambiguous instance-specific differences. To address this, the article proposes a novel approach called Hierarchical Style-aware Representation Disentangling (HSRD). HSRD improves generalization capacity by separating domain-invariant and instance-specific feature space during training, which increases the?¡­",Article,https://ieeexplore.ieee.org/abstract/document/10371379/,3,wang2023hierarchical,"Adversarial learning, contrastive learning, domain generalization, heart rate estimation, remote photoplethysmography (rPPG)"
2023,IEEE Transactions on Image Processing,Adaptive domain generalization via online disagreement minimization,"Xin Zhang, Ying-Cong Chen",Xin Zhang,English,"Deep neural networks suffer from significant performance deterioration when there exists distribution shift between deployment and training. Domain Generalization (DG) aims to safely transfer a model to unseen target domains by only relying on a set of source domains. Although various DG approaches have been proposed, a recent study named DomainBed (Gulrajani and Lopez-Paz, 2020), reveals that most of them do not beat simple empirical risk minimization (ERM). To this end, we propose a general framework that is orthogonal to existing DG algorithms and could improve their performance consistently. Unlike previous DG works that stake on a static source model to be hopefully a universal one, our proposed AdaODM adaptively modifies the source model at test time for different target domains. Specifically, we create multiple domain-specific classifiers upon a shared domain-generic feature extractor. The?¡­",Article,https://ieeexplore.ieee.org/abstract/document/10188592/,3,zhang2023adaptive,"Domain shift, domain generalization, online adaptation, consistency regularization"
2023,arXiv preprint arXiv:2303.10585,Label name is mantra: Unifying point cloud segmentation across heterogeneous datasets,"Yixun Liang, Hao He, Shishi Xiao, Hao Lu, Yingcong Chen",Yixun Liang,English,"Point cloud segmentation is a fundamental task in 3D vision that serves a wide range of applications. Although great progresses have been made these years, its practical usability is still limited by the availability of training data. Existing approaches cannot make full use of multiple datasets on hand due to the label mismatch among different datasets. In this paper, we propose a principled approach that supports learning from heterogeneous datasets with different label sets. Our idea is to utilize a pre-trained language model to embed discrete labels to a continuous latent space with the help of their label names. This unifies all labels of different datasets, so that joint training is doable. Meanwhile, classifying points in the continuous 3D space by their vocabulary tokens significantly increase the generalization ability of the model in comparison with existing approaches that have fixed decoder architecture. Besides, we also integrate prompt learning in our framework to alleviate data shifts among different data sources. Extensive experiments demonstrate that our model outperforms the state-of-the-art by a large margin.",Article,https://arxiv.org/abs/2303.10585,3,liang2023label,"point cloud segmentation,pre-trained language model,3D space"
2024,Advances in Neural Information Processing Systems 36,Retr: Modeling rendering via transformer for generalizable neural surface reconstruction,"Yixun Liang, Hao He, Yingcong Chen",Yixun Liang,English,"Generalizable neural surface reconstruction techniques have attracted great attention in recent years. However, they encounter limitations of low confidence depth distribution and inaccurate surface reasoning due to the oversimplified volume rendering process employed. In this paper, we present Reconstruction TRansformer (ReTR), a novel framework that leverages the transformer architecture to redesign the rendering process, enabling complex render interaction modeling. It introduces a learnable $\textit {meta-ray token} $ and utilizes the cross-attention mechanism to simulate the interaction of rendering process with sampled points and render the observed color. Meanwhile, by operating within a high-dimensional feature space rather than the color space, ReTR mitigates sensitivity to projected colors in source views. Such improvements result in accurate surface assessment with high confidence. We demonstrate the effectiveness of our approach on various datasets, showcasing how our method outperforms the current state-of-the-art approaches in terms of reconstruction quality and generalization ability. $\textit {Our code is available at} $ https://github. com/YixunLiang/ReTR.",Article,https://proceedings.neurips.cc/paper_files/paper/2023/hash/c47ec10bc135be5c3663ba344d29a6a5-Abstract-Conference.html,2,liang2024retr,"neural surface reconstruction,Transformer"
2024,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?¡­,Gpt as psychologist? preliminary evaluations for gpt-4v on visual affective computing,"Hao Lu, Xuesong Niu, Jiyao Wang, Yin Wang, Qingyong Hu, Jiaqi Tang, Yuting Zhang, Kaishen Yuan, Bin Huang, Zitong Yu, Dengbo He, Shuiguang Deng, Hao Chen, Yingcong Chen, Shiguang Shan",Hao Lu,English,Multimodal large language models (MLLMs) are designed to process and integrate information from multiple sources such as text speech images and videos. Despite its success in language understanding it is critical to evaluate the performance of downstream tasks for better human-centric applications. This paper assesses the application of MLLMs with 5 crucial abilities for affective computing spanning from visual affective tasks and reasoning tasks. The results show that\gpt has high accuracy in facial action unit recognition and micro-expression detection while its general facial expression recognition performance is not accurate. We also highlight the challenges of achieving fine-grained micro-expression recognition and the potential for further study and demonstrate the versatility and potential of\gpt for handling advanced tasks in emotion recognition and related fields by integrating with task-related agents for more complex tasks such as heart rate estimation through signal processing. In conclusion this paper provides valuable insights into the potential applications and challenges of MLLMs in human-centric computing.,Conference paper,https://openaccess.thecvf.com/content/CVPR2024W/CVPM/html/Lu_GPT_as_Psychologist___Preliminary_Evaluations_for_GPT-4V_on_CVPRW_2024_paper.html,2,lu2024gpt,"Mulimodal Large Language Model,GPT-4V,Affective Computing,Generative Pre-trained Transformer, Facial Expression Recognition"
2023,arXiv preprint arXiv:2310.11346,Towards generalizable multi-camera 3d object detection via perspective debiasing,"Hao Lu, Yunpeng Zhang, Qing Lian, Dalong Du, Yingcong Chen",Hao Lu,English,"Detecting objects in 3D space using multiple cameras, known as Multi-Camera 3D Object Detection (MC3D-Det), has gained prominence with the advent of bird's-eye view (BEV) approaches. However, these methods often struggle when faced with unfamiliar testing environments due to the lack of diverse training data encompassing various viewpoints and environments. To address this, we propose a novel method that aligns 3D detection with 2D camera plane results, ensuring consistent and accurate detections. Our framework, anchored in perspective debiasing, helps the learning of features resilient to domain shifts. In our approach, we render diverse view maps from BEV features and rectify the perspective bias of these maps, leveraging implicit foreground volumes to bridge the camera and BEV planes. This two-step process promotes the learning of perspective- and context-independent features, crucial for accurate object detection across varying viewpoints, camera parameters and environment conditions. Notably, our model-agnostic approach preserves the original network structure without incurring additional inference costs, facilitating seamless integration across various models and simplifying deployment. Furthermore, we also show our approach achieves satisfactory results in real data when trained only with virtual datasets, eliminating the need for real scene annotations. Experimental results on both Domain Generalization (DG) and Unsupervised Domain Adaptation (UDA) clearly demonstrate its effectiveness. Our code will be released.",Article,https://arxiv.org/abs/2310.11346,3,lu2023towards,"Multi-Camera 3D Object Detection,perspective debiasing,bird¡¯s-eye view"
2023,arXiv preprint arXiv:2306.14408,Decompose and realign: Tackling condition misalignment in text-to-image diffusion models,"Luozhou Wang, Guibao Shen, Wenhang Ge, Guangyong Chen, Yijun Li, Ying-cong Chen",Luozhou Wang,English,"Text-to-image diffusion models have advanced towards more controllable generation via supporting various additional conditions (e.g., depth map, bounding box) beyond text. However, these models are learned based on the premise of perfect alignment between the text and extra conditions. If this alignment is not satisfied, the final output could be either dominated by one condition, or ambiguity may arise, failing to meet user expectations.To address this issue, we present a training-free approach called ``Decompose and Realign'' to further improve the controllability of existing models when provided with partially aligned conditions. The ``Decompose'' phase separates conditions based on pair relationships, computing the result individually for each pair. This ensures that each pair no longer has conflicting conditions. The ``Realign'' phase aligns these independently calculated results via a cross-attention mechanism to avoid new conflicts when combining them back. Both qualitative and quantitative results demonstrate the effectiveness of our approach in handling unaligned conditions, which performs favorably against recent methods and more importantly adds flexibility to the controllable image generation process. Our code will be available at: https://github.com/EnVision-Research/Decompose-and-Realign.",Article,https://arxiv.org/abs/2306.14408,2,wang2023decompose,"Controllable Image Generation,Condition Misalignment"
2023,arXiv preprint arXiv:2304.08738,Addressing variable dependency in gnn-based SAT solving,"Zhiyuan Yan, Min Li, Zhengyuan Shi, Wenjie Zhang, Yingcong Chen, Hongce Zhang",Zhiyuan Yan,English,"Boolean satisfiability problem (SAT) is fundamental to many applications. Existing works have used graph neural networks (GNNs) for (approximate) SAT solving. Typical GNN-based end-to-end SAT solvers predict SAT solutions concurrently. We show that for a group of symmetric SAT problems, the concurrent prediction is guaranteed to produce a wrong answer because it neglects the dependency among Boolean variables in SAT problems. % We propose AsymSAT, a GNN-based architecture which integrates recurrent neural networks to generate dependent predictions for variable assignments. The experiment results show that dependent variable prediction extends the solving capability of the GNN-based method as it improves the number of solved SAT instances on large test sets.",Article,https://arxiv.org/abs/2304.08738,2,yan2023addressing,"graph neural networks,Boolean satisfiability problem"
2022,arXiv preprint arXiv:2212.03357,Contactless oxygen monitoring with gated transformer,"Hao He, Yuan Yuan, Ying-Cong Chen, Peng Cao, Dina Katabi",Hao He,English,"With the increasing popularity of telehealth, it becomes critical to ensure that basic physiological signals can be monitored accurately at home, with minimal patient overhead. In this paper, we propose a contactless approach for monitoring patients' blood oxygen at home, simply by analyzing the radio signals in the room, without any wearable devices. We extract the patients' respiration from the radio signals that bounce off their bodies and devise a novel neural network that infers a patient's oxygen estimates from their breathing signal. Our model, called \emph{Gated BERT-UNet}, is designed to adapt to the patient's medical indices (e.g., gender, sleep stages). It has multiple predictive heads and selects the most suitable head via a gate controlled by the person's physiological indices. Extensive empirical results show that our model achieves high accuracy on both medical and radio datasets.",Article,https://arxiv.org/abs/2212.03357,2,he2022contactless,"Oxygen monitoring,BERT-UNet model"
2024,IEEE Transactions on Multimedia,Self-similarity Prior Distillation for Unsupervised Remote Physiological Measurement,"Xinyu Zhang, Weiyu Sun, Hao Lu, Ying Chen, Yun Ge, Xiaolin Huang, Jie Yuan, Yingcong Chen",Xinyu Zhang,English,"Remote photoplethysmography (rPPG) is a non-invasive technique that aims to capture subtle variations in facial pixels caused by changes in blood volume resulting from cardiac activities. Most existing unsupervised methods for rPPG tasks focus on the contrastive learning between samples while neglecting the inherent self-similarity prior in physiological signals. In this paper, we propose a Self-Similarity Prior Distillation (SSPD) framework for unsupervised rPPG estimation, which capitalizes on the intrinsic temporal self-similarity of cardiac activities. Specifically, we first introduce a physical-prior embedded augmentation technique to mitigate the effect of various types of noise. Then, we tailor a self-similarity-aware network to disentangle more reliable self-similar physiological features. Finally, we develop a hierarchical self-distillation paradigm for self-similarity-aware learning and rPPG signal decoupling?¡­",Article,https://ieeexplore.ieee.org/abstract/document/10542453/,1,zhang2024self,"Remote Photoplethysmography,Multimedia Applications,Self-similarity,Unsupervised Learning,Selfdistillation"
2024,arXiv preprint arXiv:2405.17104,LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding,"Haoyu Zhao, Wenhang Ge, Ying-cong Chen",Haoyu Zhao,English,"Visual grounding is an essential tool that links user-provided text queries with query-specific regions within an image. Despite advancements in visual grounding models, their ability to comprehend complex queries remains limited. To overcome this limitation, we introduce LLM-Optic, an innovative method that utilizes Large Language Models (LLMs) as an optical lens to enhance existing visual grounding models in comprehending complex text queries involving intricate text structures, multiple objects, or object spatial relationships, situations that current models struggle with. LLM-Optic first employs an LLM as a Text Grounder to interpret complex text queries and accurately identify objects the user intends to locate. Then a pre-trained visual grounding model is used to generate candidate bounding boxes given the refined query by the Text Grounder. After that, LLM-Optic annotates the candidate bounding boxes with numerical marks to establish a connection between text and specific image regions, thereby linking two distinct modalities. Finally, it employs a Large Multimodal Model (LMM) as a Visual Grounder to select the marked candidate objects that best correspond to the original text query. Through LLM-Optic, we have achieved universal visual grounding, which allows for the detection of arbitrary objects specified by arbitrary human language input. Importantly, our method achieves this enhancement without requiring additional training or fine-tuning. Extensive experiments across various challenging benchmarks demonstrate that LLM-Optic achieves state-of-the-art zero-shot visual grounding capabilities.",Article,https://arxiv.org/abs/2405.17104,1,zhao2024llm,"large language models,visual grounding,optic, large multimodal model"
2023,Machine Learning for Healthcare Conference,Contactless Oxygen Monitoring with Radio Waves and Gated Transformer,"Hao He, Yuan Yuan, Ying-Cong Chen, Peng Cao, Dina Katabi",Hao He,English,"With the increasing popularity of telehealth, it is crucial to ensure accurate monitoring of basic physiological signals at home with minimal patient overhead. In this paper, we propose a contactless approach for monitoring blood oxygen levels simply by analyzing radio signals in a patient¡¯s room, without the need for wearable devices. Our method extracts a patient¡¯s respiration from radio signals that bounce off their body, and we use a novel neural network, called Gated BERT-UNet, to estimate blood oxygen saturation from the breathing signal. We designed our model to adapt to a patient¡¯s medical indices, such as gender and sleep stages, to provide personalized inference. Specifically, it uses multiple predictive heads, controlled by a gate, to make predictions for different sub-populations. Our extensive empirical results demonstrate that our model achieves high accuracy on both medical and radio-frequency datasets. It outperforms past work on contactless oxygen monitoring, reducing the mean absolute error in oxygen saturation from 2.0% to 1.3%.",Conference paper,https://proceedings.mlr.press/v219/he23a.html,1,he2023contactless,"Oxygen monitoring,BERT-UNet model"
2023,Proceedings of the IEEE/CVF International Conference on Computer Vision?¡­,Out-of-domain GAN inversion via Invertibility Decomposition for Photo-Realistic Human Face Manipulation,"Xin Yang, Xiaogang Xu, Yingcong Chen",Xin Yang,English,"The fidelity of Generative Adversarial Networks (GAN) inversion is impeded by Out-Of-Domain (OOD) areas (eg, background, accessories) in the image. Detecting the OOD areas beyond the generation ability of the pre-trained model and blending these regions with the input image can enhance fidelity. The"" invertibility mask"" figures out these OOD areas, and existing methods predict the mask with the reconstruction error. However, the estimated mask is usually inaccurate due to the influence of the reconstruction error in the In-Domain (ID) area. In this paper, we propose a novel framework that enhances the fidelity of human face inversion by designing a new module to decompose the input images to ID and OOD partitions with invertibility masks. Unlike previous works, our invertibility detector is simultaneously learned with a spatial alignment module. We iteratively align the generated features to the input geometry and reduce the reconstruction error in the ID regions. Thus, the OOD areas are more distinguishable and can be precisely predicted. Then, we improve the fidelity of our results by blending the OOD areas from the input image with the ID GAN inversion results. Our method produces photo-realistic results for real-world human face image inversion and manipulation. Extensive experiments demonstrate our method's superiority over existing methods in the quality of GAN inversion and attribute manipulation.",Conference paper,http://openaccess.thecvf.com/content/ICCV2023/html/Yang_Out-of-Domain_GAN_Inversion_via_Invertibility_Decomposition_for_Photo-Realistic_Human_Face_ICCV_2023_paper.html,1,yang2023out,"generative adversarial networks,Out-Of-Domain (OOD), Human Face Manipulation"
2020,IEEE Transactions on Pattern Analysis and Machine Intelligence,Homomorphic Interpolation Network for Unpaired Image-to-image Translation,"Ying-Cong Chen, Jiaya Jia",Ying-Cong Chen,English,"Generative adversarial networks have achieved great success in unpaired image-to-image translation. Cycle consistency, a key component for this task, allows modeling the relationship between two distinct domains without paired data. In this paper, we propose an alternative framework, as an extension of latent space interpolation, to consider the intermediate region between two domains during translation. It is based on the assumption that in a flat and smooth latent space, there exist many paths that connect two sample points. Properly selecting paths makes it possible to change only certain image attributes, which is useful for generating intermediate images between the two domains. With this idea, our framework includes an encoder, an interpolator and a decoder. The encoder maps natural images to a convex and smooth latent space where interpolation is applicable. The interpolator controls the?¡­",Article,https://ieeexplore.ieee.org/abstract/document/9250648/,1,chen2020homomorphic," image-to-image translation, latent space interpolation"
2024,arXiv preprint arXiv:2405.16886,Hawk: Learning to Understand Open-World Video Anomalies,"Jiaqi Tang, Hao Lu, Ruizheng Wu, Xiaogang Xu, Ke Ma, Cheng Fang, Bin Guo, Jiangbo Lu, Qifeng Chen, Ying-Cong Chen",Jiaqi Tang,English,"Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs. However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction. Additionally, the prevalent data scarcity in existing datasets restricts their applicability in open-world scenarios. In this paper, we introduce Hawk, a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely. Recognizing the difference in motion information between abnormal and normal videos, Hawk explicitly integrates motion modality to enhance anomaly identification. To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality. Moreover, to improve the interpretation of motion-to-language, we establish a clear supervisory relationship between motion and its linguistic representation. Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users' open-world questions. The final results demonstrate that Hawk achieves SOTA performance, surpassing existing baselines in both video description generation and question-answering. Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk.",Article,https://arxiv.org/abs/2405.16886,0,tang2024hawk,"Video Anomaly Detection,interactive large Visual Language Models"
2024,arXiv preprint arXiv:2405.15321,SG-Adapter: Enhancing Text-to-Image Generation with Scene Graph Guidance,"Guibao Shen, Luozhou Wang, Jiantao Lin, Wenhang Ge, Chaozhe Zhang, Xin Tao, Yuan Zhang, Pengfei Wan, Zhongyuan Wang, Guangyong Chen, Yijun Li, Ying-Cong Chen",Guibao Shen,English,"Recent advancements in text-to-image generation have been propelled by the development of diffusion models and multi-modality learning. However, since text is typically represented sequentially in these models, it often falls short in providing accurate contextualization and structural control. So the generated images do not consistently align with human expectations, especially in complex scenarios involving multiple objects and relationships. In this paper, we introduce the Scene Graph Adapter(SG-Adapter), leveraging the structured representation of scene graphs to rectify inaccuracies in the original text embeddings. The SG-Adapter's explicit and non-fully connected graph representation greatly improves the fully connected, transformer-based text representations. This enhancement is particularly notable in maintaining precise correspondence in scenarios involving multiple relationships. To address the challenges posed by low-quality annotated datasets like Visual Genome, we have manually curated a highly clean, multi-relational scene graph-image paired dataset MultiRels. Furthermore, we design three metrics derived from GPT-4V to effectively and thoroughly measure the correspondence between images and scene graphs. Both qualitative and quantitative results validate the efficacy of our approach in controlling the correspondence in multiple relationships.",Article,https://arxiv.org/abs/2405.15321,0,shen2024sg,"text-to-image generation,scene graphs,GPT-4V"
2024,arXiv preprint arXiv:2405.06201,PhysMLE: Generalizable and Priors-Inclusive Multi-task Remote Physiological Measurement,"Jiyao Wang, Hao Lu, Ange Wang, Xiao Yang, Yingcong Chen, Dengbo He, Kaishun Wu",Jiyao Wang,English,"Remote photoplethysmography (rPPG) has been widely applied to measure heart rate from face videos. To increase the generalizability of the algorithms, domain generalization (DG) attracted increasing attention in rPPG. However, when rPPG is extended to simultaneously measure more vital signs (e.g., respiration and blood oxygen saturation), achieving generalizability brings new challenges. Although partial features shared among different physiological signals can benefit multi-task learning, the sparse and imbalanced target label space brings the seesaw effect over task-specific feature learning. To resolve this problem, we designed an end-to-end Mixture of Low-rank Experts for multi-task remote Physiological measurement (PhysMLE), which is based on multiple low-rank experts with a novel router mechanism, thereby enabling the model to adeptly handle both specifications and correlations within tasks. Additionally, we introduced prior knowledge from physiology among tasks to overcome the imbalance of label space under real-world multi-task physiological measurement. For fair and comprehensive evaluations, this paper proposed a large-scale multi-task generalization benchmark, named Multi-Source Synsemantic Domain Generalization (MSSDG) protocol. Extensive experiments with MSSDG and intra-dataset have shown the effectiveness and efficiency of PhysMLE. In addition, a new dataset was collected and made publicly available to meet the needs of the MSSDG.",Article,https://arxiv.org/abs/2405.06201,0,wang2024physmle,"rPPG, multi-task learning, mixture of experts, low-rank adaptation, domain generalization"
2024,arXiv preprint arXiv:2404.07863,Backdoor Contrastive Learning via Bi-level Trigger Optimization,"Weiyu Sun, Xinyu Zhang, Hao Lu, Yingcong Chen, Ting Wang, Jinghui Chen, Lu Lin",Weiyu Sun,English,"Contrastive Learning (CL) has attracted enormous attention due to its remarkable capability in unsupervised representation learning. However, recent works have revealed the vulnerability of CL to backdoor attacks: the feature extractor could be misled to embed backdoored data close to an attack target class, thus fooling the downstream predictor to misclassify it as the target. Existing attacks usually adopt a fixed trigger pattern and poison the training set with trigger-injected data, hoping for the feature extractor to learn the association between trigger and target class. However, we find that such fixed trigger design fails to effectively associate trigger-injected data with target class in the embedding space due to special CL mechanisms, leading to a limited attack success rate (ASR). This phenomenon motivates us to find a better backdoor trigger design tailored for CL framework. In this paper, we propose a bi-level optimization approach to achieve this goal, where the inner optimization simulates the CL dynamics of a surrogate victim, and the outer optimization enforces the backdoor trigger to stay close to the target throughout the surrogate CL procedure. Extensive experiments show that our attack can achieve a higher attack success rate (e.g.,  ASR on ImageNet-100) with a very low poisoning rate (). Besides, our attack can effectively evade existing state-of-the-art defenses. Code is available at: https://github.com/SWY666/SSL-backdoor-BLTO.",Article,https://arxiv.org/abs/2404.07863,0,sun2024backdoor,"contrastive learning,backdoor,bi-level optimization"
2024,arXiv preprint arXiv:2403.20193,Motion Inversion for Video Customization,"Luozhou Wang, Guibao Shen, Yixun Liang, Xin Tao, Pengfei Wan, Di Zhang, Yijun Li, Yingcong Chen",Luozhou Wang,English,"In this research, we present a novel approach to motion customization in video generation, addressing the widespread gap in the thorough exploration of motion representation within video generative models. Recognizing the unique challenges posed by video's spatiotemporal nature, our method introduces Motion Embeddings, a set of explicit, temporally coherent one-dimensional embeddings derived from a given video. These embeddings are designed to integrate seamlessly with the temporal transformer modules of video diffusion models, modulating self-attention computations across frames without compromising spatial integrity. Our approach offers a compact and efficient solution to motion representation and enables complex manipulations of motion characteristics through vector arithmetic in the embedding space. Furthermore, we identify the Temporal Discrepancy in video generative models, which refers to variations in how different motion modules process temporal relationships between frames. We leverage this understanding to optimize the integration of our motion embeddings. Our contributions include the introduction of a tailored motion embedding for customization tasks, insights into the temporal processing differences in video models, and a demonstration of the practical advantages and effectiveness of our method through extensive experiments.",Article,https://arxiv.org/abs/2403.20193,0,wang2024motion,"Motion Customization,Video DiffusionModel,Generative Models"
2024,2024 Design,AsymSAT: Accelerating SAT Solving with Asymmetric Graph-Based Model Prediction,"Zhiyuan Yan, Min Li, Zhengyuan Shi, Wenjie Zhang, Yingcong Chen, Hongce Zhang",Zhiyuan Yan,English,"Though graph neural networks (GNNs) have been used in SAT solution prediction, for a subset of symmetric SAT problems, we unveil that the current GNN-based end-to-end SAT solvers are bound to yield incorrect outcomes as they are unable to break symmetry in variable assignments. In response, we introduce AsymSAT, a new GNN architecture coupled where a recurrent neural network is (RNN) to produce asymmetric models. Moreover, we bring up a method to integrate machine-learning-based SAT assignment prediction with classic SAT solvers and demonstrate its performance on non-trivial SAT instances including logic equivalence checking and cryptographic analysis problems with as much as 75.45% time saving.",Conference paper,https://ieeexplore.ieee.org/abstract/document/10546648/,0,yan2024asymsat," graph neural networks,recurrent neural network,SAT"
2024,arXiv preprint arXiv:2403.11899,GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors,"LI Yang, WU Ruizheng, LI Jiyong, CHEN Ying-cong",LI Yang,English,"Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods demonstrated their ability to reconstruct accurate 3D shapes of Lambertian scenes. However, their results on reflective scenes are unsatisfactory due to the entanglement of specular radiance and complicated geometry. To address the challenges, we propose a Gaussian-based representation of normals in SDF fields. Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods. Moreover, we propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. To validate the effectiveness of our design, we capture polarimetric information, and ground truth meshes in additional reflective scenes with various geometry. We also evaluated our framework on the PANDORA dataset. Comparisons prove our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin.",Article,https://arxiv.org/abs/2403.11899,0,yang2024gnerp,"Neural Radiance Fields,Signed Distance Function, Gaussian-based representation"
2024,,TSMA-BEV: Towards Robust Multi-Camera 3D Object Detection through Temporal Sequence Mix Augmentation,"Xu Cao, Hao Lu, Ying-Cong Chen",Xu Cao,English,"The advent of bird¡¯s-eye view (BEV) representation has witnessed significant advancements in camera-only 3D object detection. However, existing approaches usually struggle when applied to various corruptions that deviate from the original training domain. To address these vulnerabilities, we propose a novel framework, TSMA-BEV, which combines a new image augmentation module AugFFT based on fast fourier transformation (FFT) with a mix-sequence augmentation strategy SeqMixAug to enhance the robustness and adaptability of 3D object detection algorithms. The proposed AugFFT, involves stochastic frequency cutoffs and amplitude scaling to generate augmented images, while SeqMixAug extends this augmentation to temporal sequences, maintaining consistency across frames. This approach ensures improved performance stability in the face of multiple corruptions. As demonstrated in our experiments, the effectiveness and superiority of TSMA-BEV in handling real-world corruptions are verified.",Conference paper,https://www.researchgate.net/profile/Robodrive-Challenge/publication/380952246_TSMA-BEV_Towards_Robust_Multi-Camera_3D_Object_Detection_through_Temporal_Sequence_Mix_Augmentation/links/665736c10b0d284574650396/TSMA-BEV-Towards-Robust-Multi-Camera-3D-Object-Detection-through-Temporal-Sequence-Mix-Augmentation.pdf,0,caotsma,"bird¡¯s-eye view,3D object detection,augmentation"
2024,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?¡­,Low-Rank Approximation for Sparse Attention in Multi-Modal LLMs,"Lin Song, Yukang Chen, Shuai Yang, Xiaohan Ding, Yixiao Ge, Ying-Cong Chen, Ying Shan",Lin Song,English,This paper focuses on the high computational complexity in Large Language Models (LLMs) a significant challenge in both natural language processing (NLP) and multi-modal tasks. We propose Low-Rank Approximation for Sparse At-tention (LoRA-Sparse) an innovative approach that strate-gically reduces this complexity. LoRA-Sparse introduces low-rank linear projection layers for sparse attention ap-proximation. It utilizes an order-mimic training methodol-ogy which is crucial for efficiently approximating the self-attention mechanism in LLMs. We empirically show that sparse attention not only reduces computational demands but also enhances model performance in both NLP and multi-modal tasks. This surprisingly shows that redundant attention in LLMs might be non-beneficial. We extensively validate LoRA-Sparse through rigorous empirical studies in both (NLP) and multi-modal tasks demonstrating its effec-tiveness and general applicability. Based on LLaMA and LLaVA models our methods can reduce more than half of the self-attention computation with even better performance than full-attention baselines.,Conference paper,http://openaccess.thecvf.com/content/CVPR2024/html/Song_Low-Rank_Approximation_for_Sparse_Attention_in_Multi-Modal_LLMs_CVPR_2024_paper.html,0,song2024low,"Low-Rank Approximation,large language models,sparse attention,natural language processing"
2024,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?¡­,Learning to Remove Wrinkled Transparent Film with Polarized Prior,"Jiaqi Tang, Ruizheng Wu, Xiaogang Xu, Sixing Hu, Ying-Cong Chen",Jiaqi Tang,English,In this paper we study a new problem Film Removal (FR) which attempts to remove the interference of wrinkled transparent films and reconstruct the original information under films for industrial recognition systems. We first physically model the imaging of industrial materials covered by the film. Considering the specular highlight from the film can be effectively recorded by the polarized camera we build a practical dataset with polarization information containing paired data with and without transparent film. We aim to remove interference from the film (specular highlights and other degradations) with an end-to-end framework. To locate the specular highlight we use an angle estimation network to optimize the polarization angle with the minimized specular highlight. The image with minimized specular highlight is set as a prior for supporting the reconstruction network. Based on the prior and the polarized images the reconstruction network can decouple all degradations from the film. Extensive experiments show that our framework achieves SOTA performance in both image reconstruction and industrial downstream tasks. Our code will be released at https://github. com/jqtangust/FilmRemoval.,Conference paper,https://openaccess.thecvf.com/content/CVPR2024/html/Tang_Learning_to_Remove_Wrinkled_Transparent_Film_with_Polarized_Prior_CVPR_2024_paper.html,0,tang2024learning,"Film Removal, industrial recognition,polarization"
2023,arXiv preprint arXiv:2312.08917,An Incremental Unified Framework for Small Defect Inspection,"Jiaqi Tang, Hao Lu, Xiaogang Xu, Ruizheng Wu, Sixing Hu, Tong Zhang, Tsz Wa Cheng, Ming Ge, Ying-Cong Chen, Fugee Tsung",Jiaqi Tang,English,"Artificial Intelligence (AI)-driven defect inspection is pivotal in industrial manufacturing. Yet, many methods, tailored to specific pipelines, grapple with diverse product portfolios and evolving processes. Addressing this, we present the Incremental Unified Framework (IUF) that can reduce the feature conflict problem when continuously integrating new objects in the pipeline, making it advantageous in object-incremental learning scenarios. Employing a state-of-the-art transformer, we introduce Object-Aware Self-Attention (OASA) to delineate distinct semantic boundaries. Semantic Compression Loss (SCL) is integrated to optimize non-primary semantic space, enhancing network adaptability for novel objects. Additionally, we prioritize retaining the features of established objects during weight updates. Demonstrating prowess in both image and pixel-level defect inspection, our approach achieves state-of-the-art performance, proving indispensable for dynamic and scalable industrial inspections. Our code will be released at https://github.com/jqtangust/IUF.",Article,https://arxiv.org/abs/2312.08917,0,tang2023incremental,"Small Defect Inspection, Incremental Unified Framework,Incremental Learning"
2023,arXiv preprint arXiv:2310.17316,Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics,"Shuai Yang, Zhifei Chen, Pengguang Chen, Xi Fang, Shu Liu, Yingcong Chen",Shuai Yang,English,"Defect inspection is paramount within the closed-loop manufacturing system. However, existing datasets for defect inspection often lack precision and semantic granularity required for practical applications. In this paper, we introduce the Defect Spectrum, a comprehensive benchmark that offers precise, semantic-abundant, and large-scale annotations for a wide range of industrial defects. Building on four key industrial benchmarks, our dataset refines existing annotations and introduces rich semantic details, distinguishing multiple defect types within a single image. Furthermore, we introduce Defect-Gen, a two-stage diffusion-based generator designed to create high-quality and diverse defective images, even when working with limited datasets. The synthetic images generated by Defect-Gen significantly enhance the efficacy of defect inspection models. Overall, The Defect Spectrum dataset demonstrates its potential in defect inspection research, offering a solid platform for testing and refining advanced models.",Article,https://arxiv.org/abs/2310.17316,0,yang2023defect,"Defect inspection,benchmark,diffusion-based generator"
2023,Computer Graphics Forum,CP©\NeRF: Conditionally Parameterized Neural Radiance Fields for Cross©\scene Novel View Synthesis,"Hao He, Yixun Liang, Shishi Xiao, Jierun Chen, Yingcong Chen",Hao He,English," Neural radiance fields (NeRF) have demonstrated a promising research direction for novel view synthesis. However, the existing approaches either require per©\scene optimization that takes significant computation time or condition on local features which overlook the global context of images. To tackle this shortcoming, we propose the Conditionally Parameterized Neural Radiance Fields (CP©\NeRF), a plug©\in module that enables NeRF to leverage contextual information from different scales. Instead of optimizing the model parameters of NeRFs directly, we train a Feature Pyramid hyperNetwork (FPN) that extracts view©\dependent global and local information from images within or across scenes to produce the model parameters. Our model can be trained end©\to©\end with standard photometric loss from NeRF. Extensive experiments demonstrate that our method can significantly boost the performance of NeRF?¡­",Article,https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14940,0,he2023cp,"Neural Radiance Fields,Image-based rendering,3D imaging"
2023,arXiv preprint arXiv:2308.12029,A Scale-Invariant Task Balancing Approach for Multi-Task Learning,"Baijiong Lin, Weisen Jiang, Feiyang Ye, Yu Zhang, Pengguang Chen, Ying-Cong Chen, Shu Liu",Baijiong Lin,English,"Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.",Article,https://arxiv.org/abs/2308.12029,0,lin2023scale,"Multi-task learning,Dual-Balancing Multi-Task Learning"
2023,ECAI 2023,High Dynamic Range Image Reconstruction via Deep Explicit Polynomial Curve Estimation,"Jiaqi Tang, Xiaogang Xu, Sixing Hu, Ying-Cong Chen",Jiaqi Tang,English,"Due to limited camera capacities, digital images usually have a narrower dynamic illumination range than real-world scene radiance. To resolve this problem, High Dynamic Range (HDR) reconstruction is proposed to recover the dynamic range to better represent real-world scenes. However, due to different physical imaging parameters, the tone-mapping functions between images and real radiance are highly diverse, which makes HDR reconstruction extremely challenging. Existing solutions can not explicitly clarify a corresponding relationship between the tone-mapping function and the generated HDR image, but this relationship is vital when guiding the reconstruction of HDR images. To address this problem, we propose a method to explicitly estimate the tone mapping function and its corresponding HDR image in one network. Firstly, based on the characteristics of the tone mapping function, we construct a?¡­",Conference paper,https://ebooks.iospress.nl/doi/10.3233/FAIA230533,0,tang2023high,High Dynamic Range reconstruction
2020,Computer Vision¨CECCV 2020: 16th European Conference,Particularity Beyond Commonality: Unpaired Identity Transfer with Multiple References,"Ruizheng Wu, Xin Tao, Yingcong Chen, Xiaoyong Shen, Jiaya Jia",Ruizheng Wu,English," Unpaired image-to-image translation aims to translate images from the source class to target one by providing sufficient data for these classes. Current few-shot translation methods use multiple reference images to describe the target domain through extracting common features. In this paper, we focus on a more specific identity transfer problem and advocate that particular property in each individual image can also benefit generation. We accordingly propose a new multi-reference identity transfer framework by simultaneously making use of particularity and commonality of reference. It is achieved via a semantic pyramid alignment module to make proper use of geometric information for individual images, as well as an attention module to aggregate for the final transformation. Extensive experiments demonstrate the effectiveness of our framework given the promising results in a number of identity transfer?¡­",Conference paper,https://link.springer.com/chapter/10.1007/978-3-030-58548-8_27,0,wu2020particularity,few-shot translation
